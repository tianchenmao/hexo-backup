{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/matery/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"themes/matery/source/css/bb.css","path":"css/bb.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/matery.css","path":"css/matery.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/my-gitalk.css","path":"css/my-gitalk.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/my.css","path":"css/my.css","modified":0,"renderable":1},{"_id":"themes/matery/source/js/matery.js","path":"js/matery.js","modified":0,"renderable":1},{"_id":"themes/matery/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/avatar.jpg","path":"medias/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/comment_bg.png","path":"medias/comment_bg.png","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/contact.png","path":"medias/contact.png","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/cover.jpg","path":"medias/cover.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/icp.png","path":"medias/icp.png","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/logo.png","path":"medias/logo.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/animate/animate.min.css","path":"libs/animate/animate.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.css","path":"libs/aos/aos.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.js","path":"libs/aos/aos.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","path":"libs/aplayer/APlayer.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","path":"libs/aplayer/APlayer.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/canvas-nest.js","path":"libs/background/canvas-nest.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon-dynamic.js","path":"libs/background/ribbon-dynamic.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon-refresh.min.js","path":"libs/background/ribbon-refresh.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon.min.js","path":"libs/background/ribbon.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeBlockFuction.js","path":"libs/codeBlock/codeBlockFuction.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeCopy.js","path":"libs/codeBlock/codeCopy.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeLang.js","path":"libs/codeBlock/codeLang.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeShrink.js","path":"libs/codeBlock/codeShrink.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","path":"libs/cryptojs/crypto-js.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","path":"libs/dplayer/DPlayer.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","path":"libs/dplayer/DPlayer.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","path":"libs/echarts/echarts.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","path":"libs/gitalk/gitalk.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","path":"libs/gitalk/gitalk.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","path":"libs/gitment/gitment-default.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment.js","path":"libs/gitment/gitment.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/instantpage/instantpage.js","path":"libs/instantpage/instantpage.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","path":"libs/jqcloud/jqcloud-1.0.4.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","path":"libs/jqcloud/jqcloud.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jquery/jquery.min.js","path":"libs/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","path":"libs/masonry/masonry.pkgd.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","path":"libs/materialize/materialize.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","path":"libs/materialize/materialize.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/minivaline/MiniValine.js","path":"libs/minivaline/MiniValine.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","path":"libs/others/busuanzi.pure.mini.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/others/clicklove.js","path":"libs/others/clicklove.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/prism/prism.css","path":"libs/prism/prism.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","path":"libs/scrollprogress/scrollProgress.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","path":"libs/tocbot/tocbot.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","path":"libs/tocbot/tocbot.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/valine/Valine.min.js","path":"libs/valine/Valine.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/valine/av-min.js","path":"libs/valine/av-min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/0.jpg","path":"medias/banner/0.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/1.jpg","path":"medias/banner/1.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/2.jpg","path":"medias/banner/2.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/3.jpg","path":"medias/banner/3.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/4.jpg","path":"medias/banner/4.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/5.jpg","path":"medias/banner/5.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/6.jpg","path":"medias/banner/6.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/7.jpg","path":"medias/banner/7.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/0.jpg","path":"medias/featureimages/0.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/1.jpg","path":"medias/featureimages/1.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/10.jpg","path":"medias/featureimages/10.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/11.jpg","path":"medias/featureimages/11.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/12.jpg","path":"medias/featureimages/12.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/13.jpg","path":"medias/featureimages/13.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/14.jpg","path":"medias/featureimages/14.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/15.jpg","path":"medias/featureimages/15.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/16.jpg","path":"medias/featureimages/16.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/17.jpg","path":"medias/featureimages/17.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/18.jpg","path":"medias/featureimages/18.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/19.jpg","path":"medias/featureimages/19.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/2.jpg","path":"medias/featureimages/2.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/20.jpg","path":"medias/featureimages/20.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/21.jpg","path":"medias/featureimages/21.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/22.jpg","path":"medias/featureimages/22.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/23.jpg","path":"medias/featureimages/23.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/3.jpg","path":"medias/featureimages/3.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/4.jpg","path":"medias/featureimages/4.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/5.jpg","path":"medias/featureimages/5.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/6.jpg","path":"medias/featureimages/6.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/7.jpg","path":"medias/featureimages/7.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/8.jpg","path":"medias/featureimages/8.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/9.jpg","path":"medias/featureimages/9.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/reward/alipay.jpg","path":"medias/reward/alipay.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/reward/wechat.png","path":"medias/reward/wechat.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/css/all.css","path":"libs/awesome/css/all.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.eot","path":"libs/awesome/webfonts/fa-brands-400.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.svg","path":"libs/awesome/webfonts/fa-brands-400.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.ttf","path":"libs/awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff","path":"libs/awesome/webfonts/fa-brands-400.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff2","path":"libs/awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.eot","path":"libs/awesome/webfonts/fa-regular-400.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.svg","path":"libs/awesome/webfonts/fa-regular-400.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.ttf","path":"libs/awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff","path":"libs/awesome/webfonts/fa-regular-400.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff2","path":"libs/awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.eot","path":"libs/awesome/webfonts/fa-solid-900.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.svg","path":"libs/awesome/webfonts/fa-solid-900.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.ttf","path":"libs/awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff","path":"libs/awesome/webfonts/fa-solid-900.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff2","path":"libs/awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","path":"libs/lightGallery/css/lightgallery.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","path":"libs/lightGallery/fonts/lg.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","path":"libs/lightGallery/fonts/lg.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","path":"libs/lightGallery/fonts/lg.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","path":"libs/lightGallery/fonts/lg.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","path":"libs/lightGallery/img/loading.gif","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","path":"libs/lightGallery/img/video-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","path":"libs/lightGallery/img/vimeo-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","path":"libs/lightGallery/img/youtube-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","path":"libs/lightGallery/js/lightgallery-all.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/css/share.min.css","path":"libs/share/css/share.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","path":"libs/share/fonts/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","path":"libs/share/fonts/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","path":"libs/share/fonts/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","path":"libs/share/fonts/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","path":"libs/share/js/jquery.share.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","path":"libs/share/js/social-share.min.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/about/index.md","hash":"5745b7c392dabec5fcb947bb90590006d1800571","modified":1616850291339},{"_id":"source/_posts/Bayes-Filter.md","hash":"65ccb86bf338fbdf3b0934537522a83c2c964ee0","modified":1616513898001},{"_id":"source/_posts/Multivariate-Gaussian-Distribution.md","hash":"e981ff4c9ccf0732b20f0216c2a08127becabf45","modified":1616849592635},{"_id":"source/categories/index.md","hash":"68e258e58927162199b744bc139cab189d43727c","modified":1616850217994},{"_id":"source/contact/index.md","hash":"c2ed34aef9d15b580ff9016cb5e36f5ca931922a","modified":1616850323086},{"_id":"source/tags/index.md","hash":"f016a474af6b7ac2426c6d969c9405365f2e1762","modified":1616850259453},{"_id":"source/_posts/Bayes-Filter/Markov.png","hash":"c3549a6f782b892870f4a75c02bf07323762807a","modified":1616400855550},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/1.png","hash":"4d899e02a5320d36e630ee548dc9a73f7b37e250","modified":1616760714945},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/2.jpg","hash":"f393c8a5eb60ed16ff886b1c90047c132fbfe8d6","modified":1616760743082},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/3.jpg","hash":"9eb93f8737d87cb2320d743f248807e2bd351f28","modified":1616760774338},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/5.png","hash":"f85d4793762210b1a2b064196098be85ff5da5c8","modified":1616830750138},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/7.jpg","hash":"ff2fabcf240dbb38506685808659adaa00dc2023","modified":1616833073301},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/4.jpg","hash":"271534180c8993d9a9b0676d7adf9fd5c036b85b","modified":1616764625572},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/6.jpg","hash":"c1b0b1182a7f8cb4876e07a1f845ca75ec0fd644","modified":1616836343718},{"_id":"themes/matery/.gitignore","hash":"727607929a51db7ea10968f547c26041eee9cfff","modified":1615600226432},{"_id":"themes/matery/LICENSE","hash":"7df059597099bb7dcf25d2a9aedfaf4465f72d8d","modified":1615600226432},{"_id":"themes/matery/README.md","hash":"0366f3d50b18d095b0581e7b5974e3283d693884","modified":1615600226432},{"_id":"themes/matery/languages/default.yml","hash":"54ccc01b097c5bf6820f0edfcece1a87b78ab32d","modified":1615600226432},{"_id":"themes/matery/CHANGELOG.md","hash":"084ec8b110a20170d08a0aa5fd8accf601051835","modified":1615600226432},{"_id":"themes/matery/languages/zh-CN.yml","hash":"a957b05f70265a86a87d922e18488571809d2472","modified":1615600226432},{"_id":"themes/matery/README_CN.md","hash":"089de96e2165ea2a8a3adf38ebda85b65e7f716e","modified":1615600226432},{"_id":"themes/matery/layout/404.ejs","hash":"9c8ca67377211e5d60fdde272a975faa9a91a22a","modified":1615600226432},{"_id":"themes/matery/languages/zh-HK.yml","hash":"ae34ac0e175c3037675722e436637efbceea32f0","modified":1615600226432},{"_id":"themes/matery/layout/about.ejs","hash":"41849f9300b8dc47048333fcf4a897dd8a2a13ca","modified":1615600226436},{"_id":"themes/matery/layout/archive.ejs","hash":"cdac701de8370f9f3794a0eed4165983993a1ca7","modified":1615600226436},{"_id":"themes/matery/layout/categories.ejs","hash":"8e54665cc25d7c333da7d9f312987190be6215da","modified":1615600226436},{"_id":"themes/matery/layout/category.ejs","hash":"00019bca11fb46477f22017cb1f5ad8444da0580","modified":1615600226436},{"_id":"themes/matery/layout/bb.ejs","hash":"21959d702f17a3d98b716daf44c8b5eecd59c7c5","modified":1615600226436},{"_id":"themes/matery/layout/contact.ejs","hash":"19d62e521c4253496db559478db5164ddfd2480e","modified":1615600226436},{"_id":"themes/matery/layout/friends.ejs","hash":"92892bab5578ccf758ce57e19fca08be80d0d5b9","modified":1615600226436},{"_id":"themes/matery/layout/index.ejs","hash":"4dc6f08e7709cc04e886be72dbf0d06469f0effc","modified":1615600226436},{"_id":"themes/matery/layout/layout.ejs","hash":"974b44eb3e343cd3ee57ebad34bbb0eff4184400","modified":1615600226436},{"_id":"themes/matery/layout/post.ejs","hash":"60fc6e340f696435cde542ea5a2a41766cf95d8a","modified":1616240431440},{"_id":"themes/matery/layout/tags.ejs","hash":"cf9517aa6a0111355121f44615d6923e312283c7","modified":1615600226436},{"_id":"themes/matery/layout/tag.ejs","hash":"85a4b05bd8a6ad0f17ff2e97dae56949b379c204","modified":1615600226436},{"_id":"themes/matery/source/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1615600226436},{"_id":"themes/matery/layout/_partial/back-top.ejs","hash":"47ee36a042bb6d52bbe1d0f329637e8ffcf1d0aa","modified":1615600226432},{"_id":"themes/matery/layout/_partial/background.ejs","hash":"aef6edeeb11209831a11d8c7f5d59992e2573335","modified":1615600226432},{"_id":"themes/matery/layout/_partial/baidu-analytics.ejs","hash":"3bbcdb474ca1dcad514bdc4b7763e17c55df04fd","modified":1615600226432},{"_id":"themes/matery/layout/_partial/baidu-push.ejs","hash":"2cebcc5ea3614d7f76ec36670e68050cbe611202","modified":1615600226432},{"_id":"themes/matery/layout/_partial/bg-cover-content.ejs","hash":"28617bf2a35a4269eba6df466acd174e416d2d1e","modified":1615600226432},{"_id":"themes/matery/layout/_partial/changyan.ejs","hash":"cd919d31564e118c2ee8d5cbfb7d51ee6da15d82","modified":1615600226432},{"_id":"themes/matery/layout/_partial/bg-cover.ejs","hash":"02191109712f61c0e487b8f0b8466597181a9004","modified":1615600226432},{"_id":"themes/matery/_config.yml","hash":"fa2e8e138f69688ead380036a961c9278ef0f515","modified":1616851542164},{"_id":"themes/matery/layout/_partial/disqus.ejs","hash":"b2dc2c8b5ed56815e55cc2ea54a6dc4eeba2375d","modified":1615600226432},{"_id":"themes/matery/layout/_partial/gitalk.ejs","hash":"2aa8fbb04b046fa7679092a48372d7e036835dff","modified":1615600226432},{"_id":"themes/matery/layout/_partial/github-link.ejs","hash":"3aeb581bd78ab8e15b858e4c44c03bcf92f20b9e","modified":1615600226432},{"_id":"themes/matery/layout/_partial/gitment.ejs","hash":"90f6218512ef2eab63ada7ad2fc766ae635a2297","modified":1615600226436},{"_id":"themes/matery/layout/_partial/google-analytics.ejs","hash":"5f4992205617da5f8cc5863c62b5ec46e414e2fb","modified":1615600226436},{"_id":"themes/matery/layout/_partial/header.ejs","hash":"59e38c70f3d8e7165e686e5e84a627835f4321b0","modified":1615600226436},{"_id":"themes/matery/layout/_partial/navigation.ejs","hash":"78b70ff24b3039c871331ebec114b936c1756cc8","modified":1615600226436},{"_id":"themes/matery/layout/_partial/paging.ejs","hash":"e2df12cf92a82b1a7a7add2eac1db1d954bc5511","modified":1615600226436},{"_id":"themes/matery/layout/_partial/post-cover.ejs","hash":"d1c873c5de54498c722e155aadb8c0ec39485dfa","modified":1615600226436},{"_id":"themes/matery/layout/_partial/footer.ejs","hash":"3be24e4c370671eda53bdfd99fb748f4a22948ba","modified":1615600226432},{"_id":"themes/matery/layout/_partial/index-cover.ejs","hash":"76b4a37e0364380b143fdf94bf1a5e6941564414","modified":1615600226436},{"_id":"themes/matery/layout/_partial/head.ejs","hash":"f8438ac80df005934a330b029de292d26f0b6ecb","modified":1615600226436},{"_id":"themes/matery/layout/_partial/post-detail-toc.ejs","hash":"3ff94aff01936242a9f4e1f31adb9b43bfab8d53","modified":1615600226436},{"_id":"themes/matery/layout/_partial/post-statis.ejs","hash":"04889f9031743c6b081d02fa4027b0dbfcc45ecf","modified":1615600226436},{"_id":"themes/matery/layout/_partial/reward.ejs","hash":"ffc55bc7e73bc698bfc58d8e3780c336b83282cf","modified":1615600226436},{"_id":"themes/matery/layout/_partial/post-detail.ejs","hash":"880ebaf78a947631a38ad0b3d65201315845a264","modified":1615600226436},{"_id":"themes/matery/layout/_partial/reprint-statement.ejs","hash":"0ce3f9361f558b99cc2f059c5e50b0e2a152ae38","modified":1615600226436},{"_id":"themes/matery/layout/_partial/share.ejs","hash":"c941730a2471d6aab367cbb6e09ed08b56c83143","modified":1615600226436},{"_id":"themes/matery/layout/_partial/social-link.ejs","hash":"6f871bd3a70f720e4e451f1f4f625cbc6d8994a4","modified":1615600226436},{"_id":"themes/matery/layout/_partial/search.ejs","hash":"150529c9fb9aa8ddb42ec3e02645d301faa2503b","modified":1615600226436},{"_id":"themes/matery/layout/_widget/artitalk.ejs","hash":"b14e486f12b9ac42a273b80e4d785fcb94cf04b2","modified":1615600226436},{"_id":"themes/matery/layout/_partial/prev-next.ejs","hash":"c76b78782ea82340104fccc089417572e0adece4","modified":1615600226436},{"_id":"themes/matery/layout/_widget/category-cloud.ejs","hash":"1b3df1009234c0112424b497b18b4ad8240b3bc7","modified":1615600226436},{"_id":"themes/matery/layout/_widget/category-radar.ejs","hash":"1d8747fda89a0b2ca3c7008867cbfeecad0578a6","modified":1615600226436},{"_id":"themes/matery/layout/_partial/valine.ejs","hash":"0e4c0a6154aa34007849928ca88f05b6185b256e","modified":1615600226436},{"_id":"themes/matery/layout/_widget/dream.ejs","hash":"9a472ad5591100cdb65d0df9d01034163bd6dd9d","modified":1615600226436},{"_id":"themes/matery/layout/_widget/music.ejs","hash":"e9e3e327d5de9d7aeadbde32e1d558652d9e9195","modified":1615600226436},{"_id":"themes/matery/layout/_widget/my-gallery.ejs","hash":"65a2d2f9722f84c7fd98f6bdf79087a14848ebd8","modified":1615600226436},{"_id":"themes/matery/layout/_widget/my-projects.ejs","hash":"ef60b64021fa349b0048425d858dfcf6c906fede","modified":1615600226436},{"_id":"themes/matery/layout/_widget/my-skills.ejs","hash":"89a0092df72d23093128f2fbbdc8ca7f83ebcfd9","modified":1615600226436},{"_id":"themes/matery/layout/_partial/livere.ejs","hash":"9c3401b42ea7f26410a5593bae93ada7e57b43be","modified":1615600226436},{"_id":"themes/matery/layout/_partial/minivaline.ejs","hash":"5f09386aece8f9cf31f6059bbde79cd6c5171493","modified":1615600226436},{"_id":"themes/matery/layout/_widget/post-calendar.ejs","hash":"48821e644bc73553d7c5c56d2e8ee111a70cd776","modified":1615600226436},{"_id":"themes/matery/layout/_partial/mobile-nav.ejs","hash":"cb0cb452be1cd1857ba600f04025b506f3b6fc79","modified":1615600226436},{"_id":"themes/matery/layout/_widget/tag-wordcloud.ejs","hash":"487aacb2454d6bf0d21cdb07ddd1fd5ddbca9038","modified":1615600226436},{"_id":"themes/matery/layout/_widget/tag-cloud.ejs","hash":"fc42b72cddc231f7485cdc1fd6852b66be6add26","modified":1615600226436},{"_id":"themes/matery/layout/_widget/post-charts.ejs","hash":"ab5f986f428215941aeaa0c88aefd440c47d3bcf","modified":1615600226436},{"_id":"themes/matery/layout/_widget/recommend.ejs","hash":"8551137e94ca4e2e3b8b63d5626255884cb60cb5","modified":1615600226436},{"_id":"themes/matery/layout/_widget/video.ejs","hash":"a0e002377af2a7f7e4da6d9a644de97adb035925","modified":1615600226436},{"_id":"themes/matery/source/css/my.css","hash":"497e50351f7838f8546cac76850a42e7e380a110","modified":1615600226436},{"_id":"themes/matery/source/css/my-gitalk.css","hash":"af18dd29e58642c18bab9b89541767b494c468dd","modified":1615600226436},{"_id":"themes/matery/source/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1615600226436},{"_id":"themes/matery/source/js/search.js","hash":"e1482406c58ea2a0eb178d7e4efb2c879cdddc80","modified":1615600226436},{"_id":"themes/matery/source/medias/contact.png","hash":"443ea472dd49b74d9d70295837eb381c8c64f02c","modified":1615600226464},{"_id":"themes/matery/source/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1615600226464},{"_id":"themes/matery/source/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1615600226436},{"_id":"themes/matery/source/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1615600226472},{"_id":"themes/matery/source/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1615600226464},{"_id":"themes/matery/source/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1615600226472},{"_id":"themes/matery/source/css/matery.css","hash":"a630f6e8643904073dce9eada57b5c16c4dba5e2","modified":1615600226436},{"_id":"themes/matery/source/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1615600226440},{"_id":"themes/matery/source/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1615600226440},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1615600226440},{"_id":"themes/matery/source/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1615600226456},{"_id":"themes/matery/source/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1615600226456},{"_id":"themes/matery/source/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1615600226456},{"_id":"themes/matery/source/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1615600226456},{"_id":"themes/matery/source/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1615600226456},{"_id":"themes/matery/source/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1615600226456},{"_id":"themes/matery/source/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1615600226456},{"_id":"themes/matery/source/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1615600226436},{"_id":"themes/matery/source/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1615600226456},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1615600226440},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1615600226456},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1615600226456},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1615600226460},{"_id":"themes/matery/source/css/bb.css","hash":"aa15633888c7cf9baea8bb48d796c68b57cf14bf","modified":1615600226436},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1615600226460},{"_id":"themes/matery/source/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1615600226460},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1615600226460},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1615600226460},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1615600226460},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1615600226460},{"_id":"themes/matery/source/libs/prism/prism.css","hash":"62e5474893dece076534352f564ceabd6e088a5a","modified":1615600226460},{"_id":"themes/matery/source/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1615600226460},{"_id":"themes/matery/source/libs/minivaline/MiniValine.js","hash":"fbb58c37e2c74f127ae0c566afa9b48889aab79f","modified":1615600226460},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1615600226464},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1615600226460},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1615600226464},{"_id":"themes/matery/source/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1615600226472},{"_id":"themes/matery/source/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1615600226472},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1615600226448},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1615600226452},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1615600226452},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1615600226452},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1615600226460},{"_id":"themes/matery/source/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1615600226460},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1615600226464},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1615600226460},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1615600226464},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1615600226464},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1615600226464},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1615600226464},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1615600226456},{"_id":"themes/matery/source/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1615600226460},{"_id":"themes/matery/source/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1615600226460},{"_id":"themes/matery/source/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1615600226464},{"_id":"themes/matery/source/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1615600226472},{"_id":"themes/matery/source/medias/reward/wechat.png","hash":"dd788e8eeb8cecbec7adfeee22e61e61cb3792c5","modified":1616851171338},{"_id":"themes/matery/source/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1615600226440},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1615600226444},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1615600226448},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1615600226448},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1615600226448},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1615600226456},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1615600226456},{"_id":"themes/matery/source/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1615600226468},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1615600226460},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1615600226460},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1615600226460},{"_id":"themes/matery/source/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1615600226464},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1615600226452},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1615600226456},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1615600226452},{"_id":"themes/matery/source/medias/reward/alipay.jpg","hash":"8991290e22fd2e997a16d866df83aa5cabe72465","modified":1616851169350},{"_id":"themes/matery/source/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/6.jpg","hash":"e999639e0f733a180822d3f8359e92909540aad0","modified":1616851387046},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1615600226448},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1615600226460},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1615600226456},{"_id":"themes/matery/source/medias/banner/7.jpg","hash":"b023a2240ee8182ddf2ee51322be99ab188e3b4d","modified":1616851454269},{"_id":"public/archives/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1616851561028},{"_id":"public/archives/2021/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1616851561028},{"_id":"public/archives/2021/03/index.html","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1616851561028},{"_id":"public/about/index.html","hash":"e2932a51b8bea8e9158836acb0bec8bf397b1946","modified":1616851561028},{"_id":"public/tags/index.html","hash":"324c5dcf20317fbf809bd4d709523e1be94209c4","modified":1616851561028},{"_id":"public/categories/index.html","hash":"c4b5fea92ba65dd9881af4ce28ef726402ff626a","modified":1616851561028},{"_id":"public/contact/index.html","hash":"d675f41f28a9932135a1b64491f069c172cfdc76","modified":1616851561028},{"_id":"public/2021/03/24/Multivariate-Gaussian-Distribution/index.html","hash":"5cd86674bf27653dc754c887ae3f3be072e0b9ac","modified":1616851561028},{"_id":"public/2021/03/22/Bayes-Filter/index.html","hash":"01f393fd9acd23d9b550441976cb3620027ce14b","modified":1616851561028},{"_id":"public/index.html","hash":"06974c19fd97a4cf3e7cd3c397d6a2bf92338a73","modified":1616851561028},{"_id":"public/tags/Bayes/index.html","hash":"e1b73a1d0cb9a8b96e30b5ec417844766d62d2ca","modified":1616851561028},{"_id":"public/tags/Filter/index.html","hash":"d3b16cf6d89563aa92948b58318deb574d23384a","modified":1616851561028},{"_id":"public/tags/Gaussian/index.html","hash":"220c1c773cffe0f0d99789241a051a65da12b450","modified":1616851561028},{"_id":"public/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1616851561028},{"_id":"public/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1616851561028},{"_id":"public/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1616851561028},{"_id":"public/medias/contact.png","hash":"443ea472dd49b74d9d70295837eb381c8c64f02c","modified":1616851561028},{"_id":"public/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1616851561028},{"_id":"public/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1616851561028},{"_id":"public/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1616851561028},{"_id":"public/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1616851561028},{"_id":"public/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1616851561028},{"_id":"public/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1616851561028},{"_id":"public/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1616851561028},{"_id":"public/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1616851561028},{"_id":"public/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1616851561028},{"_id":"public/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1616851561028},{"_id":"public/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1616851561028},{"_id":"public/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1616851561028},{"_id":"public/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1616851561028},{"_id":"public/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1616851561028},{"_id":"public/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1616851561028},{"_id":"public/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1616851561028},{"_id":"public/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1616851561028},{"_id":"public/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1616851561028},{"_id":"public/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1616851561028},{"_id":"public/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1616851561028},{"_id":"public/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1616851561028},{"_id":"public/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1616851561028},{"_id":"public/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1616851561028},{"_id":"public/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1616851561028},{"_id":"public/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1616851561028},{"_id":"public/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1616851561028},{"_id":"public/2021/03/22/Bayes-Filter/Markov.png","hash":"c3549a6f782b892870f4a75c02bf07323762807a","modified":1616851561028},{"_id":"public/2021/03/24/Multivariate-Gaussian-Distribution/1.png","hash":"4d899e02a5320d36e630ee548dc9a73f7b37e250","modified":1616851561028},{"_id":"public/2021/03/24/Multivariate-Gaussian-Distribution/2.jpg","hash":"f393c8a5eb60ed16ff886b1c90047c132fbfe8d6","modified":1616851561028},{"_id":"public/2021/03/24/Multivariate-Gaussian-Distribution/3.jpg","hash":"9eb93f8737d87cb2320d743f248807e2bd351f28","modified":1616851561028},{"_id":"public/2021/03/24/Multivariate-Gaussian-Distribution/5.png","hash":"f85d4793762210b1a2b064196098be85ff5da5c8","modified":1616851561028},{"_id":"public/2021/03/24/Multivariate-Gaussian-Distribution/7.jpg","hash":"ff2fabcf240dbb38506685808659adaa00dc2023","modified":1616851561028},{"_id":"public/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1616851561028},{"_id":"public/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1616851561028},{"_id":"public/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1616851561028},{"_id":"public/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1616851561028},{"_id":"public/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1616851561028},{"_id":"public/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1616851561028},{"_id":"public/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1616851561028},{"_id":"public/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1616851561028},{"_id":"public/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1616851561028},{"_id":"public/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1616851561028},{"_id":"public/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1616851561028},{"_id":"public/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1616851561028},{"_id":"public/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1616851561028},{"_id":"public/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1616851561028},{"_id":"public/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1616851561028},{"_id":"public/medias/reward/wechat.png","hash":"dd788e8eeb8cecbec7adfeee22e61e61cb3792c5","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1616851561028},{"_id":"public/2021/03/24/Multivariate-Gaussian-Distribution/4.jpg","hash":"271534180c8993d9a9b0676d7adf9fd5c036b85b","modified":1616851561028},{"_id":"public/2021/03/24/Multivariate-Gaussian-Distribution/6.jpg","hash":"c1b0b1182a7f8cb4876e07a1f845ca75ec0fd644","modified":1616851561028},{"_id":"public/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1616851561028},{"_id":"public/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1616851561028},{"_id":"public/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1616851561028},{"_id":"public/css/bb.css","hash":"aa15633888c7cf9baea8bb48d796c68b57cf14bf","modified":1616851561028},{"_id":"public/css/my-gitalk.css","hash":"af18dd29e58642c18bab9b89541767b494c468dd","modified":1616851561028},{"_id":"public/css/my.css","hash":"497e50351f7838f8546cac76850a42e7e380a110","modified":1616851561028},{"_id":"public/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1616851561028},{"_id":"public/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1616851561028},{"_id":"public/js/search.js","hash":"e1482406c58ea2a0eb178d7e4efb2c879cdddc80","modified":1616851561028},{"_id":"public/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1616851561028},{"_id":"public/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1616851561028},{"_id":"public/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1616851561028},{"_id":"public/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1616851561028},{"_id":"public/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1616851561028},{"_id":"public/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1616851561028},{"_id":"public/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1616851561028},{"_id":"public/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1616851561028},{"_id":"public/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1616851561028},{"_id":"public/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1616851561028},{"_id":"public/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1616851561028},{"_id":"public/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1616851561028},{"_id":"public/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1616851561028},{"_id":"public/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1616851561028},{"_id":"public/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1616851561028},{"_id":"public/libs/prism/prism.css","hash":"62e5474893dece076534352f564ceabd6e088a5a","modified":1616851561028},{"_id":"public/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1616851561028},{"_id":"public/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1616851561028},{"_id":"public/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1616851561028},{"_id":"public/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1616851561028},{"_id":"public/css/matery.css","hash":"a630f6e8643904073dce9eada57b5c16c4dba5e2","modified":1616851561028},{"_id":"public/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1616851561028},{"_id":"public/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1616851561028},{"_id":"public/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1616851561028},{"_id":"public/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1616851561028},{"_id":"public/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1616851561028},{"_id":"public/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1616851561028},{"_id":"public/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1616851561028},{"_id":"public/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1616851561028},{"_id":"public/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1616851561028},{"_id":"public/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1616851561028},{"_id":"public/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1616851561028},{"_id":"public/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1616851561028},{"_id":"public/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1616851561028},{"_id":"public/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1616851561028},{"_id":"public/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1616851561028},{"_id":"public/libs/minivaline/MiniValine.js","hash":"fbb58c37e2c74f127ae0c566afa9b48889aab79f","modified":1616851561028},{"_id":"public/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1616851561028},{"_id":"public/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1616851561028},{"_id":"public/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1616851561028},{"_id":"public/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1616851561028},{"_id":"public/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1616851561028},{"_id":"public/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1616851561028},{"_id":"public/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1616851561028},{"_id":"public/medias/reward/alipay.jpg","hash":"8991290e22fd2e997a16d866df83aa5cabe72465","modified":1616851561028},{"_id":"public/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1616851561028},{"_id":"public/medias/banner/6.jpg","hash":"e999639e0f733a180822d3f8359e92909540aad0","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1616851561028},{"_id":"public/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1616851561028},{"_id":"public/medias/banner/7.jpg","hash":"b023a2240ee8182ddf2ee51322be99ab188e3b4d","modified":1616851561028}],"Category":[],"Data":[],"Page":[{"title":"about","date":"2021-03-27T13:04:35.000Z","type":"about","layout":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2021-03-27 21:04:35\ntype: \"about\"\nlayout: \"about\"\n---\n","updated":"2021-03-27T13:04:51.339Z","path":"about/index.html","comments":1,"_id":"ckmrro94i0000xnac7duihngc","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"tags","date":"2021-03-27T13:03:48.000Z","type":"tags","layout":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2021-03-27 21:03:48\ntype: \"tags\"\nlayout: \"tags\"\n---\n","updated":"2021-03-27T13:04:19.453Z","path":"tags/index.html","comments":1,"_id":"ckmrro94m0002xnacga2vhadh","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"categories","date":"2021-03-27T13:03:07.000Z","type":"categories","layout":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2021-03-27 21:03:07\ntype: \"categories\"\nlayout: \"categories\"\n---\n","updated":"2021-03-27T13:03:37.994Z","path":"categories/index.html","comments":1,"_id":"ckmrro94o0005xnac9y9v59vn","content":"","site":{"data":{}},"excerpt":"","more":""},{"title":"contact","date":"2021-03-27T13:05:03.000Z","type":"contact","layout":"contact","_content":"","source":"contact/index.md","raw":"---\ntitle: contact\ndate: 2021-03-27 21:05:03\ntype: \"contact\"\nlayout: \"contact\"\n---\n","updated":"2021-03-27T13:05:23.086Z","path":"contact/index.html","comments":1,"_id":"ckmrro94o0006xnacfb81beze","content":"","site":{"data":{}},"excerpt":"","more":""}],"Post":[{"title":"Bayes Filter","date":"2021-03-22T07:46:14.000Z","mathjax":true,"_content":"\n# Bayes Filter\n\n关于本文：这是一份简约的入门教程。\n\n## 滤波是什么\n\n滤波是一种方法，手段，在时移（time-varying）系统中，通过对带有噪声的观测${z_k}$ 来估计隐状态$x_k$的处理方法；\n\n滤波（filtering）专门是指利用前面 $k$个观测 $z_1,z_2,...,z_k$  来估计第$k$个隐状态$x_k$的方法,记作$x_k|z_{1:k}$。\n\n另外，如果利用前面$d(d<k)$个观测来估计第$k$个状态，则称为预测（prediction），具体地，叫 $k-d$步预测；\n\n如果利用前面$d$个观测 $z_{1:d}$来估计第$k$个状态，则称为平滑或内插（smoothing)。\n\n所以其实滤波(filtering)就是 $d=k$ 的情形。\n\n## 状态、控制、观测\n\n但是上述定义并没有考虑系统的控制量，如果考虑控制量，系统模型就如图1所示:\n\n![Markov](./Bayes-Filter/Markov.png)\n\n符号说明：$x$为状态量，$z$为观测量，$u$为控制量\n\n注意，这张图非常重要，它代表着隐马尔可夫模型（HMM），后者是贝叶斯滤波的基础假设，对隐马尔可夫模型的探讨超出了本篇的范畴，这里只做简单介绍。读者可以想象，作为一个普通人（不是全知全能的神明），想要知道一个系统的状态，只能通过测量的手段，再通过滤波等手段得到状态的最优估计。在这里，系统状态$x$就是隐变量，观测值$z$是观测变量。观测得到的是既定的结果，但在它的背后，真实的状态其实是在以一定概率转移的，并且状态本身未必是确定的，它可能是一个范围，有一定的方差（模型噪声）。另一方面，测量所用的仪器存在量测误差（噪声），这个量测误差是随机的，又在观测值背后增加了一层随机性。而我们的目的就是从既定的结果去估计一个随机变量（是不是感觉跟极大似然估计很像？）。\n\n## 贝叶斯滤波\n\n贝叶斯滤波的目标是得到$P(x_k|z_{1:k},u_{1:k})$，它的含义是在前k个观测值和控制量的基础上估计第k个状态（状态估计），我们的目标是得到$P(x_k|z_{1:k},u_{1:k})$的递推形式。先来看第一个重要的公式：\n\n\n$$\nP(x_k|z_{1:k},u_{1:k})=\\frac{P(z_k|x_k,z_{1:k-1},u_{1:k})P(x_k|z_{1:k-1},u_{1:k})}{P(z_k|z_{1:k-1},u_{1:k})} \\tag{1.1}\n$$\n这个公式就是在$z_{1:k-1},u_{1:k}$条件下的贝叶斯公式（这大概就是贝叶斯滤波为什么叫贝叶斯滤波的原因吧）\n\n接下来讲引入贝叶斯滤波中的基本假设：**Markov性假设: k时刻的状态由k-1时刻的状态和k时刻输入的控制量决定。k时刻的观测仅同k时刻的状态相关。**由此我们得到\n$$\nP(x_k|x_{1:k-1},z_{1:k},u_{1:k})=P(x_k|x_{k-1},u_k) \\\\\nP(z_k|x_k,z_{1:k-1},u_{1:k})=P(z_k|x_k)  \\\\\nP(z_k|z_{1:k-1},u_{1:k})=P(z_k) \\tag{1.2}\n$$\n\n**上述假设可以通过图1得到直观的理解。**$P(z_k)$与我们所关心的状态$x_k$无关，所以可以把它当做归一化常数$\\eta_k$\n\n为了得到递推形式，我们很自然能想到引出上一时刻的状态$x_{k-1}$，只需要简单地运用全概率公式即可：\n$$\nP(x_k|z_{1:k-1},u_{1:k})=\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k})dx_{k-1}\\tag{1.3}\n$$\n因为$x_{k-1}$与$u_k$无关，所以式(1.3)修正一下$u$的下标变为\n$$\nP(x_k|z_{1:k-1},u_{1:k})=\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k-1})dx_{k-1}\\tag{1.3.1}\n$$\n\n\n继而式(1.1)可以化成\n$$\nP(x_k|z_{1:k},u_{1:k})=\\eta_kP(z_k|x_k)\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k-1})dx_{k-1} \\tag{1.4}\n$$\n至此，我们得到了贝叶斯滤波的递归形式，之后可能会增加代码实现部分，和对系统模型的进一步说明。\n\n## 参考资料\n\n1.[细说贝叶斯滤波：Bayes filters](https://www.cnblogs.com/ycwang16/p/5995702.html)\n\n2.[通俗地解释卡尔曼滤波器（一）——从贝叶斯滤波器说起](https://zhuanlan.zhihu.com/p/318701822)\n\n3.[贝叶斯滤波（Bayesian filtering)](https://zhuanlan.zhihu.com/p/139215491)","source":"_posts/Bayes-Filter.md","raw":"---\ntitle: Bayes Filter\ndate: 2021-03-22 15:46:14\ntags: \n- Bayes \n- Filter \nmathjax: true\n---\n\n# Bayes Filter\n\n关于本文：这是一份简约的入门教程。\n\n## 滤波是什么\n\n滤波是一种方法，手段，在时移（time-varying）系统中，通过对带有噪声的观测${z_k}$ 来估计隐状态$x_k$的处理方法；\n\n滤波（filtering）专门是指利用前面 $k$个观测 $z_1,z_2,...,z_k$  来估计第$k$个隐状态$x_k$的方法,记作$x_k|z_{1:k}$。\n\n另外，如果利用前面$d(d<k)$个观测来估计第$k$个状态，则称为预测（prediction），具体地，叫 $k-d$步预测；\n\n如果利用前面$d$个观测 $z_{1:d}$来估计第$k$个状态，则称为平滑或内插（smoothing)。\n\n所以其实滤波(filtering)就是 $d=k$ 的情形。\n\n## 状态、控制、观测\n\n但是上述定义并没有考虑系统的控制量，如果考虑控制量，系统模型就如图1所示:\n\n![Markov](./Bayes-Filter/Markov.png)\n\n符号说明：$x$为状态量，$z$为观测量，$u$为控制量\n\n注意，这张图非常重要，它代表着隐马尔可夫模型（HMM），后者是贝叶斯滤波的基础假设，对隐马尔可夫模型的探讨超出了本篇的范畴，这里只做简单介绍。读者可以想象，作为一个普通人（不是全知全能的神明），想要知道一个系统的状态，只能通过测量的手段，再通过滤波等手段得到状态的最优估计。在这里，系统状态$x$就是隐变量，观测值$z$是观测变量。观测得到的是既定的结果，但在它的背后，真实的状态其实是在以一定概率转移的，并且状态本身未必是确定的，它可能是一个范围，有一定的方差（模型噪声）。另一方面，测量所用的仪器存在量测误差（噪声），这个量测误差是随机的，又在观测值背后增加了一层随机性。而我们的目的就是从既定的结果去估计一个随机变量（是不是感觉跟极大似然估计很像？）。\n\n## 贝叶斯滤波\n\n贝叶斯滤波的目标是得到$P(x_k|z_{1:k},u_{1:k})$，它的含义是在前k个观测值和控制量的基础上估计第k个状态（状态估计），我们的目标是得到$P(x_k|z_{1:k},u_{1:k})$的递推形式。先来看第一个重要的公式：\n\n\n$$\nP(x_k|z_{1:k},u_{1:k})=\\frac{P(z_k|x_k,z_{1:k-1},u_{1:k})P(x_k|z_{1:k-1},u_{1:k})}{P(z_k|z_{1:k-1},u_{1:k})} \\tag{1.1}\n$$\n这个公式就是在$z_{1:k-1},u_{1:k}$条件下的贝叶斯公式（这大概就是贝叶斯滤波为什么叫贝叶斯滤波的原因吧）\n\n接下来讲引入贝叶斯滤波中的基本假设：**Markov性假设: k时刻的状态由k-1时刻的状态和k时刻输入的控制量决定。k时刻的观测仅同k时刻的状态相关。**由此我们得到\n$$\nP(x_k|x_{1:k-1},z_{1:k},u_{1:k})=P(x_k|x_{k-1},u_k) \\\\\nP(z_k|x_k,z_{1:k-1},u_{1:k})=P(z_k|x_k)  \\\\\nP(z_k|z_{1:k-1},u_{1:k})=P(z_k) \\tag{1.2}\n$$\n\n**上述假设可以通过图1得到直观的理解。**$P(z_k)$与我们所关心的状态$x_k$无关，所以可以把它当做归一化常数$\\eta_k$\n\n为了得到递推形式，我们很自然能想到引出上一时刻的状态$x_{k-1}$，只需要简单地运用全概率公式即可：\n$$\nP(x_k|z_{1:k-1},u_{1:k})=\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k})dx_{k-1}\\tag{1.3}\n$$\n因为$x_{k-1}$与$u_k$无关，所以式(1.3)修正一下$u$的下标变为\n$$\nP(x_k|z_{1:k-1},u_{1:k})=\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k-1})dx_{k-1}\\tag{1.3.1}\n$$\n\n\n继而式(1.1)可以化成\n$$\nP(x_k|z_{1:k},u_{1:k})=\\eta_kP(z_k|x_k)\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k-1})dx_{k-1} \\tag{1.4}\n$$\n至此，我们得到了贝叶斯滤波的递归形式，之后可能会增加代码实现部分，和对系统模型的进一步说明。\n\n## 参考资料\n\n1.[细说贝叶斯滤波：Bayes filters](https://www.cnblogs.com/ycwang16/p/5995702.html)\n\n2.[通俗地解释卡尔曼滤波器（一）——从贝叶斯滤波器说起](https://zhuanlan.zhihu.com/p/318701822)\n\n3.[贝叶斯滤波（Bayesian filtering)](https://zhuanlan.zhihu.com/p/139215491)","slug":"Bayes-Filter","published":1,"updated":"2021-03-23T15:38:18.001Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmrro94k0001xnacd2rwf6gt","content":"<h1 id=\"Bayes-Filter\"><a href=\"#Bayes-Filter\" class=\"headerlink\" title=\"Bayes Filter\"></a>Bayes Filter</h1><p>关于本文：这是一份简约的入门教程。</p>\n<h2 id=\"滤波是什么\"><a href=\"#滤波是什么\" class=\"headerlink\" title=\"滤波是什么\"></a>滤波是什么</h2><p>滤波是一种方法，手段，在时移（time-varying）系统中，通过对带有噪声的观测${z_k}$ 来估计隐状态$x_k$的处理方法；</p>\n<p>滤波（filtering）专门是指利用前面 $k$个观测 $z_1,z_2,…,z_k$  来估计第$k$个隐状态$x_k$的方法,记作$x_k|z_{1:k}$。</p>\n<p>另外，如果利用前面$d(d&lt;k)$个观测来估计第$k$个状态，则称为预测（prediction），具体地，叫 $k-d$步预测；</p>\n<p>如果利用前面$d$个观测 $z_{1:d}$来估计第$k$个状态，则称为平滑或内插（smoothing)。</p>\n<p>所以其实滤波(filtering)就是 $d=k$ 的情形。</p>\n<h2 id=\"状态、控制、观测\"><a href=\"#状态、控制、观测\" class=\"headerlink\" title=\"状态、控制、观测\"></a>状态、控制、观测</h2><p>但是上述定义并没有考虑系统的控制量，如果考虑控制量，系统模型就如图1所示:</p>\n<p><img src=\"/2021/03/22/Bayes-Filter/Markov.png\" alt=\"Markov\"></p>\n<p>符号说明：$x$为状态量，$z$为观测量，$u$为控制量</p>\n<p>注意，这张图非常重要，它代表着隐马尔可夫模型（HMM），后者是贝叶斯滤波的基础假设，对隐马尔可夫模型的探讨超出了本篇的范畴，这里只做简单介绍。读者可以想象，作为一个普通人（不是全知全能的神明），想要知道一个系统的状态，只能通过测量的手段，再通过滤波等手段得到状态的最优估计。在这里，系统状态$x$就是隐变量，观测值$z$是观测变量。观测得到的是既定的结果，但在它的背后，真实的状态其实是在以一定概率转移的，并且状态本身未必是确定的，它可能是一个范围，有一定的方差（模型噪声）。另一方面，测量所用的仪器存在量测误差（噪声），这个量测误差是随机的，又在观测值背后增加了一层随机性。而我们的目的就是从既定的结果去估计一个随机变量（是不是感觉跟极大似然估计很像？）。</p>\n<h2 id=\"贝叶斯滤波\"><a href=\"#贝叶斯滤波\" class=\"headerlink\" title=\"贝叶斯滤波\"></a>贝叶斯滤波</h2><p>贝叶斯滤波的目标是得到$P(x_k|z_{1:k},u_{1:k})$，它的含义是在前k个观测值和控制量的基础上估计第k个状态（状态估计），我们的目标是得到$P(x_k|z_{1:k},u_{1:k})$的递推形式。先来看第一个重要的公式：</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|z_{1:k},u_{1:k})=\\frac{P(z_k|x_k,z_{1:k-1},u_{1:k})P(x_k|z_{1:k-1},u_{1:k})}{P(z_k|z_{1:k-1},u_{1:k})} \\tag{1.1}</script><p>这个公式就是在$z_{1:k-1},u_{1:k}$条件下的贝叶斯公式（这大概就是贝叶斯滤波为什么叫贝叶斯滤波的原因吧）</p>\n<p>接下来讲引入贝叶斯滤波中的基本假设：<strong>Markov性假设: k时刻的状态由k-1时刻的状态和k时刻输入的控制量决定。k时刻的观测仅同k时刻的状态相关。</strong>由此我们得到</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|x_{1:k-1},z_{1:k},u_{1:k})=P(x_k|x_{k-1},u_k) \\\\\nP(z_k|x_k,z_{1:k-1},u_{1:k})=P(z_k|x_k)  \\\\\nP(z_k|z_{1:k-1},u_{1:k})=P(z_k) \\tag{1.2}</script><p><strong>上述假设可以通过图1得到直观的理解。</strong>$P(z_k)$与我们所关心的状态$x_k$无关，所以可以把它当做归一化常数$\\eta_k$</p>\n<p>为了得到递推形式，我们很自然能想到引出上一时刻的状态$x_{k-1}$，只需要简单地运用全概率公式即可：</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|z_{1:k-1},u_{1:k})=\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k})dx_{k-1}\\tag{1.3}</script><p>因为$x_{k-1}$与$u_k$无关，所以式(1.3)修正一下$u$的下标变为</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|z_{1:k-1},u_{1:k})=\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k-1})dx_{k-1}\\tag{1.3.1}</script><p>继而式(1.1)可以化成</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|z_{1:k},u_{1:k})=\\eta_kP(z_k|x_k)\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k-1})dx_{k-1} \\tag{1.4}</script><p>至此，我们得到了贝叶斯滤波的递归形式，之后可能会增加代码实现部分，和对系统模型的进一步说明。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p>1.<a href=\"https://www.cnblogs.com/ycwang16/p/5995702.html\">细说贝叶斯滤波：Bayes filters</a></p>\n<p>2.<a href=\"https://zhuanlan.zhihu.com/p/318701822\">通俗地解释卡尔曼滤波器（一）——从贝叶斯滤波器说起</a></p>\n<p>3.<a href=\"https://zhuanlan.zhihu.com/p/139215491\">贝叶斯滤波（Bayesian filtering)</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Bayes-Filter\"><a href=\"#Bayes-Filter\" class=\"headerlink\" title=\"Bayes Filter\"></a>Bayes Filter</h1><p>关于本文：这是一份简约的入门教程。</p>\n<h2 id=\"滤波是什么\"><a href=\"#滤波是什么\" class=\"headerlink\" title=\"滤波是什么\"></a>滤波是什么</h2><p>滤波是一种方法，手段，在时移（time-varying）系统中，通过对带有噪声的观测${z_k}$ 来估计隐状态$x_k$的处理方法；</p>\n<p>滤波（filtering）专门是指利用前面 $k$个观测 $z_1,z_2,…,z_k$  来估计第$k$个隐状态$x_k$的方法,记作$x_k|z_{1:k}$。</p>\n<p>另外，如果利用前面$d(d&lt;k)$个观测来估计第$k$个状态，则称为预测（prediction），具体地，叫 $k-d$步预测；</p>\n<p>如果利用前面$d$个观测 $z_{1:d}$来估计第$k$个状态，则称为平滑或内插（smoothing)。</p>\n<p>所以其实滤波(filtering)就是 $d=k$ 的情形。</p>\n<h2 id=\"状态、控制、观测\"><a href=\"#状态、控制、观测\" class=\"headerlink\" title=\"状态、控制、观测\"></a>状态、控制、观测</h2><p>但是上述定义并没有考虑系统的控制量，如果考虑控制量，系统模型就如图1所示:</p>\n<p><img src=\"/2021/03/22/Bayes-Filter/Markov.png\" alt=\"Markov\"></p>\n<p>符号说明：$x$为状态量，$z$为观测量，$u$为控制量</p>\n<p>注意，这张图非常重要，它代表着隐马尔可夫模型（HMM），后者是贝叶斯滤波的基础假设，对隐马尔可夫模型的探讨超出了本篇的范畴，这里只做简单介绍。读者可以想象，作为一个普通人（不是全知全能的神明），想要知道一个系统的状态，只能通过测量的手段，再通过滤波等手段得到状态的最优估计。在这里，系统状态$x$就是隐变量，观测值$z$是观测变量。观测得到的是既定的结果，但在它的背后，真实的状态其实是在以一定概率转移的，并且状态本身未必是确定的，它可能是一个范围，有一定的方差（模型噪声）。另一方面，测量所用的仪器存在量测误差（噪声），这个量测误差是随机的，又在观测值背后增加了一层随机性。而我们的目的就是从既定的结果去估计一个随机变量（是不是感觉跟极大似然估计很像？）。</p>\n<h2 id=\"贝叶斯滤波\"><a href=\"#贝叶斯滤波\" class=\"headerlink\" title=\"贝叶斯滤波\"></a>贝叶斯滤波</h2><p>贝叶斯滤波的目标是得到$P(x_k|z_{1:k},u_{1:k})$，它的含义是在前k个观测值和控制量的基础上估计第k个状态（状态估计），我们的目标是得到$P(x_k|z_{1:k},u_{1:k})$的递推形式。先来看第一个重要的公式：</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|z_{1:k},u_{1:k})=\\frac{P(z_k|x_k,z_{1:k-1},u_{1:k})P(x_k|z_{1:k-1},u_{1:k})}{P(z_k|z_{1:k-1},u_{1:k})} \\tag{1.1}</script><p>这个公式就是在$z_{1:k-1},u_{1:k}$条件下的贝叶斯公式（这大概就是贝叶斯滤波为什么叫贝叶斯滤波的原因吧）</p>\n<p>接下来讲引入贝叶斯滤波中的基本假设：<strong>Markov性假设: k时刻的状态由k-1时刻的状态和k时刻输入的控制量决定。k时刻的观测仅同k时刻的状态相关。</strong>由此我们得到</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|x_{1:k-1},z_{1:k},u_{1:k})=P(x_k|x_{k-1},u_k) \\\\\nP(z_k|x_k,z_{1:k-1},u_{1:k})=P(z_k|x_k)  \\\\\nP(z_k|z_{1:k-1},u_{1:k})=P(z_k) \\tag{1.2}</script><p><strong>上述假设可以通过图1得到直观的理解。</strong>$P(z_k)$与我们所关心的状态$x_k$无关，所以可以把它当做归一化常数$\\eta_k$</p>\n<p>为了得到递推形式，我们很自然能想到引出上一时刻的状态$x_{k-1}$，只需要简单地运用全概率公式即可：</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|z_{1:k-1},u_{1:k})=\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k})dx_{k-1}\\tag{1.3}</script><p>因为$x_{k-1}$与$u_k$无关，所以式(1.3)修正一下$u$的下标变为</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|z_{1:k-1},u_{1:k})=\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k-1})dx_{k-1}\\tag{1.3.1}</script><p>继而式(1.1)可以化成</p>\n<script type=\"math/tex; mode=display\">\nP(x_k|z_{1:k},u_{1:k})=\\eta_kP(z_k|x_k)\\int P(x_k|x_{k-1},z_{1:k-1},u_{1:k})P(x_{k-1}|z_{1:k-1},u_{1:k-1})dx_{k-1} \\tag{1.4}</script><p>至此，我们得到了贝叶斯滤波的递归形式，之后可能会增加代码实现部分，和对系统模型的进一步说明。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p>1.<a href=\"https://www.cnblogs.com/ycwang16/p/5995702.html\">细说贝叶斯滤波：Bayes filters</a></p>\n<p>2.<a href=\"https://zhuanlan.zhihu.com/p/318701822\">通俗地解释卡尔曼滤波器（一）——从贝叶斯滤波器说起</a></p>\n<p>3.<a href=\"https://zhuanlan.zhihu.com/p/139215491\">贝叶斯滤波（Bayesian filtering)</a></p>\n"},{"title":"Multivariate Gaussian Distribution","date":"2021-03-24T07:57:31.000Z","mathjax":true,"_content":"\n# 多元高斯分布完全指南\n\n## 引子\n\n先来看一张图：\n\n![图1](./Multivariate-Gaussian-Distribution/1.png)\n\n如果我让你从A、B中选出一个离群点，你觉得谁更合适？从欧式距离来比较，A、B两点到均值（图中的原点）的距离相同，但从直觉上看，B显然更融入群体。这是因为两者在比较时尺度或者说量纲（scalar）没有统一，就像你拿百万面额的津巴布韦币同人民币做比较一样，图中对应的尺度就是维度的方差。当你对每个维度的尺度进行归一化处理后（即让维度上的每个数据除以维度对应的标准差（方差的算数平方根）为什么是平方根？因为我们最终希望修正后的尺度可以简单采用欧式距离，而欧式距离的定义中显然有一个平方根）图就变成下面这样：\n\n![图2](./Multivariate-Gaussian-Distribution/3.jpg)\n\n现在就可以简单地用欧式距离判断谁是离群点了\n\n## 一元高斯分布\n\n如果你学过概率论，那你应该对上面的引子感到熟悉，因为在一元高斯分布中，我们经常会对随机变量$X$进行标准化——$Z=\\frac{X-\\mu}{\\sigma}$，这同之前我们所做的工作相比不能说完全一致，只能说一模一样（XD）。引子就先告一段落，我们很快会再来回顾它。先来复习一下一元高斯分布的有关知识：\n$$\n\\begin{aligned}\np(x) &=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} p(x)dx\n\\end{aligned}\n\\tag{1}\n$$\n令$Z=\\frac{X-\\mu}{\\sigma}$\n$$\n\\begin{aligned}\n\\because x(z) &=z \\cdot \\sigma+\\mu \\\\\n\\therefore p(x(z)) &=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} p(x(z)) d x \\\\\n&=\\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} d x \\\\\n&=\\int_{-\\infty}^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} dz \n\\end{aligned}\n\\tag{2}\n$$\n此时我们说随机变量 $Z \\sim \\mathcal{N}(0,1)$ 服从一元标准高斯分布, 其均值 $\\mu=0,$ 方差 $\\sigma^{2}=1,$ 其概率密度函数为\n$$\np(z)=\\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}}\n\\tag{3}\n$$\n\n## 多元高斯分布\n\n### 1.概率密度函数（PDF）\n\n接下来我们将正式探讨多元情况下的高斯分布，从一元到多元，我们的随机变量也进化成了随机向量，我们用$X=(x_1,x_2,...,x_n)$表示，$x_i$是向量的一个分量，是单个随机变量。我们先从最简单的情况，各个随机变量之间相互独立，开始讨论。\n\n假设我们有随机向量 $\\vec{Z}=\\left[Z_{1}, \\cdots, Z_{n}\\right]^{\\top},$ 其中 $Z_{i} \\sim \\mathcal{N}(0,1)(i=1, \\cdots, n)$ 且 $Z_{i}, Z_{j}(i, j=1, \\cdots, n \\wedge i \\neq j)$ 彼此独立, 即随机向量中的每个随机变量 $Z_{i}$ 都服从标准高斯分布且两两彼此独立. 则由(3)与独立随机变量概率密度函数之间的关系, 我们可得随机向量 $\\vec{Z}=\\left[Z_{1}, \\cdots, Z_{n}\\right]^{\\top}$ 的联合概率密度函数为\n$$\n\\begin{aligned}\np\\left(z_{1}, \\cdots, z_{n}\\right) &=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(z_{i}\\right)^{2}} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(Z^{\\top} Z\\right)} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p\\left(z_{1}, \\cdots, z_{n}\\right) d z_{1} \\cdots d z_{n}\n\\end{aligned}\n\\tag{4}\n$$\n我们称随机向量 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, \\mathbf{I})$, 即随机向量服从均值为零向量, 协方差矩阵为单位矩阵的高斯分布. 在这里, 随机向量 $\\vec{Z}$ 的协方差矩阵是 $\\operatorname{Cov}\\left(Z_{i}, Z_{j}\\right), i, j=1, \\cdots, n$ 组成的矩阵, 即\n$$\n\\begin{aligned}\n\\left[\\operatorname{Cov}\\left(Z_{i}, Z_{j}\\right)\\right]_{n \\times n} &=\\mathbf{E}\\left[(Z-\\vec{\\mu})(Z-\\vec{\\mu})^{\\top}\\right] \\\\\n&=\\mathbf{I}\n\\end{aligned}\n\\tag{5}\n$$\n由于随机向量 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, \\mathbf{I}),$ 所以其协方差矩阵的对角线元素为1, 其余元素为0. 如果我们取常数 $c=p\\left(z_{1}, \\cdots, z_{n}\\right),$ 则可得函数 $p\\left(z_{1}, \\cdots, z_{n}\\right)$ 的等高线为 $c^{\\prime}=Z^{\\top} Z,$ 当随机向量 $\\vec{Z}$ 为二维向量\n时, 我们有\n$$\nc^{\\prime}=Z^{\\top} \\cdot Z=\\left(z_{1}-0\\right)^{2}+\\left(z_{2}-0\\right)^{2}\n\\tag{6}\n$$\n显然，其等高线是以(0，0)为圆心的同心圆。\n\n![图3](./Multivariate-Gaussian-Distribution/4.jpg)\n\n接下来讨论各随机变量不相互独立的一般情况：\n\n既然我们能够轻松地处理独立的情况，那只要我们能够把一般的情况转化成独立的情况，问题是不是就迎刃而解了呢？答案是肯定的，幸运的是，我们有如下定理：\n\n**定理1: 若存在随机向量 $\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma),$ 其中 $\\vec{\\mu} \\in R^{n}$ 为均值向量, $\\Sigma \\in S_{++}^{n \\times n}$ 半正定实对称矩阵为 $\\vec{X}$ 的协方差矩阵, 则存在满秩矩阵 $B \\in R^{n \\times n},$ 使得 $\\vec{Z}=B^{-1}(\\vec{X}-\\vec{\\mu}),$ 而 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0},\\mathbf{I})$.**\n\n（满秩的原因是我们需要求逆）\n\n有了定理1, 我们就可以对随机向量$\\vec{X}$ 做相应的线性变换, 使其随机变量在线性变换后彼此独立, 从而求出其联合概率密度函数, 具体地\n\n\n$$\n\\begin{aligned}\n\\because \\vec{Z}&=B^{-1}(\\vec{X}-\\vec{\\mu}), \\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, I)\\\\\n\\therefore \\quad p\\left(z_{1}, \\cdots, z_{n}\\right) &=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(Z^{\\top} Z\\right)} \\\\\np\\left(z_{1}\\left(x_{1}, \\cdots, x_{n}\\right), \\cdots\\right) &=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)^{\\top}\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)\\right]} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} \n\\end{aligned}\n\\tag{7}\n$$\n$$\n\\begin{aligned}\n\\therefore \\quad 1 &=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p\\left(z_{1}\\left(x_{1}, \\cdots, x_{n}\\right), \\cdots\\right) d z_{1} \\cdots d z_{n} \\\\\n&=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} \\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} d z_{1} \\cdots d z_{n}\n\\end{aligned}\n\\tag{8}\n$$\n\n\n\n由多元函数换元变换公式, 我们还需要求出雅可比行列式 $J(\\vec{Z} \\rightarrow \\vec{X}),$ 由(7)可得\n$$\nJ(\\vec{Z} \\rightarrow \\vec{X})=\\left|B^{-1}\\right|=|B|^{-1}=|B|^{-\\frac{1}{2}} \\cdot\\left|B^{\\top}\\right|^{-\\frac{1}{2}}=\\left|B B^{\\top}\\right|^{-\\frac{1}{2}}\n\\tag{9}\n$$\n由(8)(9), 我们可进一步得\n$$\n1=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} \\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|B B^{\\top}\\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} d x_{1} \\cdots d x_{n}\n\\tag{10}\n$$\n我们得到随机向量$\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma)$ 的联合概率密度函数为\n$$\np\\left(x_{1}, \\cdots, x_{n}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|B B^{\\top}\\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]}\n\\tag{11}\n$$\n在(11)中, 随机向量$\\vec{X}$ 的协方差矩阵还未得到体现, 我们可通过线性变换(7)做进一步处理\n$$\n\\begin{aligned}\n\\Sigma &=\\mathbf{E}[(\\vec{X}-\\vec{\\mu})(\\vec{X}-\\vec{\\mu})^{\\top}] \\\\\n&=\\mathbf{E}[(B \\vec{Z}-\\overrightarrow{0})(B \\vec{Z}-\\overrightarrow{0})^{\\top}] \\\\\n&=\\operatorname{Var}(B \\vec{Z}) \\\\\n&=B \\operatorname{Var}(\\vec{Z}) B^{\\top} \\\\\n&=B B^{\\top}\n\\end{aligned}\n\\tag{12}\n$$\n>  常用结论：$E(A\\vec{Z}+B)=AE(\\vec{Z})+B\\\\Var(A\\vec{Z}+B)=AVar(\\vec{Z})A^{\\top}$\n\n我们发现, (11)中 $B B^{\\top}$ 就是线性变换前的随机向量 $\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma)$ 的协方差矩阵 $\\Sigma$, 所以由(11)(12), 我们可以得到联合概率密度函数的最终形式\n$$\np\\left(x_{1}, \\cdots, x_{n}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\cdot e^{.-\\frac{1}{2} \\cdot [(\\vec{X}-\\vec{\\mu})^{\\top} \\Sigma^{-1}(\\vec{X}-\\vec{\\mu})]}\n\\tag{13}\n$$\n这就是多元高斯分布的概率密度函数($pdf$)，非常重要，建议刻进DNA里。\n\n### 2.马氏距离与几何意义\n\n接下来，我们将引入一个广泛被使用的距离度量——马氏距离(Mahalanobis Distance)。单个数据点的马氏距离：$D_{M}(x)=\\sqrt{(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)}$\n\n是不是很眼熟，没错，其形式与我们刚刚讨论完的多元高斯分布概率密度函数的指数部分完全一致！接下来让我们来看看这个形式究竟有什么几何意义:\n\n由定理1我们有\n$$\n\\begin{aligned}\n\\because \\quad \\vec{Z} &=B^{-1}(\\vec{X}-\\vec{\\mu}), \\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, I) \\\\\n\\therefore \\quad \\vec{Z}^{\\top} \\vec{Z} &=\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)^{\\top}\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right) \\\\\n&=(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu}) \\\\\n&=(\\vec{X}-\\vec{\\mu})^{\\top} \\Sigma^{-1}(\\vec{X}-\\vec{\\mu})\n\\end{aligned}\n\\tag{14}\n$$\n再由(12)(14)可得\n$$\n\\begin{aligned}\n\\vec{Z}^{\\top} \\vec{Z} &=\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top} \\Lambda^{-1}\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left(\\Lambda^{-\\frac{1}{2}}\\right)^{\\top} \\Lambda^{-\\frac{1}{2}}\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\Lambda^{-\\frac{1}{2}} Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left[\\Lambda^{-\\frac{1}{2}} Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\left(Q \\Lambda^{-\\frac{1}{2}}\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left[\\left(Q \\Lambda^{-\\frac{1}{2}}\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\left(Q \\cdot\\left[\\begin{array}{cc}\n\\frac{1}{\\sqrt{\\lambda_{1}}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{\\lambda_{2}}}\n\\end{array}\\right]\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top} \\\\\n& \\cdot\\left[\\left(Q \\cdot\\left[\\begin{array}{cc}\n\\frac{1}{\\sqrt{\\lambda_{1}}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{\\lambda_{2}}}\n\\end{array}\\right]\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]\n\\end{aligned}\n\\tag{15}\n$$\n由(15)我们已经可以非常明显地看出线性变换 $\\vec{Z}=B^{-1}(\\vec{X}-\\vec{\\mu})$ 的具体操作了\n\n![图4](./Multivariate-Gaussian-Distribution/5.png)\n\n上述公式看起来繁琐，实际上只是简单地对协方差矩阵$\\Sigma$做了特征分解（协方差矩阵对称半正定）$\\lambda$就是特征值。\n\n回顾一下本篇一开始的引子，由特征值组成的对角阵的作用就是对维度进行拉伸或者说放缩，就是让维度上的数据除以对应的标准差，显然，作为对角阵的协方差矩阵的特征值就是对应维度的方差，两者在此得到统一。接下来看矩阵$Q$的作用，我们继续顺着引子的思路探索下去，统一了维度间的尺度或量纲就高枕无忧了吗，来看一种情况：\n![图5](./Multivariate-Gaussian-Distribution/2.jpg)\n\nA与B的欧式距离相同，各维度的尺度也得到了统一，但直觉告诉我们，B点显然更合群，为什么？因为维度之间并不是独立的，图中两个维度线性相关，为了修正这种情况，我们需要去相关化，而这就是\n\n矩阵$Q$所做的工作。如果维度间保持独立，即协方差矩阵是对角阵，那么$Q$就是单位矩阵$I$。这个时候，等高线的图案是一个标准的椭圆（为什么是椭圆而不是圆，因为特征值对维度进行了拉伸，形成了椭圆的实轴与虚轴）等高线的公式如下,二维情况下就是椭圆：\n$$\n\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits _{i=1}^{p}(x-\\mu)^{T}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits _{i=1}^{p}\\frac{y_{i}^{2}}{\\lambda_{i}}\n\\tag{16}\n$$\n\n\n![图6](./Multivariate-Gaussian-Distribution/7.jpg)\n\n一般情况下，$Q$则是由特征向量堆叠而成的正交矩阵（如果感到陌生的化可以复习一下线代），而等高线变成一个倾斜的椭圆：\n\n![图7](./Multivariate-Gaussian-Distribution/6.jpg)\n\n你可以把$Q$理解成旋转矩阵，对标准的椭圆进行旋转（因为旋转矩阵非常普遍与实用，有机会的话我会单独拿出来讲一讲）。也可以认为椭圆其实没变，只是基变了，或者说参考的坐标轴变了，不再是标准正交基，而变成了$Q$中的特征向量组成的单位正交基，在新的坐标轴下（图中的$u_{x}$,$u_{y}$），椭圆依然是标准的，维度依然是相互独立的。\n\n总结一下：马氏距离就是对原维度进行**尺度归一化加去相关性**，在新的维度下，数据之间的度量就可以简单地用欧氏距离表示。而我们在推导高斯分布的概率密度函数时所用的矩阵$B^{-1}$代表的线性变换也是**拉伸与去相关性**（如果你对此还有疑惑，可以回顾一下图4），使得一般的情况转化为简单的独立情况，方便我们处理。这种转换的思想非常重要，在很多方面都有精彩的运用，比如把非线性问题转换成线性问题来处理。\n\n### 3.条件概率与边缘概率\n\n接下来我们将要探索多元高斯的条件概率分布与边缘概率分布，这一块在统计推断与机器学习中有重要应用，但因为推导的过程比较复杂，不要求完全掌握推导过程，只要混个眼熟就行。我们记 $x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}$，已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma)$。\n\n我们的目标是得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。\n\n先来推边缘概率，首先放一个引理：\n\n**定理2：已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b$，那么 $y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)$。$\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T$。**\n\n（这个定理应该很符合直觉，也非常容易记忆）\n\n我们可以把随机向量$X$分成两部分$\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}$，$x_a$为m维列向量，$x_b$为n维列向量。现在我们想要求出$x_a$服从的概率分布（利用对称性可以推出$x_b$）直觉上猜测可能服从某一高斯分布，事实上也确实如此（高斯分布的优良性质）。我们可以构造出如下等式：\n$$\nx_a=\\underbrace{\\begin{pmatrix}\\mathbb{I}_{m\\times m}&\\mathbb{O}_{m\\times n}\\end{pmatrix}}_{相当于A}\\underbrace{\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}}_{X}\n\\tag{17}\n$$\n\n\n利用引理可以得到：\n$$\n\\begin{align}\n&\\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_a\\\\\n&Var[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa}\\\\\n& x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})\n\\end{align}\n\\tag{18}\n$$\n同理可得$x_b$的情况，这里就不赘述了。\n\n接下来推条件概率（相关的推导方式不唯一，这里我们选择一种比较有技巧性的构造推导）\n\n我们定义3个符号量\n$$\nx_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n\\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\\\\n\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n\\tag{19}\n$$\n需要注意的是，**3个符号量中只有第一个才是我们定义的（后续将在它的基础上进行推导），后两个都是在第一个基础上推出来的，只是先在这里列出来**。$x_{b\\cdot a}$的具体定义自于技巧性的构造，但我们简单地观察一下形式可以发现，一旦我们得到了$x_{b\\cdot a}$服从的分布的信息，我们就能建立$x_a$与$x_b$之间的联系，继而推出$p(x_b|x_a)$。根据$x_{b\\cdot a}$的定义，我们构造如下等式：\n$$\nx_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\n\\tag{20}\n$$\n利用引理可得：\n$$\n\\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_{b\\cdot a}\\\\\nVar[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\\\mathbb{I}_{n\\times n}\\end{pmatrix}=\\Sigma_{bb\\cdot a}\n\\tag{21}\n$$\n我们对$x_{b\\cdot a}$的定义式（式（19））做个小变形得到\n$$\nx_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a \\tag{21}\n$$\n再次利用引理可得\n$$\n\\begin{align}\n&\\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n&Var[x_b|x_a]=\\Sigma_{bb\\cdot a}\\\\\n&x_b|x_a\\sim\\mathcal{N}(\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a,\\Sigma_{bb\\cdot a})\n\\end{align}\n\\tag{22}\n$$\n$x_b$同理可得（简单交换ab次序就行）\n\n这里再多提一点\n$$\nVar[x_b|x_a]=\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n$$\n其形式就是线性代数中的舒尔补（Schur complement），感兴趣的读者可以自行Google。\n\n## 参考资料\n\n1.[多元高斯分布完全解析](https://zhuanlan.zhihu.com/p/58987388)\n\n2.[马氏距离(Mahalanobis Distance)](https://zhuanlan.zhihu.com/p/46626607)\n\n3.机器学习-白板推导系列","source":"_posts/Multivariate-Gaussian-Distribution.md","raw":"---\ntitle: Multivariate Gaussian Distribution\ndate: 2021-03-24 15:57:31\ntags: Gaussian\nmathjax: true\n---\n\n# 多元高斯分布完全指南\n\n## 引子\n\n先来看一张图：\n\n![图1](./Multivariate-Gaussian-Distribution/1.png)\n\n如果我让你从A、B中选出一个离群点，你觉得谁更合适？从欧式距离来比较，A、B两点到均值（图中的原点）的距离相同，但从直觉上看，B显然更融入群体。这是因为两者在比较时尺度或者说量纲（scalar）没有统一，就像你拿百万面额的津巴布韦币同人民币做比较一样，图中对应的尺度就是维度的方差。当你对每个维度的尺度进行归一化处理后（即让维度上的每个数据除以维度对应的标准差（方差的算数平方根）为什么是平方根？因为我们最终希望修正后的尺度可以简单采用欧式距离，而欧式距离的定义中显然有一个平方根）图就变成下面这样：\n\n![图2](./Multivariate-Gaussian-Distribution/3.jpg)\n\n现在就可以简单地用欧式距离判断谁是离群点了\n\n## 一元高斯分布\n\n如果你学过概率论，那你应该对上面的引子感到熟悉，因为在一元高斯分布中，我们经常会对随机变量$X$进行标准化——$Z=\\frac{X-\\mu}{\\sigma}$，这同之前我们所做的工作相比不能说完全一致，只能说一模一样（XD）。引子就先告一段落，我们很快会再来回顾它。先来复习一下一元高斯分布的有关知识：\n$$\n\\begin{aligned}\np(x) &=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} p(x)dx\n\\end{aligned}\n\\tag{1}\n$$\n令$Z=\\frac{X-\\mu}{\\sigma}$\n$$\n\\begin{aligned}\n\\because x(z) &=z \\cdot \\sigma+\\mu \\\\\n\\therefore p(x(z)) &=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} p(x(z)) d x \\\\\n&=\\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} d x \\\\\n&=\\int_{-\\infty}^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} dz \n\\end{aligned}\n\\tag{2}\n$$\n此时我们说随机变量 $Z \\sim \\mathcal{N}(0,1)$ 服从一元标准高斯分布, 其均值 $\\mu=0,$ 方差 $\\sigma^{2}=1,$ 其概率密度函数为\n$$\np(z)=\\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}}\n\\tag{3}\n$$\n\n## 多元高斯分布\n\n### 1.概率密度函数（PDF）\n\n接下来我们将正式探讨多元情况下的高斯分布，从一元到多元，我们的随机变量也进化成了随机向量，我们用$X=(x_1,x_2,...,x_n)$表示，$x_i$是向量的一个分量，是单个随机变量。我们先从最简单的情况，各个随机变量之间相互独立，开始讨论。\n\n假设我们有随机向量 $\\vec{Z}=\\left[Z_{1}, \\cdots, Z_{n}\\right]^{\\top},$ 其中 $Z_{i} \\sim \\mathcal{N}(0,1)(i=1, \\cdots, n)$ 且 $Z_{i}, Z_{j}(i, j=1, \\cdots, n \\wedge i \\neq j)$ 彼此独立, 即随机向量中的每个随机变量 $Z_{i}$ 都服从标准高斯分布且两两彼此独立. 则由(3)与独立随机变量概率密度函数之间的关系, 我们可得随机向量 $\\vec{Z}=\\left[Z_{1}, \\cdots, Z_{n}\\right]^{\\top}$ 的联合概率密度函数为\n$$\n\\begin{aligned}\np\\left(z_{1}, \\cdots, z_{n}\\right) &=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(z_{i}\\right)^{2}} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(Z^{\\top} Z\\right)} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p\\left(z_{1}, \\cdots, z_{n}\\right) d z_{1} \\cdots d z_{n}\n\\end{aligned}\n\\tag{4}\n$$\n我们称随机向量 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, \\mathbf{I})$, 即随机向量服从均值为零向量, 协方差矩阵为单位矩阵的高斯分布. 在这里, 随机向量 $\\vec{Z}$ 的协方差矩阵是 $\\operatorname{Cov}\\left(Z_{i}, Z_{j}\\right), i, j=1, \\cdots, n$ 组成的矩阵, 即\n$$\n\\begin{aligned}\n\\left[\\operatorname{Cov}\\left(Z_{i}, Z_{j}\\right)\\right]_{n \\times n} &=\\mathbf{E}\\left[(Z-\\vec{\\mu})(Z-\\vec{\\mu})^{\\top}\\right] \\\\\n&=\\mathbf{I}\n\\end{aligned}\n\\tag{5}\n$$\n由于随机向量 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, \\mathbf{I}),$ 所以其协方差矩阵的对角线元素为1, 其余元素为0. 如果我们取常数 $c=p\\left(z_{1}, \\cdots, z_{n}\\right),$ 则可得函数 $p\\left(z_{1}, \\cdots, z_{n}\\right)$ 的等高线为 $c^{\\prime}=Z^{\\top} Z,$ 当随机向量 $\\vec{Z}$ 为二维向量\n时, 我们有\n$$\nc^{\\prime}=Z^{\\top} \\cdot Z=\\left(z_{1}-0\\right)^{2}+\\left(z_{2}-0\\right)^{2}\n\\tag{6}\n$$\n显然，其等高线是以(0，0)为圆心的同心圆。\n\n![图3](./Multivariate-Gaussian-Distribution/4.jpg)\n\n接下来讨论各随机变量不相互独立的一般情况：\n\n既然我们能够轻松地处理独立的情况，那只要我们能够把一般的情况转化成独立的情况，问题是不是就迎刃而解了呢？答案是肯定的，幸运的是，我们有如下定理：\n\n**定理1: 若存在随机向量 $\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma),$ 其中 $\\vec{\\mu} \\in R^{n}$ 为均值向量, $\\Sigma \\in S_{++}^{n \\times n}$ 半正定实对称矩阵为 $\\vec{X}$ 的协方差矩阵, 则存在满秩矩阵 $B \\in R^{n \\times n},$ 使得 $\\vec{Z}=B^{-1}(\\vec{X}-\\vec{\\mu}),$ 而 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0},\\mathbf{I})$.**\n\n（满秩的原因是我们需要求逆）\n\n有了定理1, 我们就可以对随机向量$\\vec{X}$ 做相应的线性变换, 使其随机变量在线性变换后彼此独立, 从而求出其联合概率密度函数, 具体地\n\n\n$$\n\\begin{aligned}\n\\because \\vec{Z}&=B^{-1}(\\vec{X}-\\vec{\\mu}), \\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, I)\\\\\n\\therefore \\quad p\\left(z_{1}, \\cdots, z_{n}\\right) &=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(Z^{\\top} Z\\right)} \\\\\np\\left(z_{1}\\left(x_{1}, \\cdots, x_{n}\\right), \\cdots\\right) &=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)^{\\top}\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)\\right]} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} \n\\end{aligned}\n\\tag{7}\n$$\n$$\n\\begin{aligned}\n\\therefore \\quad 1 &=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p\\left(z_{1}\\left(x_{1}, \\cdots, x_{n}\\right), \\cdots\\right) d z_{1} \\cdots d z_{n} \\\\\n&=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} \\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} d z_{1} \\cdots d z_{n}\n\\end{aligned}\n\\tag{8}\n$$\n\n\n\n由多元函数换元变换公式, 我们还需要求出雅可比行列式 $J(\\vec{Z} \\rightarrow \\vec{X}),$ 由(7)可得\n$$\nJ(\\vec{Z} \\rightarrow \\vec{X})=\\left|B^{-1}\\right|=|B|^{-1}=|B|^{-\\frac{1}{2}} \\cdot\\left|B^{\\top}\\right|^{-\\frac{1}{2}}=\\left|B B^{\\top}\\right|^{-\\frac{1}{2}}\n\\tag{9}\n$$\n由(8)(9), 我们可进一步得\n$$\n1=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} \\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|B B^{\\top}\\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} d x_{1} \\cdots d x_{n}\n\\tag{10}\n$$\n我们得到随机向量$\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma)$ 的联合概率密度函数为\n$$\np\\left(x_{1}, \\cdots, x_{n}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|B B^{\\top}\\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]}\n\\tag{11}\n$$\n在(11)中, 随机向量$\\vec{X}$ 的协方差矩阵还未得到体现, 我们可通过线性变换(7)做进一步处理\n$$\n\\begin{aligned}\n\\Sigma &=\\mathbf{E}[(\\vec{X}-\\vec{\\mu})(\\vec{X}-\\vec{\\mu})^{\\top}] \\\\\n&=\\mathbf{E}[(B \\vec{Z}-\\overrightarrow{0})(B \\vec{Z}-\\overrightarrow{0})^{\\top}] \\\\\n&=\\operatorname{Var}(B \\vec{Z}) \\\\\n&=B \\operatorname{Var}(\\vec{Z}) B^{\\top} \\\\\n&=B B^{\\top}\n\\end{aligned}\n\\tag{12}\n$$\n>  常用结论：$E(A\\vec{Z}+B)=AE(\\vec{Z})+B\\\\Var(A\\vec{Z}+B)=AVar(\\vec{Z})A^{\\top}$\n\n我们发现, (11)中 $B B^{\\top}$ 就是线性变换前的随机向量 $\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma)$ 的协方差矩阵 $\\Sigma$, 所以由(11)(12), 我们可以得到联合概率密度函数的最终形式\n$$\np\\left(x_{1}, \\cdots, x_{n}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\cdot e^{.-\\frac{1}{2} \\cdot [(\\vec{X}-\\vec{\\mu})^{\\top} \\Sigma^{-1}(\\vec{X}-\\vec{\\mu})]}\n\\tag{13}\n$$\n这就是多元高斯分布的概率密度函数($pdf$)，非常重要，建议刻进DNA里。\n\n### 2.马氏距离与几何意义\n\n接下来，我们将引入一个广泛被使用的距离度量——马氏距离(Mahalanobis Distance)。单个数据点的马氏距离：$D_{M}(x)=\\sqrt{(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)}$\n\n是不是很眼熟，没错，其形式与我们刚刚讨论完的多元高斯分布概率密度函数的指数部分完全一致！接下来让我们来看看这个形式究竟有什么几何意义:\n\n由定理1我们有\n$$\n\\begin{aligned}\n\\because \\quad \\vec{Z} &=B^{-1}(\\vec{X}-\\vec{\\mu}), \\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, I) \\\\\n\\therefore \\quad \\vec{Z}^{\\top} \\vec{Z} &=\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)^{\\top}\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right) \\\\\n&=(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu}) \\\\\n&=(\\vec{X}-\\vec{\\mu})^{\\top} \\Sigma^{-1}(\\vec{X}-\\vec{\\mu})\n\\end{aligned}\n\\tag{14}\n$$\n再由(12)(14)可得\n$$\n\\begin{aligned}\n\\vec{Z}^{\\top} \\vec{Z} &=\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top} \\Lambda^{-1}\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left(\\Lambda^{-\\frac{1}{2}}\\right)^{\\top} \\Lambda^{-\\frac{1}{2}}\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\Lambda^{-\\frac{1}{2}} Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left[\\Lambda^{-\\frac{1}{2}} Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\left(Q \\Lambda^{-\\frac{1}{2}}\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left[\\left(Q \\Lambda^{-\\frac{1}{2}}\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\left(Q \\cdot\\left[\\begin{array}{cc}\n\\frac{1}{\\sqrt{\\lambda_{1}}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{\\lambda_{2}}}\n\\end{array}\\right]\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top} \\\\\n& \\cdot\\left[\\left(Q \\cdot\\left[\\begin{array}{cc}\n\\frac{1}{\\sqrt{\\lambda_{1}}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{\\lambda_{2}}}\n\\end{array}\\right]\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]\n\\end{aligned}\n\\tag{15}\n$$\n由(15)我们已经可以非常明显地看出线性变换 $\\vec{Z}=B^{-1}(\\vec{X}-\\vec{\\mu})$ 的具体操作了\n\n![图4](./Multivariate-Gaussian-Distribution/5.png)\n\n上述公式看起来繁琐，实际上只是简单地对协方差矩阵$\\Sigma$做了特征分解（协方差矩阵对称半正定）$\\lambda$就是特征值。\n\n回顾一下本篇一开始的引子，由特征值组成的对角阵的作用就是对维度进行拉伸或者说放缩，就是让维度上的数据除以对应的标准差，显然，作为对角阵的协方差矩阵的特征值就是对应维度的方差，两者在此得到统一。接下来看矩阵$Q$的作用，我们继续顺着引子的思路探索下去，统一了维度间的尺度或量纲就高枕无忧了吗，来看一种情况：\n![图5](./Multivariate-Gaussian-Distribution/2.jpg)\n\nA与B的欧式距离相同，各维度的尺度也得到了统一，但直觉告诉我们，B点显然更合群，为什么？因为维度之间并不是独立的，图中两个维度线性相关，为了修正这种情况，我们需要去相关化，而这就是\n\n矩阵$Q$所做的工作。如果维度间保持独立，即协方差矩阵是对角阵，那么$Q$就是单位矩阵$I$。这个时候，等高线的图案是一个标准的椭圆（为什么是椭圆而不是圆，因为特征值对维度进行了拉伸，形成了椭圆的实轴与虚轴）等高线的公式如下,二维情况下就是椭圆：\n$$\n\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits _{i=1}^{p}(x-\\mu)^{T}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits _{i=1}^{p}\\frac{y_{i}^{2}}{\\lambda_{i}}\n\\tag{16}\n$$\n\n\n![图6](./Multivariate-Gaussian-Distribution/7.jpg)\n\n一般情况下，$Q$则是由特征向量堆叠而成的正交矩阵（如果感到陌生的化可以复习一下线代），而等高线变成一个倾斜的椭圆：\n\n![图7](./Multivariate-Gaussian-Distribution/6.jpg)\n\n你可以把$Q$理解成旋转矩阵，对标准的椭圆进行旋转（因为旋转矩阵非常普遍与实用，有机会的话我会单独拿出来讲一讲）。也可以认为椭圆其实没变，只是基变了，或者说参考的坐标轴变了，不再是标准正交基，而变成了$Q$中的特征向量组成的单位正交基，在新的坐标轴下（图中的$u_{x}$,$u_{y}$），椭圆依然是标准的，维度依然是相互独立的。\n\n总结一下：马氏距离就是对原维度进行**尺度归一化加去相关性**，在新的维度下，数据之间的度量就可以简单地用欧氏距离表示。而我们在推导高斯分布的概率密度函数时所用的矩阵$B^{-1}$代表的线性变换也是**拉伸与去相关性**（如果你对此还有疑惑，可以回顾一下图4），使得一般的情况转化为简单的独立情况，方便我们处理。这种转换的思想非常重要，在很多方面都有精彩的运用，比如把非线性问题转换成线性问题来处理。\n\n### 3.条件概率与边缘概率\n\n接下来我们将要探索多元高斯的条件概率分布与边缘概率分布，这一块在统计推断与机器学习中有重要应用，但因为推导的过程比较复杂，不要求完全掌握推导过程，只要混个眼熟就行。我们记 $x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}$，已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma)$。\n\n我们的目标是得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。\n\n先来推边缘概率，首先放一个引理：\n\n**定理2：已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b$，那么 $y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)$。$\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T$。**\n\n（这个定理应该很符合直觉，也非常容易记忆）\n\n我们可以把随机向量$X$分成两部分$\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}$，$x_a$为m维列向量，$x_b$为n维列向量。现在我们想要求出$x_a$服从的概率分布（利用对称性可以推出$x_b$）直觉上猜测可能服从某一高斯分布，事实上也确实如此（高斯分布的优良性质）。我们可以构造出如下等式：\n$$\nx_a=\\underbrace{\\begin{pmatrix}\\mathbb{I}_{m\\times m}&\\mathbb{O}_{m\\times n}\\end{pmatrix}}_{相当于A}\\underbrace{\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}}_{X}\n\\tag{17}\n$$\n\n\n利用引理可以得到：\n$$\n\\begin{align}\n&\\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_a\\\\\n&Var[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa}\\\\\n& x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})\n\\end{align}\n\\tag{18}\n$$\n同理可得$x_b$的情况，这里就不赘述了。\n\n接下来推条件概率（相关的推导方式不唯一，这里我们选择一种比较有技巧性的构造推导）\n\n我们定义3个符号量\n$$\nx_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n\\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\\\\n\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n\\tag{19}\n$$\n需要注意的是，**3个符号量中只有第一个才是我们定义的（后续将在它的基础上进行推导），后两个都是在第一个基础上推出来的，只是先在这里列出来**。$x_{b\\cdot a}$的具体定义自于技巧性的构造，但我们简单地观察一下形式可以发现，一旦我们得到了$x_{b\\cdot a}$服从的分布的信息，我们就能建立$x_a$与$x_b$之间的联系，继而推出$p(x_b|x_a)$。根据$x_{b\\cdot a}$的定义，我们构造如下等式：\n$$\nx_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\n\\tag{20}\n$$\n利用引理可得：\n$$\n\\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_{b\\cdot a}\\\\\nVar[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\\\mathbb{I}_{n\\times n}\\end{pmatrix}=\\Sigma_{bb\\cdot a}\n\\tag{21}\n$$\n我们对$x_{b\\cdot a}$的定义式（式（19））做个小变形得到\n$$\nx_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a \\tag{21}\n$$\n再次利用引理可得\n$$\n\\begin{align}\n&\\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n&Var[x_b|x_a]=\\Sigma_{bb\\cdot a}\\\\\n&x_b|x_a\\sim\\mathcal{N}(\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a,\\Sigma_{bb\\cdot a})\n\\end{align}\n\\tag{22}\n$$\n$x_b$同理可得（简单交换ab次序就行）\n\n这里再多提一点\n$$\nVar[x_b|x_a]=\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n$$\n其形式就是线性代数中的舒尔补（Schur complement），感兴趣的读者可以自行Google。\n\n## 参考资料\n\n1.[多元高斯分布完全解析](https://zhuanlan.zhihu.com/p/58987388)\n\n2.[马氏距离(Mahalanobis Distance)](https://zhuanlan.zhihu.com/p/46626607)\n\n3.机器学习-白板推导系列","slug":"Multivariate-Gaussian-Distribution","published":1,"updated":"2021-03-27T12:53:12.635Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmrro94m0003xnac2x6ibyje","content":"<h1 id=\"多元高斯分布完全指南\"><a href=\"#多元高斯分布完全指南\" class=\"headerlink\" title=\"多元高斯分布完全指南\"></a>多元高斯分布完全指南</h1><h2 id=\"引子\"><a href=\"#引子\" class=\"headerlink\" title=\"引子\"></a>引子</h2><p>先来看一张图：</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/1.png\" alt=\"图1\"></p>\n<p>如果我让你从A、B中选出一个离群点，你觉得谁更合适？从欧式距离来比较，A、B两点到均值（图中的原点）的距离相同，但从直觉上看，B显然更融入群体。这是因为两者在比较时尺度或者说量纲（scalar）没有统一，就像你拿百万面额的津巴布韦币同人民币做比较一样，图中对应的尺度就是维度的方差。当你对每个维度的尺度进行归一化处理后（即让维度上的每个数据除以维度对应的标准差（方差的算数平方根）为什么是平方根？因为我们最终希望修正后的尺度可以简单采用欧式距离，而欧式距离的定义中显然有一个平方根）图就变成下面这样：</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/3.jpg\" alt=\"图2\"></p>\n<p>现在就可以简单地用欧式距离判断谁是离群点了</p>\n<h2 id=\"一元高斯分布\"><a href=\"#一元高斯分布\" class=\"headerlink\" title=\"一元高斯分布\"></a>一元高斯分布</h2><p>如果你学过概率论，那你应该对上面的引子感到熟悉，因为在一元高斯分布中，我们经常会对随机变量$X$进行标准化——$Z=\\frac{X-\\mu}{\\sigma}$，这同之前我们所做的工作相比不能说完全一致，只能说一模一样（XD）。引子就先告一段落，我们很快会再来回顾它。先来复习一下一元高斯分布的有关知识：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(x) &=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} p(x)dx\n\\end{aligned}\n\\tag{1}</script><p>令$Z=\\frac{X-\\mu}{\\sigma}$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\because x(z) &=z \\cdot \\sigma+\\mu \\\\\n\\therefore p(x(z)) &=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} p(x(z)) d x \\\\\n&=\\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} d x \\\\\n&=\\int_{-\\infty}^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} dz \n\\end{aligned}\n\\tag{2}</script><p>此时我们说随机变量 $Z \\sim \\mathcal{N}(0,1)$ 服从一元标准高斯分布, 其均值 $\\mu=0,$ 方差 $\\sigma^{2}=1,$ 其概率密度函数为</p>\n<script type=\"math/tex; mode=display\">\np(z)=\\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}}\n\\tag{3}</script><h2 id=\"多元高斯分布\"><a href=\"#多元高斯分布\" class=\"headerlink\" title=\"多元高斯分布\"></a>多元高斯分布</h2><h3 id=\"1-概率密度函数（PDF）\"><a href=\"#1-概率密度函数（PDF）\" class=\"headerlink\" title=\"1.概率密度函数（PDF）\"></a>1.概率密度函数（PDF）</h3><p>接下来我们将正式探讨多元情况下的高斯分布，从一元到多元，我们的随机变量也进化成了随机向量，我们用$X=(x_1,x_2,…,x_n)$表示，$x_i$是向量的一个分量，是单个随机变量。我们先从最简单的情况，各个随机变量之间相互独立，开始讨论。</p>\n<p>假设我们有随机向量 $\\vec{Z}=\\left[Z_{1}, \\cdots, Z_{n}\\right]^{\\top},$ 其中 $Z_{i} \\sim \\mathcal{N}(0,1)(i=1, \\cdots, n)$ 且 $Z_{i}, Z_{j}(i, j=1, \\cdots, n \\wedge i \\neq j)$ 彼此独立, 即随机向量中的每个随机变量 $Z_{i}$ 都服从标准高斯分布且两两彼此独立. 则由(3)与独立随机变量概率密度函数之间的关系, 我们可得随机向量 $\\vec{Z}=\\left[Z_{1}, \\cdots, Z_{n}\\right]^{\\top}$ 的联合概率密度函数为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np\\left(z_{1}, \\cdots, z_{n}\\right) &=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(z_{i}\\right)^{2}} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(Z^{\\top} Z\\right)} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p\\left(z_{1}, \\cdots, z_{n}\\right) d z_{1} \\cdots d z_{n}\n\\end{aligned}\n\\tag{4}</script><p>我们称随机向量 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, \\mathbf{I})$, 即随机向量服从均值为零向量, 协方差矩阵为单位矩阵的高斯分布. 在这里, 随机向量 $\\vec{Z}$ 的协方差矩阵是 $\\operatorname{Cov}\\left(Z_{i}, Z_{j}\\right), i, j=1, \\cdots, n$ 组成的矩阵, 即</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\left[\\operatorname{Cov}\\left(Z_{i}, Z_{j}\\right)\\right]_{n \\times n} &=\\mathbf{E}\\left[(Z-\\vec{\\mu})(Z-\\vec{\\mu})^{\\top}\\right] \\\\\n&=\\mathbf{I}\n\\end{aligned}\n\\tag{5}</script><p>由于随机向量 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, \\mathbf{I}),$ 所以其协方差矩阵的对角线元素为1, 其余元素为0. 如果我们取常数 $c=p\\left(z_{1}, \\cdots, z_{n}\\right),$ 则可得函数 $p\\left(z_{1}, \\cdots, z_{n}\\right)$ 的等高线为 $c^{\\prime}=Z^{\\top} Z,$ 当随机向量 $\\vec{Z}$ 为二维向量<br>时, 我们有</p>\n<script type=\"math/tex; mode=display\">\nc^{\\prime}=Z^{\\top} \\cdot Z=\\left(z_{1}-0\\right)^{2}+\\left(z_{2}-0\\right)^{2}\n\\tag{6}</script><p>显然，其等高线是以(0，0)为圆心的同心圆。</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/4.jpg\" alt=\"图3\"></p>\n<p>接下来讨论各随机变量不相互独立的一般情况：</p>\n<p>既然我们能够轻松地处理独立的情况，那只要我们能够把一般的情况转化成独立的情况，问题是不是就迎刃而解了呢？答案是肯定的，幸运的是，我们有如下定理：</p>\n<p><strong>定理1: 若存在随机向量 $\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma),$ 其中 $\\vec{\\mu} \\in R^{n}$ 为均值向量, $\\Sigma \\in S_{++}^{n \\times n}$ 半正定实对称矩阵为 $\\vec{X}$ 的协方差矩阵, 则存在满秩矩阵 $B \\in R^{n \\times n},$ 使得 $\\vec{Z}=B^{-1}(\\vec{X}-\\vec{\\mu}),$ 而 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0},\\mathbf{I})$.</strong></p>\n<p>（满秩的原因是我们需要求逆）</p>\n<p>有了定理1, 我们就可以对随机向量$\\vec{X}$ 做相应的线性变换, 使其随机变量在线性变换后彼此独立, 从而求出其联合概率密度函数, 具体地</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\because \\vec{Z}&=B^{-1}(\\vec{X}-\\vec{\\mu}), \\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, I)\\\\\n\\therefore \\quad p\\left(z_{1}, \\cdots, z_{n}\\right) &=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(Z^{\\top} Z\\right)} \\\\\np\\left(z_{1}\\left(x_{1}, \\cdots, x_{n}\\right), \\cdots\\right) &=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)^{\\top}\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)\\right]} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} \n\\end{aligned}\n\\tag{7}</script><script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\therefore \\quad 1 &=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p\\left(z_{1}\\left(x_{1}, \\cdots, x_{n}\\right), \\cdots\\right) d z_{1} \\cdots d z_{n} \\\\\n&=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} \\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} d z_{1} \\cdots d z_{n}\n\\end{aligned}\n\\tag{8}</script><p>由多元函数换元变换公式, 我们还需要求出雅可比行列式 $J(\\vec{Z} \\rightarrow \\vec{X}),$ 由(7)可得</p>\n<script type=\"math/tex; mode=display\">\nJ(\\vec{Z} \\rightarrow \\vec{X})=\\left|B^{-1}\\right|=|B|^{-1}=|B|^{-\\frac{1}{2}} \\cdot\\left|B^{\\top}\\right|^{-\\frac{1}{2}}=\\left|B B^{\\top}\\right|^{-\\frac{1}{2}}\n\\tag{9}</script><p>由(8)(9), 我们可进一步得</p>\n<script type=\"math/tex; mode=display\">\n1=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} \\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|B B^{\\top}\\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} d x_{1} \\cdots d x_{n}\n\\tag{10}</script><p>我们得到随机向量$\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma)$ 的联合概率密度函数为</p>\n<script type=\"math/tex; mode=display\">\np\\left(x_{1}, \\cdots, x_{n}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|B B^{\\top}\\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]}\n\\tag{11}</script><p>在(11)中, 随机向量$\\vec{X}$ 的协方差矩阵还未得到体现, 我们可通过线性变换(7)做进一步处理</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Sigma &=\\mathbf{E}[(\\vec{X}-\\vec{\\mu})(\\vec{X}-\\vec{\\mu})^{\\top}] \\\\\n&=\\mathbf{E}[(B \\vec{Z}-\\overrightarrow{0})(B \\vec{Z}-\\overrightarrow{0})^{\\top}] \\\\\n&=\\operatorname{Var}(B \\vec{Z}) \\\\\n&=B \\operatorname{Var}(\\vec{Z}) B^{\\top} \\\\\n&=B B^{\\top}\n\\end{aligned}\n\\tag{12}</script><blockquote>\n<p> 常用结论：$E(A\\vec{Z}+B)=AE(\\vec{Z})+B\\\\Var(A\\vec{Z}+B)=AVar(\\vec{Z})A^{\\top}$</p>\n</blockquote>\n<p>我们发现, (11)中 $B B^{\\top}$ 就是线性变换前的随机向量 $\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma)$ 的协方差矩阵 $\\Sigma$, 所以由(11)(12), 我们可以得到联合概率密度函数的最终形式</p>\n<script type=\"math/tex; mode=display\">\np\\left(x_{1}, \\cdots, x_{n}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\cdot e^{.-\\frac{1}{2} \\cdot [(\\vec{X}-\\vec{\\mu})^{\\top} \\Sigma^{-1}(\\vec{X}-\\vec{\\mu})]}\n\\tag{13}</script><p>这就是多元高斯分布的概率密度函数($pdf$)，非常重要，建议刻进DNA里。</p>\n<h3 id=\"2-马氏距离与几何意义\"><a href=\"#2-马氏距离与几何意义\" class=\"headerlink\" title=\"2.马氏距离与几何意义\"></a>2.马氏距离与几何意义</h3><p>接下来，我们将引入一个广泛被使用的距离度量——马氏距离(Mahalanobis Distance)。单个数据点的马氏距离：$D_{M}(x)=\\sqrt{(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)}$</p>\n<p>是不是很眼熟，没错，其形式与我们刚刚讨论完的多元高斯分布概率密度函数的指数部分完全一致！接下来让我们来看看这个形式究竟有什么几何意义:</p>\n<p>由定理1我们有</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\because \\quad \\vec{Z} &=B^{-1}(\\vec{X}-\\vec{\\mu}), \\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, I) \\\\\n\\therefore \\quad \\vec{Z}^{\\top} \\vec{Z} &=\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)^{\\top}\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right) \\\\\n&=(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu}) \\\\\n&=(\\vec{X}-\\vec{\\mu})^{\\top} \\Sigma^{-1}(\\vec{X}-\\vec{\\mu})\n\\end{aligned}\n\\tag{14}</script><p>再由(12)(14)可得</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\vec{Z}^{\\top} \\vec{Z} &=\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top} \\Lambda^{-1}\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left(\\Lambda^{-\\frac{1}{2}}\\right)^{\\top} \\Lambda^{-\\frac{1}{2}}\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\Lambda^{-\\frac{1}{2}} Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left[\\Lambda^{-\\frac{1}{2}} Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\left(Q \\Lambda^{-\\frac{1}{2}}\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left[\\left(Q \\Lambda^{-\\frac{1}{2}}\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\left(Q \\cdot\\left[\\begin{array}{cc}\n\\frac{1}{\\sqrt{\\lambda_{1}}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{\\lambda_{2}}}\n\\end{array}\\right]\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top} \\\\\n& \\cdot\\left[\\left(Q \\cdot\\left[\\begin{array}{cc}\n\\frac{1}{\\sqrt{\\lambda_{1}}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{\\lambda_{2}}}\n\\end{array}\\right]\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]\n\\end{aligned}\n\\tag{15}</script><p>由(15)我们已经可以非常明显地看出线性变换 $\\vec{Z}=B^{-1}(\\vec{X}-\\vec{\\mu})$ 的具体操作了</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/5.png\" alt=\"图4\"></p>\n<p>上述公式看起来繁琐，实际上只是简单地对协方差矩阵$\\Sigma$做了特征分解（协方差矩阵对称半正定）$\\lambda$就是特征值。</p>\n<p>回顾一下本篇一开始的引子，由特征值组成的对角阵的作用就是对维度进行拉伸或者说放缩，就是让维度上的数据除以对应的标准差，显然，作为对角阵的协方差矩阵的特征值就是对应维度的方差，两者在此得到统一。接下来看矩阵$Q$的作用，我们继续顺着引子的思路探索下去，统一了维度间的尺度或量纲就高枕无忧了吗，来看一种情况：<br><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/2.jpg\" alt=\"图5\"></p>\n<p>A与B的欧式距离相同，各维度的尺度也得到了统一，但直觉告诉我们，B点显然更合群，为什么？因为维度之间并不是独立的，图中两个维度线性相关，为了修正这种情况，我们需要去相关化，而这就是</p>\n<p>矩阵$Q$所做的工作。如果维度间保持独立，即协方差矩阵是对角阵，那么$Q$就是单位矩阵$I$。这个时候，等高线的图案是一个标准的椭圆（为什么是椭圆而不是圆，因为特征值对维度进行了拉伸，形成了椭圆的实轴与虚轴）等高线的公式如下,二维情况下就是椭圆：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits _{i=1}^{p}(x-\\mu)^{T}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits _{i=1}^{p}\\frac{y_{i}^{2}}{\\lambda_{i}}\n\\tag{16}</script><p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/7.jpg\" alt=\"图6\"></p>\n<p>一般情况下，$Q$则是由特征向量堆叠而成的正交矩阵（如果感到陌生的化可以复习一下线代），而等高线变成一个倾斜的椭圆：</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/6.jpg\" alt=\"图7\"></p>\n<p>你可以把$Q$理解成旋转矩阵，对标准的椭圆进行旋转（因为旋转矩阵非常普遍与实用，有机会的话我会单独拿出来讲一讲）。也可以认为椭圆其实没变，只是基变了，或者说参考的坐标轴变了，不再是标准正交基，而变成了$Q$中的特征向量组成的单位正交基，在新的坐标轴下（图中的$u_{x}$,$u_{y}$），椭圆依然是标准的，维度依然是相互独立的。</p>\n<p>总结一下：马氏距离就是对原维度进行<strong>尺度归一化加去相关性</strong>，在新的维度下，数据之间的度量就可以简单地用欧氏距离表示。而我们在推导高斯分布的概率密度函数时所用的矩阵$B^{-1}$代表的线性变换也是<strong>拉伸与去相关性</strong>（如果你对此还有疑惑，可以回顾一下图4），使得一般的情况转化为简单的独立情况，方便我们处理。这种转换的思想非常重要，在很多方面都有精彩的运用，比如把非线性问题转换成线性问题来处理。</p>\n<h3 id=\"3-条件概率与边缘概率\"><a href=\"#3-条件概率与边缘概率\" class=\"headerlink\" title=\"3.条件概率与边缘概率\"></a>3.条件概率与边缘概率</h3><p>接下来我们将要探索多元高斯的条件概率分布与边缘概率分布，这一块在统计推断与机器学习中有重要应用，但因为推导的过程比较复杂，不要求完全掌握推导过程，只要混个眼熟就行。我们记 $x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}$，已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma)$。</p>\n<p>我们的目标是得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。</p>\n<p>先来推边缘概率，首先放一个引理：</p>\n<p><strong>定理2：已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b$，那么 $y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)$。$\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T$。</strong></p>\n<p>（这个定理应该很符合直觉，也非常容易记忆）</p>\n<p>我们可以把随机向量$X$分成两部分$\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}$，$x_a$为m维列向量，$x_b$为n维列向量。现在我们想要求出$x_a$服从的概率分布（利用对称性可以推出$x_b$）直觉上猜测可能服从某一高斯分布，事实上也确实如此（高斯分布的优良性质）。我们可以构造出如下等式：</p>\n<script type=\"math/tex; mode=display\">\nx_a=\\underbrace{\\begin{pmatrix}\\mathbb{I}_{m\\times m}&\\mathbb{O}_{m\\times n}\\end{pmatrix}}_{相当于A}\\underbrace{\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}}_{X}\n\\tag{17}</script><p>利用引理可以得到：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&\\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_a\\\\\n&Var[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa}\\\\\n& x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})\n\\end{align}\n\\tag{18}</script><p>同理可得$x_b$的情况，这里就不赘述了。</p>\n<p>接下来推条件概率（相关的推导方式不唯一，这里我们选择一种比较有技巧性的构造推导）</p>\n<p>我们定义3个符号量</p>\n<script type=\"math/tex; mode=display\">\nx_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n\\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\\\\n\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n\\tag{19}</script><p>需要注意的是，<strong>3个符号量中只有第一个才是我们定义的（后续将在它的基础上进行推导），后两个都是在第一个基础上推出来的，只是先在这里列出来</strong>。$x_{b\\cdot a}$的具体定义自于技巧性的构造，但我们简单地观察一下形式可以发现，一旦我们得到了$x_{b\\cdot a}$服从的分布的信息，我们就能建立$x_a$与$x_b$之间的联系，继而推出$p(x_b|x_a)$。根据$x_{b\\cdot a}$的定义，我们构造如下等式：</p>\n<script type=\"math/tex; mode=display\">\nx_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\n\\tag{20}</script><p>利用引理可得：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_{b\\cdot a}\\\\\nVar[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\\\mathbb{I}_{n\\times n}\\end{pmatrix}=\\Sigma_{bb\\cdot a}\n\\tag{21}</script><p>我们对$x_{b\\cdot a}$的定义式（式（19））做个小变形得到</p>\n<script type=\"math/tex; mode=display\">\nx_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a \\tag{21}</script><p>再次利用引理可得</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&\\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n&Var[x_b|x_a]=\\Sigma_{bb\\cdot a}\\\\\n&x_b|x_a\\sim\\mathcal{N}(\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a,\\Sigma_{bb\\cdot a})\n\\end{align}\n\\tag{22}</script><p>$x_b$同理可得（简单交换ab次序就行）</p>\n<p>这里再多提一点</p>\n<script type=\"math/tex; mode=display\">\nVar[x_b|x_a]=\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}</script><p>其形式就是线性代数中的舒尔补（Schur complement），感兴趣的读者可以自行Google。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p>1.<a href=\"https://zhuanlan.zhihu.com/p/58987388\">多元高斯分布完全解析</a></p>\n<p>2.<a href=\"https://zhuanlan.zhihu.com/p/46626607\">马氏距离(Mahalanobis Distance)</a></p>\n<p>3.机器学习-白板推导系列</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"多元高斯分布完全指南\"><a href=\"#多元高斯分布完全指南\" class=\"headerlink\" title=\"多元高斯分布完全指南\"></a>多元高斯分布完全指南</h1><h2 id=\"引子\"><a href=\"#引子\" class=\"headerlink\" title=\"引子\"></a>引子</h2><p>先来看一张图：</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/1.png\" alt=\"图1\"></p>\n<p>如果我让你从A、B中选出一个离群点，你觉得谁更合适？从欧式距离来比较，A、B两点到均值（图中的原点）的距离相同，但从直觉上看，B显然更融入群体。这是因为两者在比较时尺度或者说量纲（scalar）没有统一，就像你拿百万面额的津巴布韦币同人民币做比较一样，图中对应的尺度就是维度的方差。当你对每个维度的尺度进行归一化处理后（即让维度上的每个数据除以维度对应的标准差（方差的算数平方根）为什么是平方根？因为我们最终希望修正后的尺度可以简单采用欧式距离，而欧式距离的定义中显然有一个平方根）图就变成下面这样：</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/3.jpg\" alt=\"图2\"></p>\n<p>现在就可以简单地用欧式距离判断谁是离群点了</p>\n<h2 id=\"一元高斯分布\"><a href=\"#一元高斯分布\" class=\"headerlink\" title=\"一元高斯分布\"></a>一元高斯分布</h2><p>如果你学过概率论，那你应该对上面的引子感到熟悉，因为在一元高斯分布中，我们经常会对随机变量$X$进行标准化——$Z=\\frac{X-\\mu}{\\sigma}$，这同之前我们所做的工作相比不能说完全一致，只能说一模一样（XD）。引子就先告一段落，我们很快会再来回顾它。先来复习一下一元高斯分布的有关知识：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np(x) &=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(\\frac{x-\\mu}{\\sigma}\\right)^{2}} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} p(x)dx\n\\end{aligned}\n\\tag{1}</script><p>令$Z=\\frac{X-\\mu}{\\sigma}$</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\because x(z) &=z \\cdot \\sigma+\\mu \\\\\n\\therefore p(x(z)) &=\\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} p(x(z)) d x \\\\\n&=\\int_{-\\infty}^{+\\infty} \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} d x \\\\\n&=\\int_{-\\infty}^{+\\infty} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}} dz \n\\end{aligned}\n\\tag{2}</script><p>此时我们说随机变量 $Z \\sim \\mathcal{N}(0,1)$ 服从一元标准高斯分布, 其均值 $\\mu=0,$ 方差 $\\sigma^{2}=1,$ 其概率密度函数为</p>\n<script type=\"math/tex; mode=display\">\np(z)=\\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot(z)^{2}}\n\\tag{3}</script><h2 id=\"多元高斯分布\"><a href=\"#多元高斯分布\" class=\"headerlink\" title=\"多元高斯分布\"></a>多元高斯分布</h2><h3 id=\"1-概率密度函数（PDF）\"><a href=\"#1-概率密度函数（PDF）\" class=\"headerlink\" title=\"1.概率密度函数（PDF）\"></a>1.概率密度函数（PDF）</h3><p>接下来我们将正式探讨多元情况下的高斯分布，从一元到多元，我们的随机变量也进化成了随机向量，我们用$X=(x_1,x_2,…,x_n)$表示，$x_i$是向量的一个分量，是单个随机变量。我们先从最简单的情况，各个随机变量之间相互独立，开始讨论。</p>\n<p>假设我们有随机向量 $\\vec{Z}=\\left[Z_{1}, \\cdots, Z_{n}\\right]^{\\top},$ 其中 $Z_{i} \\sim \\mathcal{N}(0,1)(i=1, \\cdots, n)$ 且 $Z_{i}, Z_{j}(i, j=1, \\cdots, n \\wedge i \\neq j)$ 彼此独立, 即随机向量中的每个随机变量 $Z_{i}$ 都服从标准高斯分布且两两彼此独立. 则由(3)与独立随机变量概率密度函数之间的关系, 我们可得随机向量 $\\vec{Z}=\\left[Z_{1}, \\cdots, Z_{n}\\right]^{\\top}$ 的联合概率密度函数为</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\np\\left(z_{1}, \\cdots, z_{n}\\right) &=\\prod_{i=1}^{n} \\frac{1}{\\sqrt{2 \\pi}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(z_{i}\\right)^{2}} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(Z^{\\top} Z\\right)} \\\\\n1 &=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p\\left(z_{1}, \\cdots, z_{n}\\right) d z_{1} \\cdots d z_{n}\n\\end{aligned}\n\\tag{4}</script><p>我们称随机向量 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, \\mathbf{I})$, 即随机向量服从均值为零向量, 协方差矩阵为单位矩阵的高斯分布. 在这里, 随机向量 $\\vec{Z}$ 的协方差矩阵是 $\\operatorname{Cov}\\left(Z_{i}, Z_{j}\\right), i, j=1, \\cdots, n$ 组成的矩阵, 即</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\left[\\operatorname{Cov}\\left(Z_{i}, Z_{j}\\right)\\right]_{n \\times n} &=\\mathbf{E}\\left[(Z-\\vec{\\mu})(Z-\\vec{\\mu})^{\\top}\\right] \\\\\n&=\\mathbf{I}\n\\end{aligned}\n\\tag{5}</script><p>由于随机向量 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, \\mathbf{I}),$ 所以其协方差矩阵的对角线元素为1, 其余元素为0. 如果我们取常数 $c=p\\left(z_{1}, \\cdots, z_{n}\\right),$ 则可得函数 $p\\left(z_{1}, \\cdots, z_{n}\\right)$ 的等高线为 $c^{\\prime}=Z^{\\top} Z,$ 当随机向量 $\\vec{Z}$ 为二维向量<br>时, 我们有</p>\n<script type=\"math/tex; mode=display\">\nc^{\\prime}=Z^{\\top} \\cdot Z=\\left(z_{1}-0\\right)^{2}+\\left(z_{2}-0\\right)^{2}\n\\tag{6}</script><p>显然，其等高线是以(0，0)为圆心的同心圆。</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/4.jpg\" alt=\"图3\"></p>\n<p>接下来讨论各随机变量不相互独立的一般情况：</p>\n<p>既然我们能够轻松地处理独立的情况，那只要我们能够把一般的情况转化成独立的情况，问题是不是就迎刃而解了呢？答案是肯定的，幸运的是，我们有如下定理：</p>\n<p><strong>定理1: 若存在随机向量 $\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma),$ 其中 $\\vec{\\mu} \\in R^{n}$ 为均值向量, $\\Sigma \\in S_{++}^{n \\times n}$ 半正定实对称矩阵为 $\\vec{X}$ 的协方差矩阵, 则存在满秩矩阵 $B \\in R^{n \\times n},$ 使得 $\\vec{Z}=B^{-1}(\\vec{X}-\\vec{\\mu}),$ 而 $\\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0},\\mathbf{I})$.</strong></p>\n<p>（满秩的原因是我们需要求逆）</p>\n<p>有了定理1, 我们就可以对随机向量$\\vec{X}$ 做相应的线性变换, 使其随机变量在线性变换后彼此独立, 从而求出其联合概率密度函数, 具体地</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\because \\vec{Z}&=B^{-1}(\\vec{X}-\\vec{\\mu}), \\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, I)\\\\\n\\therefore \\quad p\\left(z_{1}, \\cdots, z_{n}\\right) &=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left(Z^{\\top} Z\\right)} \\\\\np\\left(z_{1}\\left(x_{1}, \\cdots, x_{n}\\right), \\cdots\\right) &=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)^{\\top}\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)\\right]} \\\\\n&=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} \n\\end{aligned}\n\\tag{7}</script><script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\therefore \\quad 1 &=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} p\\left(z_{1}\\left(x_{1}, \\cdots, x_{n}\\right), \\cdots\\right) d z_{1} \\cdots d z_{n} \\\\\n&=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} \\frac{1}{(2 \\pi)^{\\frac{n}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} d z_{1} \\cdots d z_{n}\n\\end{aligned}\n\\tag{8}</script><p>由多元函数换元变换公式, 我们还需要求出雅可比行列式 $J(\\vec{Z} \\rightarrow \\vec{X}),$ 由(7)可得</p>\n<script type=\"math/tex; mode=display\">\nJ(\\vec{Z} \\rightarrow \\vec{X})=\\left|B^{-1}\\right|=|B|^{-1}=|B|^{-\\frac{1}{2}} \\cdot\\left|B^{\\top}\\right|^{-\\frac{1}{2}}=\\left|B B^{\\top}\\right|^{-\\frac{1}{2}}\n\\tag{9}</script><p>由(8)(9), 我们可进一步得</p>\n<script type=\"math/tex; mode=display\">\n1=\\int_{-\\infty}^{+\\infty} \\cdots \\int_{-\\infty}^{+\\infty} \\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|B B^{\\top}\\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]} d x_{1} \\cdots d x_{n}\n\\tag{10}</script><p>我们得到随机向量$\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma)$ 的联合概率密度函数为</p>\n<script type=\"math/tex; mode=display\">\np\\left(x_{1}, \\cdots, x_{n}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}\\left|B B^{\\top}\\right|^{\\frac{1}{2}}} \\cdot e^{-\\frac{1}{2} \\cdot\\left[(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu})\\right]}\n\\tag{11}</script><p>在(11)中, 随机向量$\\vec{X}$ 的协方差矩阵还未得到体现, 我们可通过线性变换(7)做进一步处理</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\Sigma &=\\mathbf{E}[(\\vec{X}-\\vec{\\mu})(\\vec{X}-\\vec{\\mu})^{\\top}] \\\\\n&=\\mathbf{E}[(B \\vec{Z}-\\overrightarrow{0})(B \\vec{Z}-\\overrightarrow{0})^{\\top}] \\\\\n&=\\operatorname{Var}(B \\vec{Z}) \\\\\n&=B \\operatorname{Var}(\\vec{Z}) B^{\\top} \\\\\n&=B B^{\\top}\n\\end{aligned}\n\\tag{12}</script><blockquote>\n<p> 常用结论：$E(A\\vec{Z}+B)=AE(\\vec{Z})+B\\\\Var(A\\vec{Z}+B)=AVar(\\vec{Z})A^{\\top}$</p>\n</blockquote>\n<p>我们发现, (11)中 $B B^{\\top}$ 就是线性变换前的随机向量 $\\vec{X} \\sim \\mathcal{N}(\\vec{\\mu}, \\Sigma)$ 的协方差矩阵 $\\Sigma$, 所以由(11)(12), 我们可以得到联合概率密度函数的最终形式</p>\n<script type=\"math/tex; mode=display\">\np\\left(x_{1}, \\cdots, x_{n}\\right)=\\frac{1}{(2 \\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}} \\cdot e^{.-\\frac{1}{2} \\cdot [(\\vec{X}-\\vec{\\mu})^{\\top} \\Sigma^{-1}(\\vec{X}-\\vec{\\mu})]}\n\\tag{13}</script><p>这就是多元高斯分布的概率密度函数($pdf$)，非常重要，建议刻进DNA里。</p>\n<h3 id=\"2-马氏距离与几何意义\"><a href=\"#2-马氏距离与几何意义\" class=\"headerlink\" title=\"2.马氏距离与几何意义\"></a>2.马氏距离与几何意义</h3><p>接下来，我们将引入一个广泛被使用的距离度量——马氏距离(Mahalanobis Distance)。单个数据点的马氏距离：$D_{M}(x)=\\sqrt{(x-\\mu)^{T} \\Sigma^{-1}(x-\\mu)}$</p>\n<p>是不是很眼熟，没错，其形式与我们刚刚讨论完的多元高斯分布概率密度函数的指数部分完全一致！接下来让我们来看看这个形式究竟有什么几何意义:</p>\n<p>由定理1我们有</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\because \\quad \\vec{Z} &=B^{-1}(\\vec{X}-\\vec{\\mu}), \\vec{Z} \\sim \\mathcal{N}(\\overrightarrow{0}, I) \\\\\n\\therefore \\quad \\vec{Z}^{\\top} \\vec{Z} &=\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right)^{\\top}\\left(B^{-1}(\\vec{X}-\\vec{\\mu})\\right) \\\\\n&=(\\vec{X}-\\vec{\\mu})^{\\top}\\left(B B^{\\top}\\right)^{-1}(\\vec{X}-\\vec{\\mu}) \\\\\n&=(\\vec{X}-\\vec{\\mu})^{\\top} \\Sigma^{-1}(\\vec{X}-\\vec{\\mu})\n\\end{aligned}\n\\tag{14}</script><p>再由(12)(14)可得</p>\n<script type=\"math/tex; mode=display\">\n\\begin{aligned}\n\\vec{Z}^{\\top} \\vec{Z} &=\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top} \\Lambda^{-1}\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left(\\Lambda^{-\\frac{1}{2}}\\right)^{\\top} \\Lambda^{-\\frac{1}{2}}\\left[Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\Lambda^{-\\frac{1}{2}} Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left[\\Lambda^{-\\frac{1}{2}} Q^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\left(Q \\Lambda^{-\\frac{1}{2}}\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top}\\left[\\left(Q \\Lambda^{-\\frac{1}{2}}\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right] \\\\\n&=\\left[\\left(Q \\cdot\\left[\\begin{array}{cc}\n\\frac{1}{\\sqrt{\\lambda_{1}}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{\\lambda_{2}}}\n\\end{array}\\right]\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]^{\\top} \\\\\n& \\cdot\\left[\\left(Q \\cdot\\left[\\begin{array}{cc}\n\\frac{1}{\\sqrt{\\lambda_{1}}} & 0 \\\\\n0 & \\frac{1}{\\sqrt{\\lambda_{2}}}\n\\end{array}\\right]\\right)^{\\top}(\\vec{X}-\\vec{\\mu})\\right]\n\\end{aligned}\n\\tag{15}</script><p>由(15)我们已经可以非常明显地看出线性变换 $\\vec{Z}=B^{-1}(\\vec{X}-\\vec{\\mu})$ 的具体操作了</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/5.png\" alt=\"图4\"></p>\n<p>上述公式看起来繁琐，实际上只是简单地对协方差矩阵$\\Sigma$做了特征分解（协方差矩阵对称半正定）$\\lambda$就是特征值。</p>\n<p>回顾一下本篇一开始的引子，由特征值组成的对角阵的作用就是对维度进行拉伸或者说放缩，就是让维度上的数据除以对应的标准差，显然，作为对角阵的协方差矩阵的特征值就是对应维度的方差，两者在此得到统一。接下来看矩阵$Q$的作用，我们继续顺着引子的思路探索下去，统一了维度间的尺度或量纲就高枕无忧了吗，来看一种情况：<br><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/2.jpg\" alt=\"图5\"></p>\n<p>A与B的欧式距离相同，各维度的尺度也得到了统一，但直觉告诉我们，B点显然更合群，为什么？因为维度之间并不是独立的，图中两个维度线性相关，为了修正这种情况，我们需要去相关化，而这就是</p>\n<p>矩阵$Q$所做的工作。如果维度间保持独立，即协方差矩阵是对角阵，那么$Q$就是单位矩阵$I$。这个时候，等高线的图案是一个标准的椭圆（为什么是椭圆而不是圆，因为特征值对维度进行了拉伸，形成了椭圆的实轴与虚轴）等高线的公式如下,二维情况下就是椭圆：</p>\n<script type=\"math/tex; mode=display\">\n\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits _{i=1}^{p}(x-\\mu)^{T}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits _{i=1}^{p}\\frac{y_{i}^{2}}{\\lambda_{i}}\n\\tag{16}</script><p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/7.jpg\" alt=\"图6\"></p>\n<p>一般情况下，$Q$则是由特征向量堆叠而成的正交矩阵（如果感到陌生的化可以复习一下线代），而等高线变成一个倾斜的椭圆：</p>\n<p><img src=\"/2021/03/24/Multivariate-Gaussian-Distribution/6.jpg\" alt=\"图7\"></p>\n<p>你可以把$Q$理解成旋转矩阵，对标准的椭圆进行旋转（因为旋转矩阵非常普遍与实用，有机会的话我会单独拿出来讲一讲）。也可以认为椭圆其实没变，只是基变了，或者说参考的坐标轴变了，不再是标准正交基，而变成了$Q$中的特征向量组成的单位正交基，在新的坐标轴下（图中的$u_{x}$,$u_{y}$），椭圆依然是标准的，维度依然是相互独立的。</p>\n<p>总结一下：马氏距离就是对原维度进行<strong>尺度归一化加去相关性</strong>，在新的维度下，数据之间的度量就可以简单地用欧氏距离表示。而我们在推导高斯分布的概率密度函数时所用的矩阵$B^{-1}$代表的线性变换也是<strong>拉伸与去相关性</strong>（如果你对此还有疑惑，可以回顾一下图4），使得一般的情况转化为简单的独立情况，方便我们处理。这种转换的思想非常重要，在很多方面都有精彩的运用，比如把非线性问题转换成线性问题来处理。</p>\n<h3 id=\"3-条件概率与边缘概率\"><a href=\"#3-条件概率与边缘概率\" class=\"headerlink\" title=\"3.条件概率与边缘概率\"></a>3.条件概率与边缘概率</h3><p>接下来我们将要探索多元高斯的条件概率分布与边缘概率分布，这一块在统计推断与机器学习中有重要应用，但因为推导的过程比较复杂，不要求完全掌握推导过程，只要混个眼熟就行。我们记 $x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}$，已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma)$。</p>\n<p>我们的目标是得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。</p>\n<p>先来推边缘概率，首先放一个引理：</p>\n<p><strong>定理2：已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b$，那么 $y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)$。$\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T$。</strong></p>\n<p>（这个定理应该很符合直觉，也非常容易记忆）</p>\n<p>我们可以把随机向量$X$分成两部分$\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}$，$x_a$为m维列向量，$x_b$为n维列向量。现在我们想要求出$x_a$服从的概率分布（利用对称性可以推出$x_b$）直觉上猜测可能服从某一高斯分布，事实上也确实如此（高斯分布的优良性质）。我们可以构造出如下等式：</p>\n<script type=\"math/tex; mode=display\">\nx_a=\\underbrace{\\begin{pmatrix}\\mathbb{I}_{m\\times m}&\\mathbb{O}_{m\\times n}\\end{pmatrix}}_{相当于A}\\underbrace{\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}}_{X}\n\\tag{17}</script><p>利用引理可以得到：</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&\\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_a\\\\\n&Var[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa}\\\\\n& x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})\n\\end{align}\n\\tag{18}</script><p>同理可得$x_b$的情况，这里就不赘述了。</p>\n<p>接下来推条件概率（相关的推导方式不唯一，这里我们选择一种比较有技巧性的构造推导）</p>\n<p>我们定义3个符号量</p>\n<script type=\"math/tex; mode=display\">\nx_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n\\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\\\\n\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n\\tag{19}</script><p>需要注意的是，<strong>3个符号量中只有第一个才是我们定义的（后续将在它的基础上进行推导），后两个都是在第一个基础上推出来的，只是先在这里列出来</strong>。$x_{b\\cdot a}$的具体定义自于技巧性的构造，但我们简单地观察一下形式可以发现，一旦我们得到了$x_{b\\cdot a}$服从的分布的信息，我们就能建立$x_a$与$x_b$之间的联系，继而推出$p(x_b|x_a)$。根据$x_{b\\cdot a}$的定义，我们构造如下等式：</p>\n<script type=\"math/tex; mode=display\">\nx_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\n\\tag{20}</script><p>利用引理可得：</p>\n<script type=\"math/tex; mode=display\">\n\\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_{b\\cdot a}\\\\\nVar[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\\\mathbb{I}_{n\\times n}\\end{pmatrix}=\\Sigma_{bb\\cdot a}\n\\tag{21}</script><p>我们对$x_{b\\cdot a}$的定义式（式（19））做个小变形得到</p>\n<script type=\"math/tex; mode=display\">\nx_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a \\tag{21}</script><p>再次利用引理可得</p>\n<script type=\"math/tex; mode=display\">\n\\begin{align}\n&\\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n&Var[x_b|x_a]=\\Sigma_{bb\\cdot a}\\\\\n&x_b|x_a\\sim\\mathcal{N}(\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a,\\Sigma_{bb\\cdot a})\n\\end{align}\n\\tag{22}</script><p>$x_b$同理可得（简单交换ab次序就行）</p>\n<p>这里再多提一点</p>\n<script type=\"math/tex; mode=display\">\nVar[x_b|x_a]=\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}</script><p>其形式就是线性代数中的舒尔补（Schur complement），感兴趣的读者可以自行Google。</p>\n<h2 id=\"参考资料\"><a href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"></a>参考资料</h2><p>1.<a href=\"https://zhuanlan.zhihu.com/p/58987388\">多元高斯分布完全解析</a></p>\n<p>2.<a href=\"https://zhuanlan.zhihu.com/p/46626607\">马氏距离(Mahalanobis Distance)</a></p>\n<p>3.机器学习-白板推导系列</p>\n"}],"PostAsset":[{"_id":"source/_posts/Bayes-Filter/Markov.png","slug":"Markov.png","post":"ckmrro94k0001xnacd2rwf6gt","modified":0,"renderable":0},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/1.png","slug":"1.png","post":"ckmrro94m0003xnac2x6ibyje","modified":0,"renderable":0},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/2.jpg","slug":"2.jpg","post":"ckmrro94m0003xnac2x6ibyje","modified":0,"renderable":0},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/3.jpg","slug":"3.jpg","post":"ckmrro94m0003xnac2x6ibyje","modified":0,"renderable":0},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/4.jpg","slug":"4.jpg","post":"ckmrro94m0003xnac2x6ibyje","modified":0,"renderable":0},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/5.png","slug":"5.png","post":"ckmrro94m0003xnac2x6ibyje","modified":0,"renderable":0},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/6.jpg","slug":"6.jpg","post":"ckmrro94m0003xnac2x6ibyje","modified":0,"renderable":0},{"_id":"source/_posts/Multivariate-Gaussian-Distribution/7.jpg","slug":"7.jpg","post":"ckmrro94m0003xnac2x6ibyje","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"ckmrro94k0001xnacd2rwf6gt","tag_id":"ckmrro94n0004xnach6wmgnrq","_id":"ckmrro94p0009xnacdat98fh8"},{"post_id":"ckmrro94k0001xnacd2rwf6gt","tag_id":"ckmrro94p0007xnac66s2euix","_id":"ckmrro94q000axnac1yj2hvvv"},{"post_id":"ckmrro94m0003xnac2x6ibyje","tag_id":"ckmrro94p0008xnac712162q2","_id":"ckmrro94q000bxnac62rl39s8"}],"Tag":[{"name":"Bayes","_id":"ckmrro94n0004xnach6wmgnrq"},{"name":"Filter","_id":"ckmrro94p0007xnac66s2euix"},{"name":"Gaussian","_id":"ckmrro94p0008xnac712162q2"}]}}