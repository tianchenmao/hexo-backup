{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"themes/matery/source/favicon.png","path":"favicon.png","modified":0,"renderable":1},{"_id":"themes/matery/source/css/bb.css","path":"css/bb.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/gitment.css","path":"css/gitment.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/matery.css","path":"css/matery.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/my-gitalk.css","path":"css/my-gitalk.css","modified":0,"renderable":1},{"_id":"themes/matery/source/css/my.css","path":"css/my.css","modified":0,"renderable":1},{"_id":"themes/matery/source/js/matery.js","path":"js/matery.js","modified":0,"renderable":1},{"_id":"themes/matery/source/js/search.js","path":"js/search.js","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/avatar.jpg","path":"medias/avatar.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/comment_bg.png","path":"medias/comment_bg.png","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/contact.png","path":"medias/contact.png","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/cover.jpg","path":"medias/cover.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/icp.png","path":"medias/icp.png","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/logo.png","path":"medias/logo.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/animate/animate.min.css","path":"libs/animate/animate.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.css","path":"libs/aos/aos.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aos/aos.js","path":"libs/aos/aos.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","path":"libs/aplayer/APlayer.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","path":"libs/aplayer/APlayer.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/canvas-nest.js","path":"libs/background/canvas-nest.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon-dynamic.js","path":"libs/background/ribbon-dynamic.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon-refresh.min.js","path":"libs/background/ribbon-refresh.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/background/ribbon.min.js","path":"libs/background/ribbon.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeBlockFuction.js","path":"libs/codeBlock/codeBlockFuction.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeCopy.js","path":"libs/codeBlock/codeCopy.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeLang.js","path":"libs/codeBlock/codeLang.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/codeBlock/codeShrink.js","path":"libs/codeBlock/codeShrink.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","path":"libs/cryptojs/crypto-js.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","path":"libs/dplayer/DPlayer.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","path":"libs/dplayer/DPlayer.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","path":"libs/echarts/echarts.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","path":"libs/gitalk/gitalk.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","path":"libs/gitalk/gitalk.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","path":"libs/gitment/gitment-default.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/gitment/gitment.js","path":"libs/gitment/gitment.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/instantpage/instantpage.js","path":"libs/instantpage/instantpage.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","path":"libs/jqcloud/jqcloud-1.0.4.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","path":"libs/jqcloud/jqcloud.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/jquery/jquery.min.js","path":"libs/jquery/jquery.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","path":"libs/masonry/masonry.pkgd.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","path":"libs/materialize/materialize.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","path":"libs/materialize/materialize.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/minivaline/MiniValine.js","path":"libs/minivaline/MiniValine.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","path":"libs/others/busuanzi.pure.mini.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/others/clicklove.js","path":"libs/others/clicklove.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/prism/prism.css","path":"libs/prism/prism.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","path":"libs/scrollprogress/scrollProgress.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","path":"libs/tocbot/tocbot.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","path":"libs/tocbot/tocbot.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/valine/Valine.min.js","path":"libs/valine/Valine.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/valine/av-min.js","path":"libs/valine/av-min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/0.jpg","path":"medias/banner/0.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/1.jpg","path":"medias/banner/1.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/2.jpg","path":"medias/banner/2.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/3.jpg","path":"medias/banner/3.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/4.jpg","path":"medias/banner/4.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/5.jpg","path":"medias/banner/5.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/banner/6.jpg","path":"medias/banner/6.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/0.jpg","path":"medias/featureimages/0.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/1.jpg","path":"medias/featureimages/1.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/10.jpg","path":"medias/featureimages/10.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/11.jpg","path":"medias/featureimages/11.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/12.jpg","path":"medias/featureimages/12.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/13.jpg","path":"medias/featureimages/13.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/14.jpg","path":"medias/featureimages/14.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/15.jpg","path":"medias/featureimages/15.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/16.jpg","path":"medias/featureimages/16.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/17.jpg","path":"medias/featureimages/17.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/18.jpg","path":"medias/featureimages/18.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/19.jpg","path":"medias/featureimages/19.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/2.jpg","path":"medias/featureimages/2.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/20.jpg","path":"medias/featureimages/20.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/21.jpg","path":"medias/featureimages/21.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/22.jpg","path":"medias/featureimages/22.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/23.jpg","path":"medias/featureimages/23.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/3.jpg","path":"medias/featureimages/3.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/4.jpg","path":"medias/featureimages/4.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/5.jpg","path":"medias/featureimages/5.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/6.jpg","path":"medias/featureimages/6.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/7.jpg","path":"medias/featureimages/7.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/8.jpg","path":"medias/featureimages/8.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/featureimages/9.jpg","path":"medias/featureimages/9.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/reward/alipay.jpg","path":"medias/reward/alipay.jpg","modified":0,"renderable":1},{"_id":"themes/matery/source/medias/reward/wechat.png","path":"medias/reward/wechat.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/css/all.css","path":"libs/awesome/css/all.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.eot","path":"libs/awesome/webfonts/fa-brands-400.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.svg","path":"libs/awesome/webfonts/fa-brands-400.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.ttf","path":"libs/awesome/webfonts/fa-brands-400.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff","path":"libs/awesome/webfonts/fa-brands-400.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff2","path":"libs/awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.eot","path":"libs/awesome/webfonts/fa-regular-400.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.svg","path":"libs/awesome/webfonts/fa-regular-400.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.ttf","path":"libs/awesome/webfonts/fa-regular-400.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff","path":"libs/awesome/webfonts/fa-regular-400.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff2","path":"libs/awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.eot","path":"libs/awesome/webfonts/fa-solid-900.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.svg","path":"libs/awesome/webfonts/fa-solid-900.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.ttf","path":"libs/awesome/webfonts/fa-solid-900.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff","path":"libs/awesome/webfonts/fa-solid-900.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff2","path":"libs/awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","path":"libs/lightGallery/css/lightgallery.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","path":"libs/lightGallery/fonts/lg.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","path":"libs/lightGallery/fonts/lg.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","path":"libs/lightGallery/fonts/lg.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","path":"libs/lightGallery/fonts/lg.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","path":"libs/lightGallery/img/loading.gif","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","path":"libs/lightGallery/img/video-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","path":"libs/lightGallery/img/vimeo-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","path":"libs/lightGallery/img/youtube-play.png","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","path":"libs/lightGallery/js/lightgallery-all.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/css/share.min.css","path":"libs/share/css/share.min.css","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","path":"libs/share/fonts/iconfont.eot","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","path":"libs/share/fonts/iconfont.svg","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","path":"libs/share/fonts/iconfont.ttf","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","path":"libs/share/fonts/iconfont.woff","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","path":"libs/share/js/jquery.share.min.js","modified":0,"renderable":1},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","path":"libs/share/js/social-share.min.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/_posts/Intro-Math.md","hash":"c87dbd2655a708faa37fa80ad257307fa6ddbcba","modified":1616237908914},{"_id":"source/_posts/hello-world.md","hash":"7d98d6592de80fdcd2949bd7401cec12afd98cdf","modified":1615600226432},{"_id":"source/_posts/linear-regression.md","hash":"88dd18f656618192ce12f64f6743cf246817112b","modified":1615600226432},{"_id":"source/_posts/logistic-regression.md","hash":"ceee47db336f358d5e87f5977d6de0139f8ac805","modified":1616237190587},{"_id":"themes/matery/.gitignore","hash":"727607929a51db7ea10968f547c26041eee9cfff","modified":1615600226432},{"_id":"themes/matery/CHANGELOG.md","hash":"084ec8b110a20170d08a0aa5fd8accf601051835","modified":1615600226432},{"_id":"themes/matery/LICENSE","hash":"7df059597099bb7dcf25d2a9aedfaf4465f72d8d","modified":1615600226432},{"_id":"themes/matery/README.md","hash":"0366f3d50b18d095b0581e7b5974e3283d693884","modified":1615600226432},{"_id":"themes/matery/README_CN.md","hash":"089de96e2165ea2a8a3adf38ebda85b65e7f716e","modified":1615600226432},{"_id":"themes/matery/languages/default.yml","hash":"54ccc01b097c5bf6820f0edfcece1a87b78ab32d","modified":1615600226432},{"_id":"themes/matery/languages/zh-CN.yml","hash":"a957b05f70265a86a87d922e18488571809d2472","modified":1615600226432},{"_id":"themes/matery/languages/zh-HK.yml","hash":"ae34ac0e175c3037675722e436637efbceea32f0","modified":1615600226432},{"_id":"themes/matery/layout/404.ejs","hash":"9c8ca67377211e5d60fdde272a975faa9a91a22a","modified":1615600226432},{"_id":"themes/matery/layout/about.ejs","hash":"41849f9300b8dc47048333fcf4a897dd8a2a13ca","modified":1615600226436},{"_id":"themes/matery/_config.yml","hash":"153e8d6c22dfacabd4288ce029c7d563f0d69c4c","modified":1616237478381},{"_id":"themes/matery/layout/bb.ejs","hash":"21959d702f17a3d98b716daf44c8b5eecd59c7c5","modified":1615600226436},{"_id":"themes/matery/layout/categories.ejs","hash":"8e54665cc25d7c333da7d9f312987190be6215da","modified":1615600226436},{"_id":"themes/matery/layout/archive.ejs","hash":"cdac701de8370f9f3794a0eed4165983993a1ca7","modified":1615600226436},{"_id":"themes/matery/layout/category.ejs","hash":"00019bca11fb46477f22017cb1f5ad8444da0580","modified":1615600226436},{"_id":"themes/matery/layout/contact.ejs","hash":"19d62e521c4253496db559478db5164ddfd2480e","modified":1615600226436},{"_id":"themes/matery/layout/index.ejs","hash":"4dc6f08e7709cc04e886be72dbf0d06469f0effc","modified":1615600226436},{"_id":"themes/matery/layout/friends.ejs","hash":"92892bab5578ccf758ce57e19fca08be80d0d5b9","modified":1615600226436},{"_id":"themes/matery/layout/layout.ejs","hash":"974b44eb3e343cd3ee57ebad34bbb0eff4184400","modified":1615600226436},{"_id":"themes/matery/layout/tag.ejs","hash":"85a4b05bd8a6ad0f17ff2e97dae56949b379c204","modified":1615600226436},{"_id":"themes/matery/layout/tags.ejs","hash":"cf9517aa6a0111355121f44615d6923e312283c7","modified":1615600226436},{"_id":"themes/matery/source/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1615600226436},{"_id":"themes/matery/layout/_partial/back-top.ejs","hash":"47ee36a042bb6d52bbe1d0f329637e8ffcf1d0aa","modified":1615600226432},{"_id":"themes/matery/layout/_partial/background.ejs","hash":"aef6edeeb11209831a11d8c7f5d59992e2573335","modified":1615600226432},{"_id":"themes/matery/layout/_partial/baidu-push.ejs","hash":"2cebcc5ea3614d7f76ec36670e68050cbe611202","modified":1615600226432},{"_id":"themes/matery/layout/_partial/baidu-analytics.ejs","hash":"3bbcdb474ca1dcad514bdc4b7763e17c55df04fd","modified":1615600226432},{"_id":"themes/matery/layout/_partial/bg-cover-content.ejs","hash":"28617bf2a35a4269eba6df466acd174e416d2d1e","modified":1615600226432},{"_id":"themes/matery/layout/_partial/bg-cover.ejs","hash":"02191109712f61c0e487b8f0b8466597181a9004","modified":1615600226432},{"_id":"themes/matery/layout/_partial/changyan.ejs","hash":"cd919d31564e118c2ee8d5cbfb7d51ee6da15d82","modified":1615600226432},{"_id":"themes/matery/layout/_partial/disqus.ejs","hash":"b2dc2c8b5ed56815e55cc2ea54a6dc4eeba2375d","modified":1615600226432},{"_id":"themes/matery/layout/_partial/gitalk.ejs","hash":"2aa8fbb04b046fa7679092a48372d7e036835dff","modified":1615600226432},{"_id":"themes/matery/layout/_partial/github-link.ejs","hash":"3aeb581bd78ab8e15b858e4c44c03bcf92f20b9e","modified":1615600226432},{"_id":"themes/matery/layout/_partial/gitment.ejs","hash":"90f6218512ef2eab63ada7ad2fc766ae635a2297","modified":1615600226436},{"_id":"themes/matery/layout/_partial/google-analytics.ejs","hash":"5f4992205617da5f8cc5863c62b5ec46e414e2fb","modified":1615600226436},{"_id":"themes/matery/layout/post.ejs","hash":"60fc6e340f696435cde542ea5a2a41766cf95d8a","modified":1616240431440},{"_id":"themes/matery/layout/_partial/head.ejs","hash":"f8438ac80df005934a330b029de292d26f0b6ecb","modified":1615600226436},{"_id":"themes/matery/layout/_partial/footer.ejs","hash":"3be24e4c370671eda53bdfd99fb748f4a22948ba","modified":1615600226432},{"_id":"themes/matery/layout/_partial/livere.ejs","hash":"9c3401b42ea7f26410a5593bae93ada7e57b43be","modified":1615600226436},{"_id":"themes/matery/layout/_partial/header.ejs","hash":"59e38c70f3d8e7165e686e5e84a627835f4321b0","modified":1615600226436},{"_id":"themes/matery/layout/_partial/index-cover.ejs","hash":"76b4a37e0364380b143fdf94bf1a5e6941564414","modified":1615600226436},{"_id":"themes/matery/layout/_partial/minivaline.ejs","hash":"5f09386aece8f9cf31f6059bbde79cd6c5171493","modified":1615600226436},{"_id":"themes/matery/layout/_partial/mobile-nav.ejs","hash":"cb0cb452be1cd1857ba600f04025b506f3b6fc79","modified":1615600226436},{"_id":"themes/matery/layout/_partial/navigation.ejs","hash":"78b70ff24b3039c871331ebec114b936c1756cc8","modified":1615600226436},{"_id":"themes/matery/layout/_partial/post-cover.ejs","hash":"d1c873c5de54498c722e155aadb8c0ec39485dfa","modified":1615600226436},{"_id":"themes/matery/layout/_partial/post-detail-toc.ejs","hash":"3ff94aff01936242a9f4e1f31adb9b43bfab8d53","modified":1615600226436},{"_id":"themes/matery/layout/_partial/reward.ejs","hash":"ffc55bc7e73bc698bfc58d8e3780c336b83282cf","modified":1615600226436},{"_id":"themes/matery/layout/_partial/paging.ejs","hash":"e2df12cf92a82b1a7a7add2eac1db1d954bc5511","modified":1615600226436},{"_id":"themes/matery/layout/_partial/prev-next.ejs","hash":"c76b78782ea82340104fccc089417572e0adece4","modified":1615600226436},{"_id":"themes/matery/layout/_partial/share.ejs","hash":"c941730a2471d6aab367cbb6e09ed08b56c83143","modified":1615600226436},{"_id":"themes/matery/layout/_partial/reprint-statement.ejs","hash":"0ce3f9361f558b99cc2f059c5e50b0e2a152ae38","modified":1615600226436},{"_id":"themes/matery/layout/_partial/social-link.ejs","hash":"6f871bd3a70f720e4e451f1f4f625cbc6d8994a4","modified":1615600226436},{"_id":"themes/matery/layout/_partial/search.ejs","hash":"150529c9fb9aa8ddb42ec3e02645d301faa2503b","modified":1615600226436},{"_id":"themes/matery/layout/_widget/category-radar.ejs","hash":"1d8747fda89a0b2ca3c7008867cbfeecad0578a6","modified":1615600226436},{"_id":"themes/matery/layout/_partial/valine.ejs","hash":"0e4c0a6154aa34007849928ca88f05b6185b256e","modified":1615600226436},{"_id":"themes/matery/layout/_widget/artitalk.ejs","hash":"b14e486f12b9ac42a273b80e4d785fcb94cf04b2","modified":1615600226436},{"_id":"themes/matery/layout/_partial/post-statis.ejs","hash":"04889f9031743c6b081d02fa4027b0dbfcc45ecf","modified":1615600226436},{"_id":"themes/matery/layout/_widget/category-cloud.ejs","hash":"1b3df1009234c0112424b497b18b4ad8240b3bc7","modified":1615600226436},{"_id":"themes/matery/layout/_widget/dream.ejs","hash":"9a472ad5591100cdb65d0df9d01034163bd6dd9d","modified":1615600226436},{"_id":"themes/matery/layout/_widget/music.ejs","hash":"e9e3e327d5de9d7aeadbde32e1d558652d9e9195","modified":1615600226436},{"_id":"themes/matery/layout/_widget/my-gallery.ejs","hash":"65a2d2f9722f84c7fd98f6bdf79087a14848ebd8","modified":1615600226436},{"_id":"themes/matery/layout/_widget/my-projects.ejs","hash":"ef60b64021fa349b0048425d858dfcf6c906fede","modified":1615600226436},{"_id":"themes/matery/layout/_widget/my-skills.ejs","hash":"89a0092df72d23093128f2fbbdc8ca7f83ebcfd9","modified":1615600226436},{"_id":"themes/matery/layout/_widget/post-calendar.ejs","hash":"48821e644bc73553d7c5c56d2e8ee111a70cd776","modified":1615600226436},{"_id":"themes/matery/layout/_widget/tag-cloud.ejs","hash":"fc42b72cddc231f7485cdc1fd6852b66be6add26","modified":1615600226436},{"_id":"themes/matery/layout/_widget/tag-wordcloud.ejs","hash":"487aacb2454d6bf0d21cdb07ddd1fd5ddbca9038","modified":1615600226436},{"_id":"themes/matery/layout/_widget/video.ejs","hash":"a0e002377af2a7f7e4da6d9a644de97adb035925","modified":1615600226436},{"_id":"themes/matery/layout/_widget/post-charts.ejs","hash":"ab5f986f428215941aeaa0c88aefd440c47d3bcf","modified":1615600226436},{"_id":"themes/matery/source/css/bb.css","hash":"aa15633888c7cf9baea8bb48d796c68b57cf14bf","modified":1615600226436},{"_id":"themes/matery/layout/_widget/recommend.ejs","hash":"8551137e94ca4e2e3b8b63d5626255884cb60cb5","modified":1615600226436},{"_id":"themes/matery/source/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1615600226436},{"_id":"themes/matery/source/css/my-gitalk.css","hash":"af18dd29e58642c18bab9b89541767b494c468dd","modified":1615600226436},{"_id":"themes/matery/source/css/my.css","hash":"497e50351f7838f8546cac76850a42e7e380a110","modified":1615600226436},{"_id":"themes/matery/layout/_partial/post-detail.ejs","hash":"880ebaf78a947631a38ad0b3d65201315845a264","modified":1615600226436},{"_id":"themes/matery/source/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1615600226436},{"_id":"themes/matery/source/js/search.js","hash":"e1482406c58ea2a0eb178d7e4efb2c879cdddc80","modified":1615600226436},{"_id":"themes/matery/source/medias/contact.png","hash":"443ea472dd49b74d9d70295837eb381c8c64f02c","modified":1615600226464},{"_id":"themes/matery/source/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1615600226464},{"_id":"themes/matery/source/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1615600226472},{"_id":"themes/matery/source/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1615600226472},{"_id":"themes/matery/source/css/matery.css","hash":"a630f6e8643904073dce9eada57b5c16c4dba5e2","modified":1615600226436},{"_id":"themes/matery/source/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1615600226464},{"_id":"themes/matery/source/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1615600226440},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1615600226440},{"_id":"themes/matery/source/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1615600226456},{"_id":"themes/matery/source/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1615600226436},{"_id":"themes/matery/source/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1615600226456},{"_id":"themes/matery/source/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1615600226456},{"_id":"themes/matery/source/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1615600226456},{"_id":"themes/matery/source/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1615600226456},{"_id":"themes/matery/source/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1615600226456},{"_id":"themes/matery/source/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1615600226456},{"_id":"themes/matery/source/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1615600226456},{"_id":"themes/matery/source/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1615600226440},{"_id":"themes/matery/source/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1615600226440},{"_id":"themes/matery/source/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1615600226456},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1615600226456},{"_id":"themes/matery/source/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1615600226460},{"_id":"themes/matery/source/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1615600226460},{"_id":"themes/matery/source/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1615600226460},{"_id":"themes/matery/source/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1615600226460},{"_id":"themes/matery/source/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1615600226460},{"_id":"themes/matery/source/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1615600226460},{"_id":"themes/matery/source/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1615600226460},{"_id":"themes/matery/source/libs/prism/prism.css","hash":"62e5474893dece076534352f564ceabd6e088a5a","modified":1615600226460},{"_id":"themes/matery/source/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1615600226460},{"_id":"themes/matery/source/libs/minivaline/MiniValine.js","hash":"fbb58c37e2c74f127ae0c566afa9b48889aab79f","modified":1615600226460},{"_id":"themes/matery/source/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1615600226460},{"_id":"themes/matery/source/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1615600226464},{"_id":"themes/matery/source/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1615600226464},{"_id":"themes/matery/source/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1615600226472},{"_id":"themes/matery/source/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1615600226472},{"_id":"themes/matery/source/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1615600226472},{"_id":"themes/matery/source/medias/reward/wechat.png","hash":"fe93385aa92fe328e01c8221a80b039be9e4e140","modified":1615600226472},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1615600226448},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1615600226452},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1615600226452},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1615600226452},{"_id":"themes/matery/source/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1615600226460},{"_id":"themes/matery/source/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1615600226460},{"_id":"themes/matery/source/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1615600226460},{"_id":"themes/matery/source/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1615600226460},{"_id":"themes/matery/source/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1615600226464},{"_id":"themes/matery/source/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1615600226464},{"_id":"themes/matery/source/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1615600226464},{"_id":"themes/matery/source/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1615600226464},{"_id":"themes/matery/source/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1615600226464},{"_id":"themes/matery/source/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1615600226456},{"_id":"themes/matery/source/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1615600226460},{"_id":"themes/matery/source/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1615600226460},{"_id":"themes/matery/source/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1615600226464},{"_id":"themes/matery/source/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1615600226468},{"_id":"themes/matery/source/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1615600226472},{"_id":"themes/matery/source/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1615600226440},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1615600226448},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1615600226444},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1615600226448},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1615600226448},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1615600226456},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1615600226456},{"_id":"themes/matery/source/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1615600226468},{"_id":"themes/matery/source/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1615600226460},{"_id":"themes/matery/source/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1615600226460},{"_id":"themes/matery/source/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1615600226460},{"_id":"themes/matery/source/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1615600226464},{"_id":"themes/matery/source/medias/banner/6.jpg","hash":"ed7282cc129c4ff9f322d2f2897fb4aac5c48589","modified":1615600226464},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1615600226452},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1615600226452},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1615600226456},{"_id":"themes/matery/source/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1615600226464},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1615600226448},{"_id":"themes/matery/source/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1615600226460},{"_id":"themes/matery/source/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1615600226456},{"_id":"public/2021/03/20/Intro-Math/index.html","hash":"0b613ff01d12f7ebbe2f285b3b450f057e01d93c","modified":1616240437148},{"_id":"public/2021/03/13/hello-world/index.html","hash":"1f51459d68b10ab0c3ad196ddc7806d6879d5453","modified":1616240437148},{"_id":"public/2021/03/06/logistic-regression/index.html","hash":"d0f4d9c97b679fb1180c665939234b6eb42dbde8","modified":1616240437148},{"_id":"public/2021/03/05/linear-regression/index.html","hash":"2cde973d26a435f7a453ca15d0af5580f992275d","modified":1616240437148},{"_id":"public/archives/index.html","hash":"6094d76cfdfd714ef5c430c0413f000ff7617435","modified":1616394959713},{"_id":"public/archives/2021/index.html","hash":"cab33e5d64de92b0db89cf2eda6572e106ea3db7","modified":1616394959713},{"_id":"public/archives/2021/03/index.html","hash":"0d502baa328afa274109a3e29eee2b0f0713ac3c","modified":1616394959713},{"_id":"public/categories/Machine-Learning/index.html","hash":"ff045aea277388cb693816f282f2eefbb64160c8","modified":1616240437148},{"_id":"public/categories/Machine-Learning/regression/index.html","hash":"8e3e6f44d3ac27c2f95a18a8778110627c079f65","modified":1616240437148},{"_id":"public/categories/Machine-Learning/classification/index.html","hash":"f48b17ccc044f70c501185eb4c4bfbe97a02da49","modified":1616240437148},{"_id":"public/index.html","hash":"3f8e981de7988436e1a3cd02c6885cb65d273eea","modified":1616240437148},{"_id":"public/favicon.png","hash":"774fee8c6d0be9dbb010b20f36c06848d06e3da0","modified":1616240437148},{"_id":"public/medias/avatar.jpg","hash":"2a6287308628881ce27b9a7de53ba15c2be00d02","modified":1616240437148},{"_id":"public/medias/contact.png","hash":"443ea472dd49b74d9d70295837eb381c8c64f02c","modified":1616240437148},{"_id":"public/medias/comment_bg.png","hash":"dfc93d24081884fbc58cab0f8fd19e77d31d6123","modified":1616240437148},{"_id":"public/medias/icp.png","hash":"27a96f31f7d0413c6ade6f40e06f021f501151c7","modified":1616240437148},{"_id":"public/medias/logo.png","hash":"d9095f5ea8719374d9d1ff020279426f5b2a1396","modified":1616240437148},{"_id":"public/medias/featureimages/10.jpg","hash":"98e7f6fd9c97d4de9044b6871ca08ebf14db11b9","modified":1616240437148},{"_id":"public/medias/featureimages/15.jpg","hash":"da0fbee3b7bde1607eace377ddf834c0be99edfe","modified":1616240437148},{"_id":"public/medias/featureimages/16.jpg","hash":"97a829c4bc94f9d2929b20a1a9b798c57b9f7205","modified":1616240437148},{"_id":"public/medias/featureimages/13.jpg","hash":"35a320174f8e316e3eadaec658024276b651c6e9","modified":1616240437148},{"_id":"public/medias/featureimages/2.jpg","hash":"4bba691cf71a517ecaeaf42afd3e8f8b31e346c1","modified":1616240437148},{"_id":"public/medias/featureimages/22.jpg","hash":"754579747a3e99747d890fca3162f370b96a7941","modified":1616240437148},{"_id":"public/medias/featureimages/21.jpg","hash":"b26edb128bb0bf58b23fd2f014e9555e89a2ca3b","modified":1616240437148},{"_id":"public/medias/featureimages/23.jpg","hash":"7d7f37da3fa7128343adac23866449eb2c6a549a","modified":1616240437148},{"_id":"public/medias/featureimages/3.jpg","hash":"6ec646c2a70f5f11edacf225c1477f2200a37a96","modified":1616240437148},{"_id":"public/medias/featureimages/5.jpg","hash":"41ca20129a37fedc573eec28dd7d7b9e5b09228a","modified":1616240437148},{"_id":"public/medias/featureimages/7.jpg","hash":"7975141cd64e875122c0ea33daaca1a06bf00b8e","modified":1616240437148},{"_id":"public/medias/featureimages/8.jpg","hash":"8e4b7186352085483ca1174c7c0800114c48df8b","modified":1616240437148},{"_id":"public/medias/reward/alipay.jpg","hash":"1abc719b95d1b26f1f898e6b0a9b7609146e332f","modified":1616240437148},{"_id":"public/medias/reward/wechat.png","hash":"fe93385aa92fe328e01c8221a80b039be9e4e140","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-regular-400.eot","hash":"439c8afd3373acb4a73135a34e220464a89cd5e2","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-regular-400.ttf","hash":"0f4bd02942a54a6b3200d9078adff88c2812e751","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff","hash":"59439d3ad31d856d78ec3e2bd9f1eafa2c7a581c","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-regular-400.woff2","hash":"f6f653b4ea8fc487bdb590d39d5a726258a55f40","modified":1616240437148},{"_id":"public/libs/lightGallery/fonts/lg.eot","hash":"54caf05a81e33d7bf04f2e420736ce6f1de5f936","modified":1616240437148},{"_id":"public/libs/lightGallery/fonts/lg.svg","hash":"9c6632aeec67d3e84a1434884aa801514ff8103b","modified":1616240437148},{"_id":"public/libs/lightGallery/fonts/lg.ttf","hash":"f6421c0c397311ae09f9257aa58bcd5e9720f493","modified":1616240437148},{"_id":"public/libs/lightGallery/fonts/lg.woff","hash":"3048de344dd5cad4624e0127e58eaae4b576f574","modified":1616240437148},{"_id":"public/libs/lightGallery/img/loading.gif","hash":"15a76af2739482d8de7354abc6d8dc4fca8d145e","modified":1616240437148},{"_id":"public/libs/lightGallery/img/video-play.png","hash":"2962e03ddbe04d7e201a5acccac531a2bbccddfc","modified":1616240437148},{"_id":"public/libs/lightGallery/img/vimeo-play.png","hash":"9b72fc0f86a01467ed0b68c9cc4d604ec316d517","modified":1616240437148},{"_id":"public/libs/lightGallery/img/youtube-play.png","hash":"f8d11384d33b7a79ee2ba8d522844f14d5067a80","modified":1616240437148},{"_id":"public/libs/share/fonts/iconfont.eot","hash":"00ff749c8e202401190cc98d56087cdda716abe4","modified":1616240437148},{"_id":"public/libs/share/fonts/iconfont.svg","hash":"1d56c9d5db0273f07c43cc1397e440f98ba7827a","modified":1616240437148},{"_id":"public/libs/share/fonts/iconfont.ttf","hash":"afd898f59d363887418669520b24d175f966a083","modified":1616240437148},{"_id":"public/libs/share/fonts/iconfont.woff","hash":"2e3fce1dcfbd6e2114e7bfbeaf72d3c62e15a1bd","modified":1616240437148},{"_id":"public/medias/banner/0.jpg","hash":"69ec96cd9b4bc3aa631adc9da61353f50c39f031","modified":1616240437148},{"_id":"public/medias/banner/2.jpg","hash":"39fb2535460ce66cc0b34e07ffb9411db1405f09","modified":1616240437148},{"_id":"public/medias/banner/3.jpg","hash":"4ac047e92d0363b1a61ab756aca6dac13fb77494","modified":1616240437148},{"_id":"public/medias/featureimages/0.jpg","hash":"1c3300f029fc85d6dda6fa4f1d699551034cdaf7","modified":1616240437148},{"_id":"public/medias/featureimages/1.jpg","hash":"684ae89de8cb7acefae19f5aee6c612037c46393","modified":1616240437148},{"_id":"public/medias/featureimages/11.jpg","hash":"f55972ce7175684f2b11c3c9fc2b5b14bccbfae8","modified":1616240437148},{"_id":"public/medias/featureimages/12.jpg","hash":"8a4b2e7d92ae95c3b0c921db23c35aa9a41a7d58","modified":1616240437148},{"_id":"public/medias/featureimages/14.jpg","hash":"38e11221406785bcd93aa9cd23e568e164630ef1","modified":1616240437148},{"_id":"public/medias/featureimages/17.jpg","hash":"42d47903551ee81885c1386022982cae165841c5","modified":1616240437148},{"_id":"public/medias/featureimages/18.jpg","hash":"64829272ec85bb819d55ff89e5b5fd6f64aa436b","modified":1616240437148},{"_id":"public/medias/featureimages/19.jpg","hash":"eb250906fdbc0c408f42ae9933725bc1a05d79fb","modified":1616240437148},{"_id":"public/medias/featureimages/20.jpg","hash":"3b11f9b461168d907073f793190865fe621a8573","modified":1616240437148},{"_id":"public/medias/featureimages/4.jpg","hash":"e06c47de27619984be9d5d02947f8370a432dfea","modified":1616240437148},{"_id":"public/medias/featureimages/6.jpg","hash":"c8f2aa4bbb041158b4e73733a341e6a77c8583f7","modified":1616240437148},{"_id":"public/medias/featureimages/9.jpg","hash":"b956a2291a04b2132366b53666cf34858b8bdb1f","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-brands-400.eot","hash":"22f9e7d5226408eb2d0a11e118257a3ca22b8670","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-brands-400.ttf","hash":"91cbeeaceb644a971241c08362898599d6d968ce","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff","hash":"18838f5260317da3c5ed29bf844ac8a4f7ad0529","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-brands-400.woff2","hash":"a46bd47ff0a90b812aafafda587d095cdb844271","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff","hash":"92803b8753ceda573c6906774677c5a7081d2fbb","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-solid-900.woff2","hash":"9c081b88b106c6c04ecb895ba7ba7d3dcb3b55ac","modified":1616240437148},{"_id":"public/medias/cover.jpg","hash":"d4957ff7cc5e88555cd840f2956ab0561e6f1ccf","modified":1616240437148},{"_id":"public/medias/banner/1.jpg","hash":"ab122a36998a4f62a61e61a4fc5e00248113413b","modified":1616240437148},{"_id":"public/medias/banner/6.jpg","hash":"ed7282cc129c4ff9f322d2f2897fb4aac5c48589","modified":1616240437148},{"_id":"public/medias/banner/5.jpg","hash":"852418f4f09e796e12bc3bab7a1488d3f37d6486","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-regular-400.svg","hash":"3d3a49445343d80f3b553e3e3425b9a7bd49acaf","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-solid-900.eot","hash":"cab8e84ae5682d1d556e234df9c790985888def8","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-solid-900.ttf","hash":"9521ed12274c2cbc910cea77657116fcf6545da3","modified":1616240437148},{"_id":"public/css/bb.css","hash":"aa15633888c7cf9baea8bb48d796c68b57cf14bf","modified":1616240437148},{"_id":"public/css/gitment.css","hash":"2bd15cc17dca35ac3ecc0acf167a23a1dd362acd","modified":1616240437148},{"_id":"public/css/my-gitalk.css","hash":"af18dd29e58642c18bab9b89541767b494c468dd","modified":1616240437148},{"_id":"public/css/my.css","hash":"497e50351f7838f8546cac76850a42e7e380a110","modified":1616240437148},{"_id":"public/js/matery.js","hash":"b86de5fe3e9766b7ff80df12ea41c3a9e30825f7","modified":1616240437148},{"_id":"public/js/search.js","hash":"e1482406c58ea2a0eb178d7e4efb2c879cdddc80","modified":1616240437148},{"_id":"public/libs/aos/aos.js","hash":"02bfb40b0c4b6e9b0b4081218357145cbb327d74","modified":1616240437148},{"_id":"public/libs/aplayer/APlayer.min.css","hash":"07372a2ba507388d0fed166d761b1c2c2a659dce","modified":1616240437148},{"_id":"public/libs/background/canvas-nest.js","hash":"65333d0dbb9c1173a1b13031b230161fc42c8b2f","modified":1616240437148},{"_id":"public/libs/background/ribbon-dynamic.js","hash":"052b80c29e6bc585aa28d4504b743bdbac220a88","modified":1616240437148},{"_id":"public/libs/background/ribbon-refresh.min.js","hash":"6d98692b2cad8c746a562db18b170b35c24402f4","modified":1616240437148},{"_id":"public/libs/background/ribbon.min.js","hash":"6a99d494c030388f96f6086a7aaa0f03f3fe532e","modified":1616240437148},{"_id":"public/libs/codeBlock/codeBlockFuction.js","hash":"c7ab06d27a525b15b1eb69027135269e9b9132fb","modified":1616240437148},{"_id":"public/libs/codeBlock/codeCopy.js","hash":"6d39a766af62e625f177c4d5cf3adc35eed71e61","modified":1616240437148},{"_id":"public/libs/codeBlock/codeLang.js","hash":"bac88b4d4e3679732d29bd037c34f089cf27cf05","modified":1616240437148},{"_id":"public/libs/codeBlock/codeShrink.js","hash":"201e8cd761b4be557247bdaf1ebc7c11c83194f6","modified":1616240437148},{"_id":"public/libs/jqcloud/jqcloud-1.0.4.min.js","hash":"257eaae3020599e4939f50d5008a743827f25b8c","modified":1616240437148},{"_id":"public/libs/instantpage/instantpage.js","hash":"83ce8919b1a69b2f1809ffaf99b52a8627e650e9","modified":1616240437148},{"_id":"public/libs/jqcloud/jqcloud.css","hash":"20d9f11a19d95c70e27cb922e0d6dccbec4eae89","modified":1616240437148},{"_id":"public/libs/others/busuanzi.pure.mini.js","hash":"6e41f31100ae7eb3a6f23f2c168f6dd56e7f7a9a","modified":1616240437148},{"_id":"public/libs/others/clicklove.js","hash":"6a39b8c683ba5dcd92f70c6ab45d1cfac3213e8e","modified":1616240437148},{"_id":"public/libs/prism/prism.css","hash":"62e5474893dece076534352f564ceabd6e088a5a","modified":1616240437148},{"_id":"public/libs/scrollprogress/scrollProgress.min.js","hash":"777ffe5d07e85a14fbe97d846f45ffc0087251cc","modified":1616240437148},{"_id":"public/libs/tocbot/tocbot.css","hash":"9ab8ef576c9a57115194152e79cca79b0a41dd70","modified":1616240437148},{"_id":"public/libs/tocbot/tocbot.min.js","hash":"5ec27317f0270b8cf6b884c6f12025700b9a565c","modified":1616240437148},{"_id":"public/libs/share/css/share.min.css","hash":"8a778a86f3ce9a042df6be63a9f1039631e351a5","modified":1616240437148},{"_id":"public/css/matery.css","hash":"a630f6e8643904073dce9eada57b5c16c4dba5e2","modified":1616240437148},{"_id":"public/libs/aos/aos.css","hash":"191a3705a8f63e589a50a0ff2f2c5559f1a1b6b2","modified":1616240437148},{"_id":"public/libs/animate/animate.min.css","hash":"97afa151569f046b2e01f27c1871646e9cd87caf","modified":1616240437148},{"_id":"public/libs/aplayer/APlayer.min.js","hash":"22caa28ff6b41a16ff40f15d38f1739e22359478","modified":1616240437148},{"_id":"public/libs/dplayer/DPlayer.min.css","hash":"f7d19655f873b813ffba5d1a17145c91f82631b8","modified":1616240437148},{"_id":"public/libs/cryptojs/crypto-js.min.js","hash":"5989527a378b55011a59522f41eeb3981518325c","modified":1616240437148},{"_id":"public/libs/dplayer/DPlayer.min.js","hash":"c3bad7b265574fab0ae4d45867422ea1cb9d6599","modified":1616240437148},{"_id":"public/libs/gitalk/gitalk.css","hash":"940ded3ea12c2fe1ab0820d2831ec405f3f1fe9f","modified":1616240437148},{"_id":"public/libs/echarts/echarts.min.js","hash":"9496f386a0da4601cad22c479cc5543913a4d67f","modified":1616240437148},{"_id":"public/libs/gitment/gitment-default.css","hash":"2903c59ee06b965bef32e937bd69f5b0b2190717","modified":1616240437148},{"_id":"public/libs/gitalk/gitalk.min.js","hash":"8fefe38f28804f90116bdcb74a0875c9de9f3b7d","modified":1616240437148},{"_id":"public/libs/gitment/gitment.js","hash":"28c02c45ce568e084cd1041dc493f83f9c6c88c6","modified":1616240437148},{"_id":"public/libs/jquery/jquery.min.js","hash":"2115753ca5fb7032aec498db7bb5dca624dbe6be","modified":1616240437148},{"_id":"public/libs/masonry/masonry.pkgd.min.js","hash":"ff940b4ea68368ca0e4d5560cbb79fb147dfc3c5","modified":1616240437148},{"_id":"public/libs/materialize/materialize.min.css","hash":"a69d456e3345e7f59cd0d47d1b3e70fd4a496a05","modified":1616240437148},{"_id":"public/libs/materialize/materialize.min.js","hash":"c8b4c65651921d888cf5f27430dfe2ad190d35bf","modified":1616240437148},{"_id":"public/libs/minivaline/MiniValine.js","hash":"fbb58c37e2c74f127ae0c566afa9b48889aab79f","modified":1616240437148},{"_id":"public/libs/valine/Valine.min.js","hash":"6cbdbf91e1f046dd41267a5ff0691a1fccba99df","modified":1616240437148},{"_id":"public/libs/valine/av-min.js","hash":"541efb9edc1ce425cbe3897cfc25803211fe6a05","modified":1616240437148},{"_id":"public/libs/awesome/css/all.css","hash":"ecc41e32ad2696877a1656749841f3b5543bbe3d","modified":1616240437148},{"_id":"public/libs/lightGallery/css/lightgallery.min.css","hash":"1b7227237f9785c66062a4811508916518e4132c","modified":1616240437148},{"_id":"public/libs/lightGallery/js/lightgallery-all.min.js","hash":"9f5ef4bc8a0a3c746ca4f3c3e6d64493b1a977d8","modified":1616240437148},{"_id":"public/libs/share/js/jquery.share.min.js","hash":"41367dcb857e02e3c417ebe68a554ce1d4430806","modified":1616240437148},{"_id":"public/libs/share/js/social-share.min.js","hash":"a3090a02786dcd4efc6355c1c1dc978add8d6827","modified":1616240437148},{"_id":"public/medias/banner/4.jpg","hash":"e5ac5033678afa9d69edffe9a61004f836cb5734","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-brands-400.svg","hash":"5e2d2a159294576bea69cc3360efb5ffe110ab2d","modified":1616240437148},{"_id":"public/libs/awesome/webfonts/fa-solid-900.svg","hash":"7da88b19e1486f8c968d3cf5ab3f194f01ea17fd","modified":1616240437148}],"Category":[{"name":"Machine Learning","_id":"ckmhntquy0004p8achdiq3p5z"},{"name":"regression","parent":"ckmhntquy0004p8achdiq3p5z","_id":"ckmhntquz0005p8ac01l60j56"},{"name":"classification","parent":"ckmhntquy0004p8achdiq3p5z","_id":"ckmhntquz0006p8ac76bjbzpd"}],"Data":[],"Page":[],"Post":[{"title":"Intro_Math","date":"2021-03-20T10:54:42.000Z","mathjax":true,"_content":"\n# Introduction\n\n对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：\n$$\nX_{N\\times p}=(x_{1},x_{2},\\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\\cdots,x_{ip})^{T}\n$$\n 这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\\theta)$ 生成的。\n\n## 频率派的观点\n\n$p(x|\\theta)$中的 $\\theta$ 是一个常量。对于 $N$ 个观测来说观测集的概率为 $p(X|\\theta)\\mathop{=}\\limits _{iid}\\prod\\limits _{i=1}^{N}p(x_{i}|\\theta))$ 。为了求 $\\theta$ 的大小，我们采用最大对数似然MLE的方法：\n\n$$\n\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)\n$$\n\n\n## 贝叶斯派的观点\n\n贝叶斯派认为 $p(x|\\theta)$ 中的 $\\theta$ 不是一个常量。这个 $\\theta$ 满足一个预设的先验的分布 $\\theta\\sim p(\\theta)$ 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：\n\n$$\np(\\theta|X)=\\frac{p(X|\\theta)\\cdot p(\\theta)}{p(X)}=\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}\n$$\n为了求 $\\theta$ 的值，我们要最大化这个参数后验MAP：\n\n\n$$\n\\theta_{MAP}=\\mathop{argmax}\\limits _{\\theta}p(\\theta|X)=\\mathop{argmax}\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)\n$$\n其中第二个等号是由于分母和 $\\theta$ 没有关系。求解这个 $\\theta$ 值后计算$\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}$ ，就得到了参数的后验概率。其中 $p(X|\\theta)$ 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测贝叶斯预测：\n$$\np(x_{new}|X)=\\int\\limits _{\\theta}p(x_{new}|\\theta)\\cdot p(\\theta|X)d\\theta\n$$\n 其中积分中的被乘数是模型，乘数是后验分布。\n\n## 小结\n\n频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如 MCMC 有很多应用。\n\n# MathBasics\n\n## 高斯分布\n\n### 一维情况 MLE\n\n高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：\n\n$$\n\\theta=(\\mu,\\Sigma)=(\\mu,\\sigma^{2}),\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)\n$$\n一般地，高斯分布的概率密度函数PDF写为：\n\n$$\np(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n$$\n带入 MLE 中我们考虑一维的情况\n\n$$\n\\log p(X|\\theta)=\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)=\\sum\\limits _{i=1}^{N}\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-(x_{i}-\\mu)^{2}/2\\sigma^{2})\n$$\n首先对 $\\mu$ 的极值可以得到 ：\n$$\n\\mu_{MLE}=\\mathop{argmax}\\limits _{\\mu}\\log p(X|\\theta)=\\mathop{argmax}\\limits _{\\mu}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n$$\n 于是：\n$$\n\\frac{\\partial}{\\partial\\mu}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}=0\\longrightarrow\\mu_{MLE}=\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}\n$$\n其次对 $\\theta$ 中的另一个参数 $\\sigma$ ，有：\n$$\n\\begin{align}\n\\sigma_{MLE}=\\mathop{argmax}\\limits _{\\sigma}\\log p(X|\\theta)&=\\mathop{argmax}\\limits _{\\sigma}\\sum\\limits _{i=1}^{N}[-\\log\\sigma-\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]\\nonumber\\\\\n&=\\mathop{argmin}\\limits _{\\sigma}\\sum\\limits _{i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]\n\\end{align}\n$$\n于是：\n$$\n\\frac{\\partial}{\\partial\\sigma}\\sum\\limits _{i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]=0\\longrightarrow\\sigma_{MLE}^{2}=\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n$$\n值得注意的是，上面的推导中，首先对 $\\mu$ 求 MLE， 然后利用这个结果求 $\\sigma_{MLE}$ ，因此可以预期的是对数据集求期望时 $\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}]$ 是无偏差的：\n$$\n\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}]=\\frac{1}{N}\\sum\\limits _{i=1}^{N}\\mathbb{E}_{\\mathcal{D}}[x_{i}]=\\mu\n$$\n但是当对 $\\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\\sigma_{MLE}$ 是 有偏的：\n\n$$\n\\begin{align}\n\\mathbb{E}_{\\mathcal{D}}[\\sigma_{MLE}^{2}]&=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu_{MLE})^{2}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}^{2}-2x_{i}\\mu_{MLE}+\\mu_{MLE}^{2})\\nonumber\n\\\\&=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu_{MLE}^{2}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu^{2}+\\mu^{2}-\\mu_{MLE}^{2}]\\nonumber\\\\\n&= \\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu^{2}]-\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}-\\mu^{2}]=\\sigma^{2}-(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}]-\\mu^{2})\\nonumber\\\\&=\\sigma^{2}-(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}]-\\mathbb{E}_{\\mathcal{D}}^{2}[\\mu_{MLE}])=\\sigma^{2}-Var[\\mu_{MLE}]\\nonumber\\\\&=\\sigma^{2}-Var[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}]=\\sigma^{2}-\\frac{1}{N^{2}}\\sum\\limits _{i=1}^{N}Var[x_{i}]=\\frac{N-1}{N}\\sigma^{2}\n\\end{align}\n$$\n所以：\n$$\n\\hat{\\sigma}^{2}=\\frac{1}{N-1}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n$$\n\n\n### 多维情况\n\n多维高斯分布表达式为：\n$$\np(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n$$\n其中 $x,\\mu\\in\\mathbb{R}^{p},\\Sigma\\in\\mathbb{R}^{p\\times p}$ ，$\\Sigma$ 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，$\\Sigma=U\\Lambda U^{T}=(u_{1},u_{2},\\cdots,u_{p})diag(\\lambda_{i})(u_{1},u_{2},\\cdots,u_{p})^{T}=\\sum\\limits _{i=1}^{p}u_{i}\\lambda_{i}u_{i}^{T}$ ，于是：\n\n$$\n\\Sigma^{-1}=\\sum\\limits _{i=1}^{p}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}\n$$\n\n$$\n\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits _{i=1}^{p}(x-\\mu)^{T}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits _{i=1}^{p}\\frac{y_{i}^{2}}{\\lambda_{i}}\n$$\n\n我们注意到 $y_{i}$ 是 $x-\\mu$ 在特征向量 $u_{i}$ 上的投影长度，因此上式子就是 $\\Delta$ 取不同值时的同心椭圆。\n\n下面我们看多维高斯模型在实际应用时的两个问题\n\n1.  参数 $\\Sigma,\\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\\Sigma$ 有 $\\frac{p(p+1)}{2}$ 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。\n\n2.  第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。\n\n下面对多维高斯分布的常用定理进行介绍。\n\n我们记 $x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}$，已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma)$。\n\n首先是一个高斯分布的定理：\n\n>   定理：已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b$，那么 $y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)$。\n>\n>   证明：$\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T$。\n\n下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。\n\n1. $x_a=\\begin{pmatrix}\\mathbb{I}_{m\\times m}&\\mathbb{O}_{m\\times n})\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}$，代入定理中得到：\n   $$\n   \\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_a\\\\\n   Var[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa}\n   $$\n   所以 $x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})$。\n\n2. 同样的，$x_b\\sim\\mathcal{N}(\\mu_b,\\Sigma_{bb})$。\n\n3. 对于两个条件概率，我们引入三个量：\n   $$\n   x_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n   \\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\\\\n   \\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n   $$\n   特别的，最后一个式子叫做 $\\Sigma_{bb}$ 的 Schur Complementary。可以看到：\n   $$\n   x_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\n   $$\n   所以：\n   $$\n   \\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_{b\\cdot a}\\\\\n   Var[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\\\mathbb{I}_{n\\times n}\\end{pmatrix}=\\Sigma_{bb\\cdot a}\n   $$\n   利用这三个量可以得到 $x_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a$。因此：\n   $$\n   \\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\n   $$\n\n   $$\n   Var[x_b|x_a]=\\Sigma_{bb\\cdot a}\n   $$\n\n   这里同样用到了定理。\n\n4. 同样：\n   $$\n   x_{a\\cdot b}=x_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\\\\\n   \\mu_{a\\cdot b}=\\mu_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\mu_b\\\\\n   \\Sigma_{aa\\cdot b}=\\Sigma_{aa}-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\Sigma_{ba}\n   $$\n   所以：\n   $$\n   \\mathbb{E}[x_a|x_b]=\\mu_{a\\cdot b}+\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\n   $$\n\n   $$\n   Var[x_a|x_b]=\\Sigma_{aa\\cdot b}\n   $$\n\n下面利用上边四个量，求解线性模型：\n\n>   已知：$p(x)=\\mathcal{N}(\\mu,\\Lambda^{-1}),p(y|x)=\\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。\n>\n>   解：令 $y=Ax+b+\\epsilon,\\epsilon\\sim\\mathcal{N}(0,L^{-1})$，所以 $\\mathbb{E}[y]=\\mathbb{E}[Ax+b+\\epsilon]=A\\mu+b$，$Var[y]=A \\Lambda^{-1}A^T+L^{-1}$，因此：\n>   $$\n>   p(y)=\\mathcal{N}(A\\mu+b,L^{-1}+A\\Lambda^{-1}A^T)\n>   $$\n>   引入 $z=\\begin{pmatrix}x\\\\y\\end{pmatrix}$，我们可以得到 $Cov[x,y]=\\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])^T]$。对于这个协方差可以直接计算：\n>   $$\n>   \\begin{align}\n>   Cov(x,y)&=\\mathbb{E}[(x-\\mu)(Ax-A\\mu+\\epsilon)^T]=\\mathbb{E}[(x-\\mu)(x-\\mu)^TA^T]=Var[x]A^T=\\Lambda^{-1}A^T\n>   \\end{align}\n>   $$\n>   注意到协方差矩阵的对称性，所以 $p(z)=\\mathcal{N}\\begin{pmatrix}\\mu\\\\A\\mu+b\\end{pmatrix},\\begin{pmatrix}\\Lambda^{-1}&\\Lambda^{-1}A^T\\\\A\\Lambda^{-1}&L^{-1}+A\\Lambda^{-1}A^T\\end{pmatrix})$。根据之前的公式，我们可以得到：\n>   $$\n>   \\mathbb{E}[x|y]=\\mu+\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}(y-A\\mu-b)\n>   $$\n>\n>   $$\n>   Var[x|y]=\\Lambda^{-1}-\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}A\\Lambda^{-1}\n>   $$\n\n","source":"_posts/Intro-Math.md","raw":"---\ntitle: Intro_Math\ndate: 2021-03-20 18:54:42\ntags:\nmathjax: true\n---\n\n# Introduction\n\n对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号：\n$$\nX_{N\\times p}=(x_{1},x_{2},\\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\\cdots,x_{ip})^{T}\n$$\n 这个记号表示有 $N$ 个样本，每个样本都是 $p$ 维向量。其中每个观测都是由 $p(x|\\theta)$ 生成的。\n\n## 频率派的观点\n\n$p(x|\\theta)$中的 $\\theta$ 是一个常量。对于 $N$ 个观测来说观测集的概率为 $p(X|\\theta)\\mathop{=}\\limits _{iid}\\prod\\limits _{i=1}^{N}p(x_{i}|\\theta))$ 。为了求 $\\theta$ 的大小，我们采用最大对数似然MLE的方法：\n\n$$\n\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)\n$$\n\n\n## 贝叶斯派的观点\n\n贝叶斯派认为 $p(x|\\theta)$ 中的 $\\theta$ 不是一个常量。这个 $\\theta$ 满足一个预设的先验的分布 $\\theta\\sim p(\\theta)$ 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：\n\n$$\np(\\theta|X)=\\frac{p(X|\\theta)\\cdot p(\\theta)}{p(X)}=\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}\n$$\n为了求 $\\theta$ 的值，我们要最大化这个参数后验MAP：\n\n\n$$\n\\theta_{MAP}=\\mathop{argmax}\\limits _{\\theta}p(\\theta|X)=\\mathop{argmax}\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)\n$$\n其中第二个等号是由于分母和 $\\theta$ 没有关系。求解这个 $\\theta$ 值后计算$\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}$ ，就得到了参数的后验概率。其中 $p(X|\\theta)$ 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测贝叶斯预测：\n$$\np(x_{new}|X)=\\int\\limits _{\\theta}p(x_{new}|\\theta)\\cdot p(\\theta|X)d\\theta\n$$\n 其中积分中的被乘数是模型，乘数是后验分布。\n\n## 小结\n\n频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如 MCMC 有很多应用。\n\n# MathBasics\n\n## 高斯分布\n\n### 一维情况 MLE\n\n高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：\n\n$$\n\\theta=(\\mu,\\Sigma)=(\\mu,\\sigma^{2}),\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)\n$$\n一般地，高斯分布的概率密度函数PDF写为：\n\n$$\np(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n$$\n带入 MLE 中我们考虑一维的情况\n\n$$\n\\log p(X|\\theta)=\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)=\\sum\\limits _{i=1}^{N}\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-(x_{i}-\\mu)^{2}/2\\sigma^{2})\n$$\n首先对 $\\mu$ 的极值可以得到 ：\n$$\n\\mu_{MLE}=\\mathop{argmax}\\limits _{\\mu}\\log p(X|\\theta)=\\mathop{argmax}\\limits _{\\mu}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n$$\n 于是：\n$$\n\\frac{\\partial}{\\partial\\mu}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}=0\\longrightarrow\\mu_{MLE}=\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}\n$$\n其次对 $\\theta$ 中的另一个参数 $\\sigma$ ，有：\n$$\n\\begin{align}\n\\sigma_{MLE}=\\mathop{argmax}\\limits _{\\sigma}\\log p(X|\\theta)&=\\mathop{argmax}\\limits _{\\sigma}\\sum\\limits _{i=1}^{N}[-\\log\\sigma-\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]\\nonumber\\\\\n&=\\mathop{argmin}\\limits _{\\sigma}\\sum\\limits _{i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]\n\\end{align}\n$$\n于是：\n$$\n\\frac{\\partial}{\\partial\\sigma}\\sum\\limits _{i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]=0\\longrightarrow\\sigma_{MLE}^{2}=\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n$$\n值得注意的是，上面的推导中，首先对 $\\mu$ 求 MLE， 然后利用这个结果求 $\\sigma_{MLE}$ ，因此可以预期的是对数据集求期望时 $\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}]$ 是无偏差的：\n$$\n\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}]=\\frac{1}{N}\\sum\\limits _{i=1}^{N}\\mathbb{E}_{\\mathcal{D}}[x_{i}]=\\mu\n$$\n但是当对 $\\sigma_{MLE}$ 求 期望的时候由于使用了单个数据集的 $\\mu_{MLE}$，因此对所有数据集求期望的时候我们会发现 $\\sigma_{MLE}$ 是 有偏的：\n\n$$\n\\begin{align}\n\\mathbb{E}_{\\mathcal{D}}[\\sigma_{MLE}^{2}]&=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu_{MLE})^{2}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}^{2}-2x_{i}\\mu_{MLE}+\\mu_{MLE}^{2})\\nonumber\n\\\\&=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu_{MLE}^{2}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu^{2}+\\mu^{2}-\\mu_{MLE}^{2}]\\nonumber\\\\\n&= \\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu^{2}]-\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}-\\mu^{2}]=\\sigma^{2}-(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}]-\\mu^{2})\\nonumber\\\\&=\\sigma^{2}-(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}]-\\mathbb{E}_{\\mathcal{D}}^{2}[\\mu_{MLE}])=\\sigma^{2}-Var[\\mu_{MLE}]\\nonumber\\\\&=\\sigma^{2}-Var[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}]=\\sigma^{2}-\\frac{1}{N^{2}}\\sum\\limits _{i=1}^{N}Var[x_{i}]=\\frac{N-1}{N}\\sigma^{2}\n\\end{align}\n$$\n所以：\n$$\n\\hat{\\sigma}^{2}=\\frac{1}{N-1}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n$$\n\n\n### 多维情况\n\n多维高斯分布表达式为：\n$$\np(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n$$\n其中 $x,\\mu\\in\\mathbb{R}^{p},\\Sigma\\in\\mathbb{R}^{p\\times p}$ ，$\\Sigma$ 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 $x$ 和 $\\mu$ 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，$\\Sigma=U\\Lambda U^{T}=(u_{1},u_{2},\\cdots,u_{p})diag(\\lambda_{i})(u_{1},u_{2},\\cdots,u_{p})^{T}=\\sum\\limits _{i=1}^{p}u_{i}\\lambda_{i}u_{i}^{T}$ ，于是：\n\n$$\n\\Sigma^{-1}=\\sum\\limits _{i=1}^{p}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}\n$$\n\n$$\n\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits _{i=1}^{p}(x-\\mu)^{T}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits _{i=1}^{p}\\frac{y_{i}^{2}}{\\lambda_{i}}\n$$\n\n我们注意到 $y_{i}$ 是 $x-\\mu$ 在特征向量 $u_{i}$ 上的投影长度，因此上式子就是 $\\Delta$ 取不同值时的同心椭圆。\n\n下面我们看多维高斯模型在实际应用时的两个问题\n\n1.  参数 $\\Sigma,\\mu$ 的自由度为 $O(p^{2})$ 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 $\\Sigma$ 有 $\\frac{p(p+1)}{2}$ 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。\n\n2.  第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。\n\n下面对多维高斯分布的常用定理进行介绍。\n\n我们记 $x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}$，已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma)$。\n\n首先是一个高斯分布的定理：\n\n>   定理：已知 $x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b$，那么 $y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)$。\n>\n>   证明：$\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b$，$Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T$。\n\n下面利用这个定理得到 $p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)$ 这四个量。\n\n1. $x_a=\\begin{pmatrix}\\mathbb{I}_{m\\times m}&\\mathbb{O}_{m\\times n})\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}$，代入定理中得到：\n   $$\n   \\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_a\\\\\n   Var[x_a]=\\begin{pmatrix}\\mathbb{I}&\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa}\n   $$\n   所以 $x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})$。\n\n2. 同样的，$x_b\\sim\\mathcal{N}(\\mu_b,\\Sigma_{bb})$。\n\n3. 对于两个条件概率，我们引入三个量：\n   $$\n   x_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n   \\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\\\\n   \\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n   $$\n   特别的，最后一个式子叫做 $\\Sigma_{bb}$ 的 Schur Complementary。可以看到：\n   $$\n   x_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\n   $$\n   所以：\n   $$\n   \\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_{b\\cdot a}\\\\\n   Var[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&\\Sigma_{ab}\\\\\\Sigma_{ba}&\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\\\mathbb{I}_{n\\times n}\\end{pmatrix}=\\Sigma_{bb\\cdot a}\n   $$\n   利用这三个量可以得到 $x_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a$。因此：\n   $$\n   \\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\n   $$\n\n   $$\n   Var[x_b|x_a]=\\Sigma_{bb\\cdot a}\n   $$\n\n   这里同样用到了定理。\n\n4. 同样：\n   $$\n   x_{a\\cdot b}=x_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\\\\\n   \\mu_{a\\cdot b}=\\mu_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\mu_b\\\\\n   \\Sigma_{aa\\cdot b}=\\Sigma_{aa}-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\Sigma_{ba}\n   $$\n   所以：\n   $$\n   \\mathbb{E}[x_a|x_b]=\\mu_{a\\cdot b}+\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\n   $$\n\n   $$\n   Var[x_a|x_b]=\\Sigma_{aa\\cdot b}\n   $$\n\n下面利用上边四个量，求解线性模型：\n\n>   已知：$p(x)=\\mathcal{N}(\\mu,\\Lambda^{-1}),p(y|x)=\\mathcal{N}(Ax+b,L^{-1})$，求解：$p(y),p(x|y)$。\n>\n>   解：令 $y=Ax+b+\\epsilon,\\epsilon\\sim\\mathcal{N}(0,L^{-1})$，所以 $\\mathbb{E}[y]=\\mathbb{E}[Ax+b+\\epsilon]=A\\mu+b$，$Var[y]=A \\Lambda^{-1}A^T+L^{-1}$，因此：\n>   $$\n>   p(y)=\\mathcal{N}(A\\mu+b,L^{-1}+A\\Lambda^{-1}A^T)\n>   $$\n>   引入 $z=\\begin{pmatrix}x\\\\y\\end{pmatrix}$，我们可以得到 $Cov[x,y]=\\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])^T]$。对于这个协方差可以直接计算：\n>   $$\n>   \\begin{align}\n>   Cov(x,y)&=\\mathbb{E}[(x-\\mu)(Ax-A\\mu+\\epsilon)^T]=\\mathbb{E}[(x-\\mu)(x-\\mu)^TA^T]=Var[x]A^T=\\Lambda^{-1}A^T\n>   \\end{align}\n>   $$\n>   注意到协方差矩阵的对称性，所以 $p(z)=\\mathcal{N}\\begin{pmatrix}\\mu\\\\A\\mu+b\\end{pmatrix},\\begin{pmatrix}\\Lambda^{-1}&\\Lambda^{-1}A^T\\\\A\\Lambda^{-1}&L^{-1}+A\\Lambda^{-1}A^T\\end{pmatrix})$。根据之前的公式，我们可以得到：\n>   $$\n>   \\mathbb{E}[x|y]=\\mu+\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}(y-A\\mu-b)\n>   $$\n>\n>   $$\n>   Var[x|y]=\\Lambda^{-1}-\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}A\\Lambda^{-1}\n>   $$\n\n","slug":"Intro-Math","published":1,"updated":"2021-03-20T10:58:28.914Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmhntqur0000p8ac8vqc3nab","content":"<h1 id=\"introduction\">Introduction</h1>\n<p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号： <span class=\"math display\">\\[\nX_{N\\times p}=(x_{1},x_{2},\\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\\cdots,x_{ip})^{T}\n\\]</span> 这个记号表示有 <span class=\"math inline\">\\(N\\)</span> 个样本，每个样本都是 <span class=\"math inline\">\\(p\\)</span> 维向量。其中每个观测都是由 <span class=\"math inline\">\\(p(x|\\theta)\\)</span> 生成的。</p>\n<h2 id=\"频率派的观点\">频率派的观点</h2>\n<p><span class=\"math inline\">\\(p(x|\\theta)\\)</span>中的 <span class=\"math inline\">\\(\\theta\\)</span> 是一个常量。对于 <span class=\"math inline\">\\(N\\)</span> 个观测来说观测集的概率为 <span class=\"math inline\">\\(p(X|\\theta)\\mathop{=}\\limits _{iid}\\prod\\limits _{i=1}^{N}p(x_{i}|\\theta))\\)</span> 。为了求 <span class=\"math inline\">\\(\\theta\\)</span> 的大小，我们采用最大对数似然MLE的方法：</p>\n<p><span class=\"math display\">\\[\n\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)\n\\]</span></p>\n<h2 id=\"贝叶斯派的观点\">贝叶斯派的观点</h2>\n<p>贝叶斯派认为 <span class=\"math inline\">\\(p(x|\\theta)\\)</span> 中的 <span class=\"math inline\">\\(\\theta\\)</span> 不是一个常量。这个 <span class=\"math inline\">\\(\\theta\\)</span> 满足一个预设的先验的分布 <span class=\"math inline\">\\(\\theta\\sim p(\\theta)\\)</span> 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p>\n<p><span class=\"math display\">\\[\np(\\theta|X)=\\frac{p(X|\\theta)\\cdot p(\\theta)}{p(X)}=\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}\n\\]</span> 为了求 <span class=\"math inline\">\\(\\theta\\)</span> 的值，我们要最大化这个参数后验MAP：</p>\n<p><span class=\"math display\">\\[\n\\theta_{MAP}=\\mathop{argmax}\\limits _{\\theta}p(\\theta|X)=\\mathop{argmax}\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)\n\\]</span> 其中第二个等号是由于分母和 <span class=\"math inline\">\\(\\theta\\)</span> 没有关系。求解这个 <span class=\"math inline\">\\(\\theta\\)</span> 值后计算<span class=\"math inline\">\\(\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}\\)</span> ，就得到了参数的后验概率。其中 <span class=\"math inline\">\\(p(X|\\theta)\\)</span> 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测贝叶斯预测： <span class=\"math display\">\\[\np(x_{new}|X)=\\int\\limits _{\\theta}p(x_{new}|\\theta)\\cdot p(\\theta|X)d\\theta\n\\]</span> 其中积分中的被乘数是模型，乘数是后验分布。</p>\n<h2 id=\"小结\">小结</h2>\n<p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如 MCMC 有很多应用。</p>\n<h1 id=\"mathbasics\">MathBasics</h1>\n<h2 id=\"高斯分布\">高斯分布</h2>\n<h3 id=\"一维情况-mle\">一维情况 MLE</h3>\n<p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p>\n<p><span class=\"math display\">\\[\n\\theta=(\\mu,\\Sigma)=(\\mu,\\sigma^{2}),\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)\n\\]</span> 一般地，高斯分布的概率密度函数PDF写为：</p>\n<p><span class=\"math display\">\\[\np(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n\\]</span> 带入 MLE 中我们考虑一维的情况</p>\n<p><span class=\"math display\">\\[\n\\log p(X|\\theta)=\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)=\\sum\\limits _{i=1}^{N}\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-(x_{i}-\\mu)^{2}/2\\sigma^{2})\n\\]</span> 首先对 <span class=\"math inline\">\\(\\mu\\)</span> 的极值可以得到 ： <span class=\"math display\">\\[\n\\mu_{MLE}=\\mathop{argmax}\\limits _{\\mu}\\log p(X|\\theta)=\\mathop{argmax}\\limits _{\\mu}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n\\]</span> 于是： <span class=\"math display\">\\[\n\\frac{\\partial}{\\partial\\mu}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}=0\\longrightarrow\\mu_{MLE}=\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}\n\\]</span> 其次对 <span class=\"math inline\">\\(\\theta\\)</span> 中的另一个参数 <span class=\"math inline\">\\(\\sigma\\)</span> ，有： <span class=\"math display\">\\[\n\\begin{align}\n\\sigma_{MLE}=\\mathop{argmax}\\limits _{\\sigma}\\log p(X|\\theta)&amp;=\\mathop{argmax}\\limits _{\\sigma}\\sum\\limits _{i=1}^{N}[-\\log\\sigma-\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]\\nonumber\\\\\n&amp;=\\mathop{argmin}\\limits _{\\sigma}\\sum\\limits _{i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]\n\\end{align}\n\\]</span> 于是： <span class=\"math display\">\\[\n\\frac{\\partial}{\\partial\\sigma}\\sum\\limits _{i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]=0\\longrightarrow\\sigma_{MLE}^{2}=\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n\\]</span> 值得注意的是，上面的推导中，首先对 <span class=\"math inline\">\\(\\mu\\)</span> 求 MLE， 然后利用这个结果求 <span class=\"math inline\">\\(\\sigma_{MLE}\\)</span> ，因此可以预期的是对数据集求期望时 <span class=\"math inline\">\\(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}]\\)</span> 是无偏差的： <span class=\"math display\">\\[\n\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}]=\\frac{1}{N}\\sum\\limits _{i=1}^{N}\\mathbb{E}_{\\mathcal{D}}[x_{i}]=\\mu\n\\]</span> 但是当对 <span class=\"math inline\">\\(\\sigma_{MLE}\\)</span> 求 期望的时候由于使用了单个数据集的 <span class=\"math inline\">\\(\\mu_{MLE}\\)</span>，因此对所有数据集求期望的时候我们会发现 <span class=\"math inline\">\\(\\sigma_{MLE}\\)</span> 是 有偏的：</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\mathbb{E}_{\\mathcal{D}}[\\sigma_{MLE}^{2}]&amp;=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu_{MLE})^{2}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}^{2}-2x_{i}\\mu_{MLE}+\\mu_{MLE}^{2})\\nonumber\n\\\\&amp;=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu_{MLE}^{2}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu^{2}+\\mu^{2}-\\mu_{MLE}^{2}]\\nonumber\\\\\n&amp;= \\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu^{2}]-\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}-\\mu^{2}]=\\sigma^{2}-(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}]-\\mu^{2})\\nonumber\\\\&amp;=\\sigma^{2}-(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}]-\\mathbb{E}_{\\mathcal{D}}^{2}[\\mu_{MLE}])=\\sigma^{2}-Var[\\mu_{MLE}]\\nonumber\\\\&amp;=\\sigma^{2}-Var[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}]=\\sigma^{2}-\\frac{1}{N^{2}}\\sum\\limits _{i=1}^{N}Var[x_{i}]=\\frac{N-1}{N}\\sigma^{2}\n\\end{align}\n\\]</span> 所以： <span class=\"math display\">\\[\n\\hat{\\sigma}^{2}=\\frac{1}{N-1}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n\\]</span></p>\n<h3 id=\"多维情况\">多维情况</h3>\n<p>多维高斯分布表达式为： <span class=\"math display\">\\[\np(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n\\]</span> 其中 <span class=\"math inline\">\\(x,\\mu\\in\\mathbb{R}^{p},\\Sigma\\in\\mathbb{R}^{p\\times p}\\)</span> ，<span class=\"math inline\">\\(\\Sigma\\)</span> 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(\\mu\\)</span> 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，<span class=\"math inline\">\\(\\Sigma=U\\Lambda U^{T}=(u_{1},u_{2},\\cdots,u_{p})diag(\\lambda_{i})(u_{1},u_{2},\\cdots,u_{p})^{T}=\\sum\\limits _{i=1}^{p}u_{i}\\lambda_{i}u_{i}^{T}\\)</span> ，于是：</p>\n<p><span class=\"math display\">\\[\n\\Sigma^{-1}=\\sum\\limits _{i=1}^{p}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}\n\\]</span></p>\n<p><span class=\"math display\">\\[\n\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits _{i=1}^{p}(x-\\mu)^{T}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits _{i=1}^{p}\\frac{y_{i}^{2}}{\\lambda_{i}}\n\\]</span></p>\n<p>我们注意到 <span class=\"math inline\">\\(y_{i}\\)</span> 是 <span class=\"math inline\">\\(x-\\mu\\)</span> 在特征向量 <span class=\"math inline\">\\(u_{i}\\)</span> 上的投影长度，因此上式子就是 <span class=\"math inline\">\\(\\Delta\\)</span> 取不同值时的同心椭圆。</p>\n<p>下面我们看多维高斯模型在实际应用时的两个问题</p>\n<ol type=\"1\">\n<li><p>参数 <span class=\"math inline\">\\(\\Sigma,\\mu\\)</span> 的自由度为 <span class=\"math inline\">\\(O(p^{2})\\)</span> 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 <span class=\"math inline\">\\(\\Sigma\\)</span> 有 <span class=\"math inline\">\\(\\frac{p(p+1)}{2}\\)</span> 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。</p></li>\n<li><p>第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。</p></li>\n</ol>\n<p>下面对多维高斯分布的常用定理进行介绍。</p>\n<p>我们记 <span class=\"math inline\">\\(x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}\\)</span>，已知 <span class=\"math inline\">\\(x\\sim\\mathcal{N}(\\mu,\\Sigma)\\)</span>。</p>\n<p>首先是一个高斯分布的定理：</p>\n<blockquote>\n<p>定理：已知 <span class=\"math inline\">\\(x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b\\)</span>，那么 <span class=\"math inline\">\\(y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)\\)</span>。</p>\n<p>证明：<span class=\"math inline\">\\(\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b\\)</span>，<span class=\"math inline\">\\(Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T\\)</span>。</p>\n</blockquote>\n<p>下面利用这个定理得到 <span class=\"math inline\">\\(p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)\\)</span> 这四个量。</p>\n<ol type=\"1\">\n<li><p><span class=\"math inline\">\\(x_a=\\begin{pmatrix}\\mathbb{I}_{m\\times m}&amp;\\mathbb{O}_{m\\times n})\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\\)</span>，代入定理中得到： <span class=\"math display\">\\[\n\\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&amp;\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_a\\\\\nVar[x_a]=\\begin{pmatrix}\\mathbb{I}&amp;\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa}\n\\]</span> 所以 <span class=\"math inline\">\\(x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})\\)</span>。</p></li>\n<li><p>同样的，<span class=\"math inline\">\\(x_b\\sim\\mathcal{N}(\\mu_b,\\Sigma_{bb})\\)</span>。</p></li>\n<li><p>对于两个条件概率，我们引入三个量： <span class=\"math display\">\\[\nx_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n\\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\\\\n\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n\\]</span> 特别的，最后一个式子叫做 <span class=\"math inline\">\\(\\Sigma_{bb}\\)</span> 的 Schur Complementary。可以看到： <span class=\"math display\">\\[\nx_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\n\\]</span> 所以： <span class=\"math display\">\\[\n\\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_{b\\cdot a}\\\\\nVar[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\\\mathbb{I}_{n\\times n}\\end{pmatrix}=\\Sigma_{bb\\cdot a}\n\\]</span> 利用这三个量可以得到 <span class=\"math inline\">\\(x_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\)</span>。因此： <span class=\"math display\">\\[\n\\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\n\\]</span></p>\n<p><span class=\"math display\">\\[\nVar[x_b|x_a]=\\Sigma_{bb\\cdot a}\n\\]</span></p>\n<p>这里同样用到了定理。</p></li>\n<li><p>同样： <span class=\"math display\">\\[\nx_{a\\cdot b}=x_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\\\\\n\\mu_{a\\cdot b}=\\mu_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\mu_b\\\\\n\\Sigma_{aa\\cdot b}=\\Sigma_{aa}-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\Sigma_{ba}\n\\]</span> 所以： <span class=\"math display\">\\[\n\\mathbb{E}[x_a|x_b]=\\mu_{a\\cdot b}+\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\n\\]</span></p>\n<p><span class=\"math display\">\\[\nVar[x_a|x_b]=\\Sigma_{aa\\cdot b}\n\\]</span></p></li>\n</ol>\n<p>下面利用上边四个量，求解线性模型：</p>\n<blockquote>\n<p>已知：<span class=\"math inline\">\\(p(x)=\\mathcal{N}(\\mu,\\Lambda^{-1}),p(y|x)=\\mathcal{N}(Ax+b,L^{-1})\\)</span>，求解：<span class=\"math inline\">\\(p(y),p(x|y)\\)</span>。</p>\n<p>解：令 <span class=\"math inline\">\\(y=Ax+b+\\epsilon,\\epsilon\\sim\\mathcal{N}(0,L^{-1})\\)</span>，所以 <span class=\"math inline\">\\(\\mathbb{E}[y]=\\mathbb{E}[Ax+b+\\epsilon]=A\\mu+b\\)</span>，<span class=\"math inline\">\\(Var[y]=A \\Lambda^{-1}A^T+L^{-1}\\)</span>，因此： <span class=\"math display\">\\[\n  p(y)=\\mathcal{N}(A\\mu+b,L^{-1}+A\\Lambda^{-1}A^T)\n  \\]</span> 引入 <span class=\"math inline\">\\(z=\\begin{pmatrix}x\\\\y\\end{pmatrix}\\)</span>，我们可以得到 <span class=\"math inline\">\\(Cov[x,y]=\\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])^T]\\)</span>。对于这个协方差可以直接计算： <span class=\"math display\">\\[\n  \\begin{align}\n  Cov(x,y)&amp;=\\mathbb{E}[(x-\\mu)(Ax-A\\mu+\\epsilon)^T]=\\mathbb{E}[(x-\\mu)(x-\\mu)^TA^T]=Var[x]A^T=\\Lambda^{-1}A^T\n  \\end{align}\n  \\]</span> 注意到协方差矩阵的对称性，所以 <span class=\"math inline\">\\(p(z)=\\mathcal{N}\\begin{pmatrix}\\mu\\\\A\\mu+b\\end{pmatrix},\\begin{pmatrix}\\Lambda^{-1}&amp;\\Lambda^{-1}A^T\\\\A\\Lambda^{-1}&amp;L^{-1}+A\\Lambda^{-1}A^T\\end{pmatrix})\\)</span>。根据之前的公式，我们可以得到： <span class=\"math display\">\\[\n  \\mathbb{E}[x|y]=\\mu+\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}(y-A\\mu-b)\n  \\]</span></p>\n<p><span class=\"math display\">\\[\n  Var[x|y]=\\Lambda^{-1}-\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}A\\Lambda^{-1}\n  \\]</span></p>\n</blockquote>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"introduction\">Introduction</h1>\n<p>对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派。后面我们对观测集采用下面记号： <span class=\"math display\">\\[\nX_{N\\times p}=(x_{1},x_{2},\\cdots,x_{N})^{T},x_{i}=(x_{i1},x_{i2},\\cdots,x_{ip})^{T}\n\\]</span> 这个记号表示有 <span class=\"math inline\">\\(N\\)</span> 个样本，每个样本都是 <span class=\"math inline\">\\(p\\)</span> 维向量。其中每个观测都是由 <span class=\"math inline\">\\(p(x|\\theta)\\)</span> 生成的。</p>\n<h2 id=\"频率派的观点\">频率派的观点</h2>\n<p><span class=\"math inline\">\\(p(x|\\theta)\\)</span>中的 <span class=\"math inline\">\\(\\theta\\)</span> 是一个常量。对于 <span class=\"math inline\">\\(N\\)</span> 个观测来说观测集的概率为 <span class=\"math inline\">\\(p(X|\\theta)\\mathop{=}\\limits _{iid}\\prod\\limits _{i=1}^{N}p(x_{i}|\\theta))\\)</span> 。为了求 <span class=\"math inline\">\\(\\theta\\)</span> 的大小，我们采用最大对数似然MLE的方法：</p>\n<p><span class=\"math display\">\\[\n\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)\n\\]</span></p>\n<h2 id=\"贝叶斯派的观点\">贝叶斯派的观点</h2>\n<p>贝叶斯派认为 <span class=\"math inline\">\\(p(x|\\theta)\\)</span> 中的 <span class=\"math inline\">\\(\\theta\\)</span> 不是一个常量。这个 <span class=\"math inline\">\\(\\theta\\)</span> 满足一个预设的先验的分布 <span class=\"math inline\">\\(\\theta\\sim p(\\theta)\\)</span> 。于是根据贝叶斯定理依赖观测集参数的后验可以写成：</p>\n<p><span class=\"math display\">\\[\np(\\theta|X)=\\frac{p(X|\\theta)\\cdot p(\\theta)}{p(X)}=\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}\n\\]</span> 为了求 <span class=\"math inline\">\\(\\theta\\)</span> 的值，我们要最大化这个参数后验MAP：</p>\n<p><span class=\"math display\">\\[\n\\theta_{MAP}=\\mathop{argmax}\\limits _{\\theta}p(\\theta|X)=\\mathop{argmax}\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)\n\\]</span> 其中第二个等号是由于分母和 <span class=\"math inline\">\\(\\theta\\)</span> 没有关系。求解这个 <span class=\"math inline\">\\(\\theta\\)</span> 值后计算<span class=\"math inline\">\\(\\frac{p(X|\\theta)\\cdot p(\\theta)}{\\int\\limits _{\\theta}p(X|\\theta)\\cdot p(\\theta)d\\theta}\\)</span> ，就得到了参数的后验概率。其中 <span class=\"math inline\">\\(p(X|\\theta)\\)</span> 叫似然，是我们的模型分布。得到了参数的后验分布后，我们可以将这个分布用于预测贝叶斯预测： <span class=\"math display\">\\[\np(x_{new}|X)=\\int\\limits _{\\theta}p(x_{new}|\\theta)\\cdot p(\\theta|X)d\\theta\n\\]</span> 其中积分中的被乘数是模型，乘数是后验分布。</p>\n<h2 id=\"小结\">小结</h2>\n<p>频率派和贝叶斯派分别给出了一系列的机器学习算法。频率派的观点导出了一系列的统计机器学习算法而贝叶斯派导出了概率图理论。在应用频率派的 MLE 方法时最优化理论占有重要地位。而贝叶斯派的算法无论是后验概率的建模还是应用这个后验进行推断时积分占有重要地位。因此采样积分方法如 MCMC 有很多应用。</p>\n<h1 id=\"mathbasics\">MathBasics</h1>\n<h2 id=\"高斯分布\">高斯分布</h2>\n<h3 id=\"一维情况-mle\">一维情况 MLE</h3>\n<p>高斯分布在机器学习中占有举足轻重的作用。在 MLE 方法中：</p>\n<p><span class=\"math display\">\\[\n\\theta=(\\mu,\\Sigma)=(\\mu,\\sigma^{2}),\\theta_{MLE}=\\mathop{argmax}\\limits _{\\theta}\\log p(X|\\theta)\\mathop{=}\\limits _{iid}\\mathop{argmax}\\limits _{\\theta}\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)\n\\]</span> 一般地，高斯分布的概率密度函数PDF写为：</p>\n<p><span class=\"math display\">\\[\np(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n\\]</span> 带入 MLE 中我们考虑一维的情况</p>\n<p><span class=\"math display\">\\[\n\\log p(X|\\theta)=\\sum\\limits _{i=1}^{N}\\log p(x_{i}|\\theta)=\\sum\\limits _{i=1}^{N}\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp(-(x_{i}-\\mu)^{2}/2\\sigma^{2})\n\\]</span> 首先对 <span class=\"math inline\">\\(\\mu\\)</span> 的极值可以得到 ： <span class=\"math display\">\\[\n\\mu_{MLE}=\\mathop{argmax}\\limits _{\\mu}\\log p(X|\\theta)=\\mathop{argmax}\\limits _{\\mu}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n\\]</span> 于是： <span class=\"math display\">\\[\n\\frac{\\partial}{\\partial\\mu}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}=0\\longrightarrow\\mu_{MLE}=\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}\n\\]</span> 其次对 <span class=\"math inline\">\\(\\theta\\)</span> 中的另一个参数 <span class=\"math inline\">\\(\\sigma\\)</span> ，有： <span class=\"math display\">\\[\n\\begin{align}\n\\sigma_{MLE}=\\mathop{argmax}\\limits _{\\sigma}\\log p(X|\\theta)&amp;=\\mathop{argmax}\\limits _{\\sigma}\\sum\\limits _{i=1}^{N}[-\\log\\sigma-\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]\\nonumber\\\\\n&amp;=\\mathop{argmin}\\limits _{\\sigma}\\sum\\limits _{i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]\n\\end{align}\n\\]</span> 于是： <span class=\"math display\">\\[\n\\frac{\\partial}{\\partial\\sigma}\\sum\\limits _{i=1}^{N}[\\log\\sigma+\\frac{1}{2\\sigma^{2}}(x_{i}-\\mu)^{2}]=0\\longrightarrow\\sigma_{MLE}^{2}=\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n\\]</span> 值得注意的是，上面的推导中，首先对 <span class=\"math inline\">\\(\\mu\\)</span> 求 MLE， 然后利用这个结果求 <span class=\"math inline\">\\(\\sigma_{MLE}\\)</span> ，因此可以预期的是对数据集求期望时 <span class=\"math inline\">\\(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}]\\)</span> 是无偏差的： <span class=\"math display\">\\[\n\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}]=\\frac{1}{N}\\sum\\limits _{i=1}^{N}\\mathbb{E}_{\\mathcal{D}}[x_{i}]=\\mu\n\\]</span> 但是当对 <span class=\"math inline\">\\(\\sigma_{MLE}\\)</span> 求 期望的时候由于使用了单个数据集的 <span class=\"math inline\">\\(\\mu_{MLE}\\)</span>，因此对所有数据集求期望的时候我们会发现 <span class=\"math inline\">\\(\\sigma_{MLE}\\)</span> 是 有偏的：</p>\n<p><span class=\"math display\">\\[\n\\begin{align}\n\\mathbb{E}_{\\mathcal{D}}[\\sigma_{MLE}^{2}]&amp;=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu_{MLE})^{2}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}(x_{i}^{2}-2x_{i}\\mu_{MLE}+\\mu_{MLE}^{2})\\nonumber\n\\\\&amp;=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu_{MLE}^{2}]=\\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu^{2}+\\mu^{2}-\\mu_{MLE}^{2}]\\nonumber\\\\\n&amp;= \\mathbb{E}_{\\mathcal{D}}[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}^{2}-\\mu^{2}]-\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}-\\mu^{2}]=\\sigma^{2}-(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}]-\\mu^{2})\\nonumber\\\\&amp;=\\sigma^{2}-(\\mathbb{E}_{\\mathcal{D}}[\\mu_{MLE}^{2}]-\\mathbb{E}_{\\mathcal{D}}^{2}[\\mu_{MLE}])=\\sigma^{2}-Var[\\mu_{MLE}]\\nonumber\\\\&amp;=\\sigma^{2}-Var[\\frac{1}{N}\\sum\\limits _{i=1}^{N}x_{i}]=\\sigma^{2}-\\frac{1}{N^{2}}\\sum\\limits _{i=1}^{N}Var[x_{i}]=\\frac{N-1}{N}\\sigma^{2}\n\\end{align}\n\\]</span> 所以： <span class=\"math display\">\\[\n\\hat{\\sigma}^{2}=\\frac{1}{N-1}\\sum\\limits _{i=1}^{N}(x_{i}-\\mu)^{2}\n\\]</span></p>\n<h3 id=\"多维情况\">多维情况</h3>\n<p>多维高斯分布表达式为： <span class=\"math display\">\\[\np(x|\\mu,\\Sigma)=\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)}\n\\]</span> 其中 <span class=\"math inline\">\\(x,\\mu\\in\\mathbb{R}^{p},\\Sigma\\in\\mathbb{R}^{p\\times p}\\)</span> ，<span class=\"math inline\">\\(\\Sigma\\)</span> 为协方差矩阵，一般而言也是半正定矩阵。这里我们只考虑正定矩阵。首先我们处理指数上的数字，指数上的数字可以记为 <span class=\"math inline\">\\(x\\)</span> 和 <span class=\"math inline\">\\(\\mu\\)</span> 之间的马氏距离。对于对称的协方差矩阵可进行特征值分解，<span class=\"math inline\">\\(\\Sigma=U\\Lambda U^{T}=(u_{1},u_{2},\\cdots,u_{p})diag(\\lambda_{i})(u_{1},u_{2},\\cdots,u_{p})^{T}=\\sum\\limits _{i=1}^{p}u_{i}\\lambda_{i}u_{i}^{T}\\)</span> ，于是：</p>\n<p><span class=\"math display\">\\[\n\\Sigma^{-1}=\\sum\\limits _{i=1}^{p}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}\n\\]</span></p>\n<p><span class=\"math display\">\\[\n\\Delta=(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)=\\sum\\limits _{i=1}^{p}(x-\\mu)^{T}u_{i}\\frac{1}{\\lambda_{i}}u_{i}^{T}(x-\\mu)=\\sum\\limits _{i=1}^{p}\\frac{y_{i}^{2}}{\\lambda_{i}}\n\\]</span></p>\n<p>我们注意到 <span class=\"math inline\">\\(y_{i}\\)</span> 是 <span class=\"math inline\">\\(x-\\mu\\)</span> 在特征向量 <span class=\"math inline\">\\(u_{i}\\)</span> 上的投影长度，因此上式子就是 <span class=\"math inline\">\\(\\Delta\\)</span> 取不同值时的同心椭圆。</p>\n<p>下面我们看多维高斯模型在实际应用时的两个问题</p>\n<ol type=\"1\">\n<li><p>参数 <span class=\"math inline\">\\(\\Sigma,\\mu\\)</span> 的自由度为 <span class=\"math inline\">\\(O(p^{2})\\)</span> 对于维度很高的数据其自由度太高。解决方案：高自由度的来源是 <span class=\"math inline\">\\(\\Sigma\\)</span> 有 <span class=\"math inline\">\\(\\frac{p(p+1)}{2}\\)</span> 个自由参数，可以假设其是对角矩阵，甚至在各向同性假设中假设其对角线上的元素都相同。前一种的算法有 Factor Analysis，后一种有概率 PCA(p-PCA) 。</p></li>\n<li><p>第二个问题是单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果。解决方案：高斯混合GMM 模型。</p></li>\n</ol>\n<p>下面对多维高斯分布的常用定理进行介绍。</p>\n<p>我们记 <span class=\"math inline\">\\(x=(x_1, x_2,\\cdots,x_p)^T=(x_{a,m\\times 1}, x_{b,n\\times1})^T,\\mu=(\\mu_{a,m\\times1}, \\mu_{b,n\\times1}),\\Sigma=\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}\\)</span>，已知 <span class=\"math inline\">\\(x\\sim\\mathcal{N}(\\mu,\\Sigma)\\)</span>。</p>\n<p>首先是一个高斯分布的定理：</p>\n<blockquote>\n<p>定理：已知 <span class=\"math inline\">\\(x\\sim\\mathcal{N}(\\mu,\\Sigma), y\\sim Ax+b\\)</span>，那么 <span class=\"math inline\">\\(y\\sim\\mathcal{N}(A\\mu+b, A\\Sigma A^T)\\)</span>。</p>\n<p>证明：<span class=\"math inline\">\\(\\mathbb{E}[y]=\\mathbb{E}[Ax+b]=A\\mathbb{E}[x]+b=A\\mu+b\\)</span>，<span class=\"math inline\">\\(Var[y]=Var[Ax+b]=Var[Ax]=A\\cdot Var[x]\\cdot A^T\\)</span>。</p>\n</blockquote>\n<p>下面利用这个定理得到 <span class=\"math inline\">\\(p(x_a),p(x_b),p(x_a|x_b),p(x_b|x_a)\\)</span> 这四个量。</p>\n<ol type=\"1\">\n<li><p><span class=\"math inline\">\\(x_a=\\begin{pmatrix}\\mathbb{I}_{m\\times m}&amp;\\mathbb{O}_{m\\times n})\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\\)</span>，代入定理中得到： <span class=\"math display\">\\[\n\\mathbb{E}[x_a]=\\begin{pmatrix}\\mathbb{I}&amp;\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_a\\\\\nVar[x_a]=\\begin{pmatrix}\\mathbb{I}&amp;\\mathbb{O}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}\\mathbb{I}\\\\\\mathbb{O}\\end{pmatrix}=\\Sigma_{aa}\n\\]</span> 所以 <span class=\"math inline\">\\(x_a\\sim\\mathcal{N}(\\mu_a,\\Sigma_{aa})\\)</span>。</p></li>\n<li><p>同样的，<span class=\"math inline\">\\(x_b\\sim\\mathcal{N}(\\mu_b,\\Sigma_{bb})\\)</span>。</p></li>\n<li><p>对于两个条件概率，我们引入三个量： <span class=\"math display\">\\[\nx_{b\\cdot a}=x_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\\\\n\\mu_{b\\cdot a}=\\mu_b-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\mu_a\\\\\n\\Sigma_{bb\\cdot a}=\\Sigma_{bb}-\\Sigma_{ba}\\Sigma_{aa}^{-1}\\Sigma_{ab}\n\\]</span> 特别的，最后一个式子叫做 <span class=\"math inline\">\\(\\Sigma_{bb}\\)</span> 的 Schur Complementary。可以看到： <span class=\"math display\">\\[\nx_{b\\cdot a}=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}x_a\\\\x_b\\end{pmatrix}\n\\]</span> 所以： <span class=\"math display\">\\[\n\\mathbb{E}[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\mu_a\\\\\\mu_b\\end{pmatrix}=\\mu_{b\\cdot a}\\\\\nVar[x_{b\\cdot a}]=\\begin{pmatrix}-\\Sigma_{ba}\\Sigma_{aa}^{-1}&amp;\\mathbb{I}_{n\\times n}\\end{pmatrix}\\begin{pmatrix}\\Sigma_{aa}&amp;\\Sigma_{ab}\\\\\\Sigma_{ba}&amp;\\Sigma_{bb}\\end{pmatrix}\\begin{pmatrix}-\\Sigma_{aa}^{-1}\\Sigma_{ba}^T\\\\\\mathbb{I}_{n\\times n}\\end{pmatrix}=\\Sigma_{bb\\cdot a}\n\\]</span> 利用这三个量可以得到 <span class=\"math inline\">\\(x_b=x_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\\)</span>。因此： <span class=\"math display\">\\[\n\\mathbb{E}[x_b|x_a]=\\mu_{b\\cdot a}+\\Sigma_{ba}\\Sigma_{aa}^{-1}x_a\n\\]</span></p>\n<p><span class=\"math display\">\\[\nVar[x_b|x_a]=\\Sigma_{bb\\cdot a}\n\\]</span></p>\n<p>这里同样用到了定理。</p></li>\n<li><p>同样： <span class=\"math display\">\\[\nx_{a\\cdot b}=x_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\\\\\n\\mu_{a\\cdot b}=\\mu_a-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\mu_b\\\\\n\\Sigma_{aa\\cdot b}=\\Sigma_{aa}-\\Sigma_{ab}\\Sigma_{bb}^{-1}\\Sigma_{ba}\n\\]</span> 所以： <span class=\"math display\">\\[\n\\mathbb{E}[x_a|x_b]=\\mu_{a\\cdot b}+\\Sigma_{ab}\\Sigma_{bb}^{-1}x_b\n\\]</span></p>\n<p><span class=\"math display\">\\[\nVar[x_a|x_b]=\\Sigma_{aa\\cdot b}\n\\]</span></p></li>\n</ol>\n<p>下面利用上边四个量，求解线性模型：</p>\n<blockquote>\n<p>已知：<span class=\"math inline\">\\(p(x)=\\mathcal{N}(\\mu,\\Lambda^{-1}),p(y|x)=\\mathcal{N}(Ax+b,L^{-1})\\)</span>，求解：<span class=\"math inline\">\\(p(y),p(x|y)\\)</span>。</p>\n<p>解：令 <span class=\"math inline\">\\(y=Ax+b+\\epsilon,\\epsilon\\sim\\mathcal{N}(0,L^{-1})\\)</span>，所以 <span class=\"math inline\">\\(\\mathbb{E}[y]=\\mathbb{E}[Ax+b+\\epsilon]=A\\mu+b\\)</span>，<span class=\"math inline\">\\(Var[y]=A \\Lambda^{-1}A^T+L^{-1}\\)</span>，因此： <span class=\"math display\">\\[\n  p(y)=\\mathcal{N}(A\\mu+b,L^{-1}+A\\Lambda^{-1}A^T)\n  \\]</span> 引入 <span class=\"math inline\">\\(z=\\begin{pmatrix}x\\\\y\\end{pmatrix}\\)</span>，我们可以得到 <span class=\"math inline\">\\(Cov[x,y]=\\mathbb{E}[(x-\\mathbb{E}[x])(y-\\mathbb{E}[y])^T]\\)</span>。对于这个协方差可以直接计算： <span class=\"math display\">\\[\n  \\begin{align}\n  Cov(x,y)&amp;=\\mathbb{E}[(x-\\mu)(Ax-A\\mu+\\epsilon)^T]=\\mathbb{E}[(x-\\mu)(x-\\mu)^TA^T]=Var[x]A^T=\\Lambda^{-1}A^T\n  \\end{align}\n  \\]</span> 注意到协方差矩阵的对称性，所以 <span class=\"math inline\">\\(p(z)=\\mathcal{N}\\begin{pmatrix}\\mu\\\\A\\mu+b\\end{pmatrix},\\begin{pmatrix}\\Lambda^{-1}&amp;\\Lambda^{-1}A^T\\\\A\\Lambda^{-1}&amp;L^{-1}+A\\Lambda^{-1}A^T\\end{pmatrix})\\)</span>。根据之前的公式，我们可以得到： <span class=\"math display\">\\[\n  \\mathbb{E}[x|y]=\\mu+\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}(y-A\\mu-b)\n  \\]</span></p>\n<p><span class=\"math display\">\\[\n  Var[x|y]=\\Lambda^{-1}-\\Lambda^{-1}A^T(L^{-1}+A\\Lambda^{-1}A^T)^{-1}A\\Lambda^{-1}\n  \\]</span></p>\n</blockquote>\n"},{"title":"Hello World","_content":"Welcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","source":"_posts/hello-world.md","raw":"---\ntitle: Hello World\n---\nWelcome to [Hexo](https://hexo.io/)! This is your very first post. Check [documentation](https://hexo.io/docs/) for more info. If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues).\n\n## Quick Start\n\n### Create a new post\n\n``` bash\n$ hexo new \"My New Post\"\n```\n\nMore info: [Writing](https://hexo.io/docs/writing.html)\n\n### Run server\n\n``` bash\n$ hexo server\n```\n\nMore info: [Server](https://hexo.io/docs/server.html)\n\n### Generate static files\n\n``` bash\n$ hexo generate\n```\n\nMore info: [Generating](https://hexo.io/docs/generating.html)\n\n### Deploy to remote sites\n\n``` bash\n$ hexo deploy\n```\n\nMore info: [Deployment](https://hexo.io/docs/one-command-deployment.html)\n","slug":"hello-world","published":1,"date":"2021-03-13T01:50:26.432Z","updated":"2021-03-13T01:50:26.432Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmhntquu0001p8ac1a4uedhq","content":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"quick-start\">Quick Start</h2>\n<h3 id=\"create-a-new-post\">Create a new post</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"run-server\">Run server</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"generate-static-files\">Generate static files</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"deploy-to-remote-sites\">Deploy to remote sites</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Welcome to <a href=\"https://hexo.io/\">Hexo</a>! This is your very first post. Check <a href=\"https://hexo.io/docs/\">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href=\"https://hexo.io/docs/troubleshooting.html\">troubleshooting</a> or you can ask me on <a href=\"https://github.com/hexojs/hexo/issues\">GitHub</a>.</p>\n<h2 id=\"quick-start\">Quick Start</h2>\n<h3 id=\"create-a-new-post\">Create a new post</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo new <span class=\"string\">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/writing.html\">Writing</a></p>\n<h3 id=\"run-server\">Run server</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo server</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/server.html\">Server</a></p>\n<h3 id=\"generate-static-files\">Generate static files</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo generate</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/generating.html\">Generating</a></p>\n<h3 id=\"deploy-to-remote-sites\">Deploy to remote sites</h3>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo deploy</span><br></pre></td></tr></table></figure>\n<p>More info: <a href=\"https://hexo.io/docs/one-command-deployment.html\">Deployment</a></p>\n"},{"title":"linear_regression","date":"2021-03-05T08:52:07.000Z","_content":"","source":"_posts/linear-regression.md","raw":"---\ntitle: linear_regression\ndate: 2021-03-05 16:52:07\ntags:\n---\n","slug":"linear-regression","published":1,"updated":"2021-03-13T01:50:26.432Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmhntquw0002p8acfbu6924c","content":"\n","site":{"data":{}},"excerpt":"","more":"\n"},{"title":"logistic regression","date":"2021-03-06T05:08:41.000Z","mathjax":true,"_content":"\n#  Logistic  Regression\n\n$$\n\\theta\n$$\n\n","source":"_posts/logistic-regression.md","raw":"---\ntitle: logistic regression\ndate: 2021-03-06 13:08:41\ntags:\ncategories:\n- [Machine Learning,regression]\n- [Machine Learning,classification]\nmathjax: true\n---\n\n#  Logistic  Regression\n\n$$\n\\theta\n$$\n\n","slug":"logistic-regression","published":1,"updated":"2021-03-20T10:46:30.587Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckmhntqux0003p8ac08gf7pk4","content":"<h1 id=\"logistic-regression\">Logistic Regression</h1>\n<p><span class=\"math display\">\\[\n\\theta\n\\]</span></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"logistic-regression\">Logistic Regression</h1>\n<p><span class=\"math display\">\\[\n\\theta\n\\]</span></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"ckmhntqux0003p8ac08gf7pk4","category_id":"ckmhntquy0004p8achdiq3p5z","_id":"ckmhntqv00007p8ac7lrp9suh"},{"post_id":"ckmhntqux0003p8ac08gf7pk4","category_id":"ckmhntquz0005p8ac01l60j56","_id":"ckmhntqv00008p8ac1ig87p0d"},{"post_id":"ckmhntqux0003p8ac08gf7pk4","category_id":"ckmhntquy0004p8achdiq3p5z","_id":"ckmhntqv00009p8ac4kobebl6"},{"post_id":"ckmhntqux0003p8ac08gf7pk4","category_id":"ckmhntquz0006p8ac76bjbzpd","_id":"ckmhntqv0000ap8acgcx62su5"}],"PostTag":[],"Tag":[]}}